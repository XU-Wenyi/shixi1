{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "from collections import namedtuple\n",
    "#import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import argparse\n",
    "\n",
    "from knowledge_graph import KnowledgeGraph\n",
    "from kg_env import BatchKGEnvironment\n",
    "from utils import *\n",
    "\n",
    "logger = None\n",
    "\n",
    "SavedAction = namedtuple('SavedAction', ['log_prob'])#, 'value'])\n",
    "\n",
    "# Hyper Parameters for DQN\n",
    "GAMMA = 0.9 # discount factor for target Q\n",
    "INITIAL_EPSILON = 0.5 # starting value of epsilon\n",
    "FINAL_EPSILON = 0.01 # final value of epsilon\n",
    "REPLAY_SIZE = 10000 # experience replay buffer size\n",
    "BATCH_SIZE = 32 # size of minibatch\n",
    "REPLACE_TARGET_FREQ = 10 # frequency to update target Q network\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    # DQN Agent\n",
    "    def __init__(self, env, state_dim, act_dim, gamma=0.99,hidden_sizes=[512,256]):\n",
    "        super(DQN, self).__init__()\n",
    "        self.replay_buffer = deque() #双向队列 可以从左append些什么\n",
    "        # init some parameters\n",
    "        self.time_step = 0\n",
    "        self.epsilon = INITIAL_EPSILON\n",
    "        self.state_dim = 400 #state_dim\n",
    "        self.action_dim = 251       \n",
    "        \n",
    "        self.current_net1 = nn.Linear(self.state_dim,20)\n",
    "        self.current_net2 = nn.Linear(20,self.action_dim)\n",
    "        \n",
    "        self.target_net1 = nn.Linear(self.state_dim,20)\n",
    "        self.target_net2 = nn.Linear(20,self.action_dim)\n",
    "        \n",
    "        '''state_dim 要改!!!'''\n",
    "         #act_dim\n",
    "        #env.action_space.n\n",
    "        \n",
    "        self.saved_actions = []        \n",
    "        self.rewards = []\n",
    "        self.entropy = []\n",
    "\n",
    "    \n",
    "    def forward(self,inputs):\n",
    "        state,act_mask = inputs\n",
    "        #act_mask = a_m\n",
    "        h_1 = self.current_net1(state)\n",
    "        \n",
    "        h_1 = F.dropout(F.relu(h_1))\n",
    "        self.Q_value = self.current_net2(h_1)\n",
    "        \n",
    "        h_2 = self.target_net1(state)\n",
    "        h_2 = F.dropout(F.relu(h_2))\n",
    "        self.target_Q_value = self.target_net2(h_2)\n",
    "\n",
    "        #optimizer = optim.Adam(model.parameters(), lr=args.lr)\n",
    "        \n",
    "\n",
    "    def perceive(self,state,action,reward,next_state,done,optimizer):\n",
    "        one_hot_action = np.zeros(self.action_dim)\n",
    "        #应该要 32*action_dim\n",
    "        one_hot_action[action] = 1\n",
    "        self.replay_buffer.append((state,one_hot_action,reward,next_state,done))\n",
    "        self.optimizer = optimizer\n",
    "        if len(self.replay_buffer) > REPLAY_SIZE:\n",
    "            self.replay_buffer.popleft()\n",
    "\n",
    "        if len(self.replay_buffer) > BATCH_SIZE:\n",
    "            self.train_Q_network()\n",
    "    \n",
    "    \n",
    "    def train_Q_network(self):\n",
    "        self.time_step += 1\n",
    "        # Step 1: obtain random minibatch from replay memory\n",
    "        minibatch = random.sample(self.replay_buffer,BATCH_SIZE)\n",
    "        state_batch = [data[0] for data in minibatch]\n",
    "        action_batch = [data[1] for data in minibatch]\n",
    "        reward_batch = [data[2] for data in minibatch]\n",
    "        next_state_batch = [data[3] for data in minibatch]\n",
    "        reward_batch = torch.tensor(reward_batch)\n",
    "        # Step 2: calculate y\n",
    "        y_batch = []\n",
    "        current_Q_batch = self.Q_value\n",
    "        #max_action_next = np.argmax(current_Q_batch, axis=1)\n",
    "        max_action_next=torch.argmax(current_Q_batch ,dim=1)\n",
    "        target_Q_batch = self.target_Q_value\n",
    "\n",
    "        for i in range(0,BATCH_SIZE):\n",
    "            done = minibatch[i][4]\n",
    "            if i == 0:\n",
    "                if done:\n",
    "                    y_batch = torch.tensor(reward_batch[i])\n",
    "                else:\n",
    "                    #print('2:',len(target_Q_batch),len(target_Q_batch[0]))\n",
    "                    target_Q_value = target_Q_batch[i, max_action_next[i]] #[i, i, max_action_next[i]]\n",
    "                    #print('type:',type(reward_batch),type(target_Q_value))\n",
    "                    #print(reward_batch)\n",
    "                    #print(target_Q_value)\n",
    "                    y_batch = torch.tensor(torch.tensor(reward_batch[i]) + GAMMA * target_Q_value)\n",
    "            else:\n",
    "                if done:\n",
    "                    #y_batch = np.append(y_batch,reward_batch[i])\n",
    "                    y_batch = torch.tensor(y_batch)\n",
    "                    app = torch.tensor(reward_batch[i])\n",
    "                    y_batch = torch.cat((y_batch,app),0)\n",
    "                else :\n",
    "                    #print('1:',len(target_Q_batch),len(target_Q_batch[0]))\n",
    "                    #print(len(target_Q_batch[0][0]))\n",
    "                    target_Q_value = target_Q_batch[i, max_action_next[i]] #[i, i,max_action_next[i]]\n",
    "                    #print('type:',type(reward_batch),type(target_Q_value))\n",
    "                    y_batch = torch.tensor(y_batch)\n",
    "                    app = torch.tensor(reward_batch[i]+GAMMA*target_Q_value)\n",
    "                    y_batch = torch.cat((y_batch,app),0)\n",
    "                    #y_batch = np.append(y_batch, reward_batch[i] + GAMMA * target_Q_value)\n",
    "                    \n",
    "        y_batch = y_batch.reshape(32,32)\n",
    "        #print(self.entropy)\n",
    "        self.loss = self.entropy.mean()\n",
    "        self.optimizer.zero_grad()\n",
    "        self.loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "    def egreedy_action(self,batch_state,act_mask):#batch_state = state\n",
    "        state = torch.FloatTensor(batch_state)#.to(device)  # Tensor [bs, state_dim]\n",
    "        act_mask = torch.FloatTensor(act_mask)#.to(device)  # Tensor of [bs, act_dim]\n",
    "        self((state,act_mask))\n",
    "        ''''''\n",
    "        act_mask = act_mask.type(torch.uint8)        \n",
    "        self.Q_value[1-act_mask] = -999999.0\n",
    "        self.Q_value = F.softmax(self.Q_value, dim=-1)\n",
    "        \n",
    "        m = Categorical(self.Q_value)#加起来应该不是1？说不准\n",
    "        acts = m.sample()\n",
    "        # [CAVEAT] If sampled action is out of action_space, choose the first action in action_space.\n",
    "        self.entropy = torch.cat((torch.tensor(self.entropy),m.entropy()))\n",
    "        #self.entropy.append(m.entropy())\n",
    "        \n",
    "        if random.random() <= self.epsilon:\n",
    "            acts = []\n",
    "            self.epsilon -= (INITIAL_EPSILON - FINAL_EPSILON) / 10000\n",
    "            for i in range(BATCH_SIZE):\n",
    "                acts.append(random.randint(0,self.action_dim - 1))\n",
    "            acts_log = torch.tensor(acts)\n",
    "            self.saved_actions.append(SavedAction(m.log_prob(acts_log)))#, value))\n",
    "            return acts #要return一行，不能只有一个值吧!!\n",
    "        else:\n",
    "            self.epsilon -= (INITIAL_EPSILON - FINAL_EPSILON) / 10000\n",
    "            acts_log = torch.tensor(acts)\n",
    "            self.saved_actions.append(SavedAction(m.log_prob(acts_log)))#, value))\n",
    "            return acts #np.argmax(Q_value)\n",
    "\n",
    "        \n",
    "    def action(self,batch_state,act_mask):\n",
    "        batch_state = torch.tensor(batch_state,dtype=torch.float32)\n",
    "        act_mask = torch.tensor(act_mask,dtype=torch.float32)\n",
    "        \n",
    "        self((batch_state,act_mask))\n",
    "        #self.forward(batch_state,act_mask)\n",
    "        ''''''\n",
    "        act_mask = act_mask.type(torch.uint8)\n",
    "        #self((batch_state, act_mask))        \n",
    "        self.Q_value[1-act_mask] = -999999.0\n",
    "        self.Q_value = F.softmax(self.Q_value, dim=-1)\n",
    "\n",
    "        m = Categorical(self.Q_value)#加起来应该不是1？说不准 \n",
    "        acts = m.sample()\n",
    "        # [CAVEAT] If sampled action is out of action_space, choose the first action in action_space.\n",
    "        self.entropy = torch.cat((torch.tensor(self.entropy),m.entropy()))\n",
    "        #self.entropy.append(m.entropy())\n",
    "        acts_log = torch.tensor(acts)\n",
    "        self.saved_actions.append(SavedAction(m.log_prob(acts_log)))#, value))\n",
    "        return acts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ACDataLoader(object):\n",
    "    def __init__(self, uids, batch_size):\n",
    "        self.uids = np.array(uids)\n",
    "        self.num_users = len(uids)\n",
    "        self.batch_size = batch_size\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self._rand_perm = np.random.permutation(self.num_users)\n",
    "        self._start_idx = 0\n",
    "        self._has_next = True\n",
    "\n",
    "    def has_next(self):\n",
    "        return self._has_next\n",
    "\n",
    "\n",
    "    #直接把训练集和测试机给改了就行了 序号都是 32的倍数就可\n",
    "    def get_batch(self):\n",
    "        if not self._has_next:\n",
    "            return None\n",
    "        # Multiple users per batch\n",
    "        end_idx = min(self._start_idx + self.batch_size, self.num_users)\n",
    "        #print('get_batch,end_idx:',end_idx,';',self.num_users)\n",
    "        batch_idx = self._rand_perm[self._start_idx:end_idx]\n",
    "        batch_uids = self.uids[batch_idx]\n",
    "        self._has_next = self._has_next and end_idx < self.num_users\n",
    "        self._start_idx = end_idx\n",
    "        \n",
    "        return batch_uids.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPISODE = 3000 # Episode limitation\n",
    "STEP = 300 # Step limitation in an episode\n",
    "TEST = 5 # The number of experiment test every 100 episode\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--dataset', type=str, default=BEAUTY, help='One of {clothing, cell, beauty, cd}')\n",
    "parser.add_argument('--name', type=str, default='train_agent', help='directory name.')\n",
    "parser.add_argument('--seed', type=int, default=123, help='random seed.')\n",
    "parser.add_argument('--gpu', type=str, default='0', help='gpu device.')\n",
    "parser.add_argument('--epochs', type=int, default=300, help='Max number of epochs.')\n",
    "parser.add_argument('--batch_size', type=int, default=32, help='batch size.')\n",
    "parser.add_argument('--lr', type=float, default=1e-4, help='learning rate.')\n",
    "parser.add_argument('--max_acts', type=int, default=250, help='Max number of actions.')\n",
    "#parser.add_argument('--state_dim', type=int, default=250, help='Max number of actions.')\n",
    "#\n",
    "parser.add_argument('--max_path_len', type=int, default=3, help='Max path length.')\n",
    "parser.add_argument('--gamma', type=float, default=0.99, help='reward discount factor.')\n",
    "parser.add_argument('--ent_weight', type=float, default=1e-2, help='weight factor for entropy loss')#!!!-3\n",
    "parser.add_argument('--act_dropout', type=float, default=0.5, help='action dropout rate.')\n",
    "parser.add_argument('--state_history', type=int, default=1, help='state history length')\n",
    "parser.add_argument('--hidden', type=int, nargs='*', default=[512, 256], help='number of samples')\n",
    "args = parser.parse_args(['--dataset',CELL])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]  Namespace(act_dropout=0.5, batch_size=32, dataset='cell', device='cpu', ent_weight=0.01, epochs=300, gamma=0.99, gpu='0', hidden=[512, 256], log_dir='./tmp/Amazon_Cellphones/train_agent', lr=0.0001, max_acts=250, max_path_len=3, name='train_agent', seed=123, state_history=1)\n"
     ]
    }
   ],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = args.gpu\n",
    "args.device = torch.device('cuda:0') if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "args.log_dir = '{}/{}'.format(TMP_DIR[args.dataset], args.name)\n",
    "if not os.path.isdir(args.log_dir):\n",
    "    os.makedirs(args.log_dir)\n",
    "\n",
    "global logger\n",
    "logger = get_logger(args.log_dir + '/train_log_DQN.txt')\n",
    "logger.info(args)\n",
    "\n",
    "set_random_seed(args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args):\n",
    "    \n",
    "    # initialize OpenAI Gym env and dqn agent\n",
    "    #env = gym.make(ENV_NAME)\n",
    "    env = BatchKGEnvironment(args.dataset, args.max_acts, max_path_len=args.max_path_len, state_history=args.state_history)\n",
    "    uids = list(env.kg(USER).keys())\n",
    "    print('uids:',len(uids))\n",
    "    uids = np.arange(19488).tolist()\n",
    "    agent = DQN(env,env.state_dim,env.act_dim,gamma = args.gamma,hidden_sizes = args.hidden)\n",
    "    dataloader = ACDataLoader(uids, args.batch_size)\n",
    "    logger.info('Parameters:' + str([i[0] for i in agent.named_parameters()]))\n",
    "    optimizer = optim.Adam(agent.parameters(), lr=args.lr)\n",
    "    #model = ActorCritic(env.state_dim, env.act_dim, gamma=args.gamma, hidden_sizes=args.hidden).to(args.device)\n",
    "    #logger.info('Parameters:' + str([i[0] for i in model.named_parameters()]))\n",
    "\n",
    "    '''    \n",
    "    uids = list(env.kg(USER).keys())\n",
    "    dataloader = ACDataLoader(uids, args.batch_size)\n",
    "    model = ActorCritic(env.state_dim, env.act_dim, gamma=args.gamma, hidden_sizes=args.hidden).to(args.device)\n",
    "    logger.info('Parameters:' + str([i[0] for i in model.named_parameters()]))\n",
    "    '''\n",
    "    episode = 1\n",
    "    for epoch in range(0, args.epochs):\n",
    "        ### Start epoch ###\n",
    "        if epoch % 10 == 0:\n",
    "            print('epoch:',epoch)\n",
    "        dataloader.reset()\n",
    "        while dataloader.has_next():\n",
    "            batch_uids = dataloader.get_batch()\n",
    "            ### Start batch episodes ###\n",
    "            #print('batch_uids:',batch_uids,';',len(batch_uids))\n",
    "            batch_state1 = env.reset(batch_uids)  # numpy array of [bs, state_dim]\n",
    "            #print('egreedy_action, batch_state1:',batch_state1,';',len(batch_state1),';',len(batch_state1[0]))\n",
    "            for step in range(STEP):#while not done:\n",
    "\n",
    "                batch_act_mask = env.batch_action_mask(dropout=args.act_dropout)  # numpy array of size [bs, act_dim]\n",
    "                '''select action'''\n",
    "                #print('shape of batch state1:',batch_state1.shape)\n",
    "                batch_act_idx = agent.egreedy_action(batch_state1,batch_act_mask)#batch_act_mask\n",
    "                batch_state2, batch_reward, done = env.batch_step(batch_act_idx)\n",
    "                batch_state2 = batch_state2.reshape((-1,400))\n",
    "                agent.perceive(batch_state1,batch_act_idx,batch_reward,batch_state2,done,optimizer)\n",
    "                agent.rewards.append(batch_reward)\n",
    "                #print('batch_state2:',batch_state2,';',len(batch_state2),';',len(batch_state2[0]))\n",
    "                batch_state1 = batch_state2\n",
    "                #train Q network\n",
    "                if done:\n",
    "                    break\n",
    "                # Test every 100 episodes\n",
    "            if epoch % 100 == 0:\n",
    "                total_reward = 0\n",
    "                for i in range(TEST):\n",
    "                    #batch_uids = dataloader.get_batch()\n",
    "                    #print('action batch_uids:',batch_uids)\n",
    "                    ### Start batch episodes ###\n",
    "                    batch_state1 = env.reset(batch_uids)  # numpy array of [bs, state_dim]\n",
    "                    #state = env.reset()\n",
    "                    for j in range(STEP):\n",
    "                        #env.render()#在屏幕上显示画面，不需要\n",
    "                        batch_act_mask = env.batch_action_mask(dropout=args.act_dropout)\n",
    "                        #print('before action, batch_state1:',batch_state1,';',len(batch_state1),';',len(batch_state1[0]))\n",
    "                        action = agent.action(batch_state1,batch_act_mask) # direct action for test\n",
    "                        batch_state2, batch_reward, done = env.batch_step(action)\n",
    "                        batch_state1 = batch_state2\n",
    "                        total_reward += batch_reward\n",
    "                        if done:\n",
    "                            break\n",
    "                ave_reward = total_reward/TEST\n",
    "                if episode % 100 == 0:\n",
    "                    print ('episode: ',episode,'Evaluation Average Reward:',sum(ave_reward)/len(ave_reward))\n",
    "                episode = episode + 1 \n",
    "        #agent.update_target_q_network(epoch)\n",
    "        \n",
    "        #########\n",
    "        if epoch % 100 == 0 or epoch % 290 == 0:\n",
    "            policy_file = '{}/dqn5_model_epoch_{}.ckpt'.format(args.log_dir, epoch)\n",
    "            logger.info(\"Save model to \" + policy_file)\n",
    "            print('epoch:',epoch,',episode:',episode)\n",
    "            torch.save(agent.state_dict(), policy_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load embedding: ./tmp/Amazon_Cellphones/transe_embed.pkl\n",
      "uids: 27879\n",
      "[INFO]  Parameters:['current_net1.weight', 'current_net1.bias', 'current_net2.weight', 'current_net2.bias', 'target_net1.weight', 'target_net1.bias', 'target_net2.weight', 'target_net2.bias']\n",
      "epoch: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:131: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:151: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:153: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:82: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:86: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:87: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:94: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:95: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:75: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode:  100 Evaluation Average Reward: 0.042938327498268335\n",
      "episode:  200 Evaluation Average Reward: 0.023402003130468075\n",
      "episode:  300 Evaluation Average Reward: 0.04071598120863201\n",
      "episode:  400 Evaluation Average Reward: 0.01928118197247386\n",
      "episode:  500 Evaluation Average Reward: 0.018164239246834774\n",
      "episode:  600 Evaluation Average Reward: 0.02113535286625847\n",
      "[INFO]  Save model to ./tmp/Amazon_Cellphones/train_agent/dqn5_model_epoch_0.ckpt\n",
      "epoch: 0 ,episode: 610\n",
      "epoch: 10\n",
      "epoch: 20\n",
      "epoch: 30\n",
      "epoch: 40\n",
      "epoch: 50\n",
      "epoch: 60\n",
      "epoch: 70\n",
      "epoch: 80\n",
      "epoch: 90\n",
      "epoch: 100\n",
      "episode:  700 Evaluation Average Reward: 0.028025530029844956\n",
      "episode:  800 Evaluation Average Reward: 0.025786363496445124\n",
      "episode:  900 Evaluation Average Reward: 0.02942872275962145\n",
      "episode:  1000 Evaluation Average Reward: 0.018672979387338272\n",
      "episode:  1100 Evaluation Average Reward: 0.04457003694551531\n",
      "episode:  1200 Evaluation Average Reward: 0.05009445287287234\n",
      "[INFO]  Save model to ./tmp/Amazon_Cellphones/train_agent/dqn5_model_epoch_100.ckpt\n",
      "epoch: 100 ,episode: 1219\n",
      "epoch: 110\n",
      "epoch: 120\n",
      "epoch: 130\n",
      "epoch: 140\n",
      "epoch: 150\n",
      "epoch: 160\n",
      "epoch: 170\n",
      "epoch: 180\n"
     ]
    }
   ],
   "source": [
    "train(args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load embedding: ./tmp/Amazon_Cellphones/transe_embed.pkl\n",
      "uids: 27879\n",
      "[INFO]  Parameters:['current_net1.weight', 'current_net1.bias', 'current_net2.weight', 'current_net2.bias', 'target_net1.weight', 'target_net1.bias', 'target_net2.weight', 'target_net2.bias']\n",
      "epoch: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:131: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:151: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:153: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:82: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:86: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:87: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:94: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:95: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:75: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode:  100 Evaluation Average Reward: 0.042938327498268335\n",
      "episode:  200 Evaluation Average Reward: 0.023402003130468075\n",
      "episode:  300 Evaluation Average Reward: 0.04071598120863201\n",
      "episode:  400 Evaluation Average Reward: 0.01928118197247386\n",
      "episode:  500 Evaluation Average Reward: 0.018164239246834774\n",
      "episode:  600 Evaluation Average Reward: 0.02113535286625847\n",
      "[INFO]  Save model to ./tmp/Amazon_Cellphones/train_agent/dqn5_model_epoch_0.ckpt\n",
      "epoch: 0 ,episode: 610\n",
      "epoch: 10\n",
      "epoch: 20\n",
      "epoch: 30\n",
      "epoch: 40\n",
      "epoch: 50\n",
      "epoch: 60\n",
      "epoch: 70\n",
      "epoch: 80\n",
      "epoch: 90\n",
      "epoch: 100\n",
      "episode:  700 Evaluation Average Reward: 0.028025530029844956\n",
      "episode:  800 Evaluation Average Reward: 0.025786363496445124\n",
      "episode:  900 Evaluation Average Reward: 0.02942872275962145\n",
      "episode:  1000 Evaluation Average Reward: 0.018672979387338272\n",
      "episode:  1100 Evaluation Average Reward: 0.04457003694551531\n",
      "episode:  1200 Evaluation Average Reward: 0.05009445287287234\n",
      "[INFO]  Save model to ./tmp/Amazon_Cellphones/train_agent/dqn5_model_epoch_100.ckpt\n",
      "epoch: 100 ,episode: 1219\n",
      "epoch: 110\n",
      "epoch: 120\n",
      "epoch: 130\n",
      "epoch: 140\n",
      "epoch: 150\n",
      "epoch: 160\n",
      "epoch: 170\n",
      "epoch: 180\n",
      "epoch: 190\n",
      "epoch: 200\n",
      "episode:  1300 Evaluation Average Reward: 0.026888771216181342\n",
      "episode:  1400 Evaluation Average Reward: 0.04933094367588637\n",
      "episode:  1500 Evaluation Average Reward: 0.04011438122252002\n",
      "episode:  1600 Evaluation Average Reward: 0.027146374666335755\n",
      "episode:  1700 Evaluation Average Reward: 0.027946504269493744\n",
      "episode:  1800 Evaluation Average Reward: 0.028498666612722445\n",
      "[INFO]  Save model to ./tmp/Amazon_Cellphones/train_agent/dqn5_model_epoch_200.ckpt\n",
      "epoch: 200 ,episode: 1828\n",
      "epoch: 210\n",
      "epoch: 220\n",
      "epoch: 230\n",
      "epoch: 240\n",
      "epoch: 250\n",
      "epoch: 260\n",
      "epoch: 270\n",
      "epoch: 280\n",
      "epoch: 290\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "$ Torch: not enough memory: you tried to allocate 0GB. Buy new RAM! at ..\\aten\\src\\TH\\THGeneral.cpp:201",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-aad596905035>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-8-edaeb623ede5>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(args)\u001b[0m\n\u001b[0;32m     37\u001b[0m                 \u001b[1;34m'''select action'''\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m                 \u001b[1;31m#print('shape of batch state1:',batch_state1.shape)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m                 \u001b[0mbatch_act_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0megreedy_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_state1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_act_mask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;31m#batch_act_mask\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m                 \u001b[0mbatch_state2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_reward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_act_idx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m                 \u001b[0mbatch_state2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch_state2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m400\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-69208ac806ad>\u001b[0m in \u001b[0;36megreedy_action\u001b[1;34m(self, batch_state, act_mask)\u001b[0m\n\u001b[0;32m    116\u001b[0m         \u001b[0macts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    117\u001b[0m         \u001b[1;31m# [CAVEAT] If sampled action is out of action_space, choose the first action in action_space.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 118\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mentropy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mentropy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mentropy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    119\u001b[0m         \u001b[1;31m#self.entropy.append(m.entropy())\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    120\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: $ Torch: not enough memory: you tried to allocate 0GB. Buy new RAM! at ..\\aten\\src\\TH\\THGeneral.cpp:201"
     ]
    }
   ],
   "source": [
    "train(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-aad596905035>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'train' is not defined"
     ]
    }
   ],
   "source": [
    "train(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load embedding: ./tmp/Amazon_Cellphones/transe_embed.pkl\n",
      "uids: 27879\n",
      "[INFO]  Parameters:['current_net1.weight', 'current_net1.bias', 'current_net2.weight', 'current_net2.bias', 'target_net1.weight', 'target_net1.bias', 'target_net2.weight', 'target_net2.bias']\n",
      "epoch: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:131: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:151: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:153: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:82: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:86: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:87: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:94: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:95: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:75: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode:  100 Evaluation Average Reward: 0.042938327498268335\n",
      "episode:  200 Evaluation Average Reward: 0.023402003130468075\n",
      "episode:  300 Evaluation Average Reward: 0.04071598120863201\n",
      "episode:  400 Evaluation Average Reward: 0.01928118197247386\n",
      "episode:  500 Evaluation Average Reward: 0.018164239246834774\n",
      "episode:  600 Evaluation Average Reward: 0.02113535286625847\n",
      "[INFO]  Save model to ./tmp/Amazon_Cellphones/train_agent/dqn3_model_epoch_0.ckpt\n",
      "epoch: 0 ,episode: 610\n",
      "epoch: 10\n",
      "[INFO]  Save model to ./tmp/Amazon_Cellphones/train_agent/dqn3_model_epoch_10.ckpt\n",
      "epoch: 10 ,episode: 610\n",
      "epoch: 20\n",
      "[INFO]  Save model to ./tmp/Amazon_Cellphones/train_agent/dqn3_model_epoch_20.ckpt\n",
      "epoch: 20 ,episode: 610\n",
      "epoch: 30\n",
      "[INFO]  Save model to ./tmp/Amazon_Cellphones/train_agent/dqn3_model_epoch_30.ckpt\n",
      "epoch: 30 ,episode: 610\n",
      "epoch: 40\n",
      "[INFO]  Save model to ./tmp/Amazon_Cellphones/train_agent/dqn3_model_epoch_40.ckpt\n",
      "epoch: 40 ,episode: 610\n",
      "epoch: 50\n",
      "[INFO]  Save model to ./tmp/Amazon_Cellphones/train_agent/dqn3_model_epoch_50.ckpt\n",
      "epoch: 50 ,episode: 610\n",
      "epoch: 60\n",
      "[INFO]  Save model to ./tmp/Amazon_Cellphones/train_agent/dqn3_model_epoch_60.ckpt\n",
      "epoch: 60 ,episode: 610\n",
      "epoch: 70\n",
      "[INFO]  Save model to ./tmp/Amazon_Cellphones/train_agent/dqn3_model_epoch_70.ckpt\n",
      "epoch: 70 ,episode: 610\n",
      "epoch: 80\n",
      "[INFO]  Save model to ./tmp/Amazon_Cellphones/train_agent/dqn3_model_epoch_80.ckpt\n",
      "epoch: 80 ,episode: 610\n",
      "epoch: 90\n",
      "[INFO]  Save model to ./tmp/Amazon_Cellphones/train_agent/dqn3_model_epoch_90.ckpt\n",
      "epoch: 90 ,episode: 610\n",
      "epoch: 100\n",
      "episode:  700 Evaluation Average Reward: 0.028025530029844956\n",
      "episode:  800 Evaluation Average Reward: 0.025786363496445124\n",
      "episode:  900 Evaluation Average Reward: 0.02942872275962145\n",
      "episode:  1000 Evaluation Average Reward: 0.018672979387338272\n",
      "episode:  1100 Evaluation Average Reward: 0.04457003694551531\n",
      "episode:  1200 Evaluation Average Reward: 0.05009445287287234\n",
      "[INFO]  Save model to ./tmp/Amazon_Cellphones/train_agent/dqn3_model_epoch_100.ckpt\n",
      "epoch: 100 ,episode: 1219\n",
      "epoch: 110\n",
      "[INFO]  Save model to ./tmp/Amazon_Cellphones/train_agent/dqn3_model_epoch_110.ckpt\n",
      "epoch: 110 ,episode: 1219\n",
      "epoch: 120\n",
      "[INFO]  Save model to ./tmp/Amazon_Cellphones/train_agent/dqn3_model_epoch_120.ckpt\n",
      "epoch: 120 ,episode: 1219\n",
      "epoch: 130\n",
      "[INFO]  Save model to ./tmp/Amazon_Cellphones/train_agent/dqn3_model_epoch_130.ckpt\n",
      "epoch: 130 ,episode: 1219\n",
      "epoch: 140\n",
      "[INFO]  Save model to ./tmp/Amazon_Cellphones/train_agent/dqn3_model_epoch_140.ckpt\n",
      "epoch: 140 ,episode: 1219\n",
      "epoch: 150\n",
      "[INFO]  Save model to ./tmp/Amazon_Cellphones/train_agent/dqn3_model_epoch_150.ckpt\n",
      "epoch: 150 ,episode: 1219\n",
      "epoch: 160\n",
      "[INFO]  Save model to ./tmp/Amazon_Cellphones/train_agent/dqn3_model_epoch_160.ckpt\n",
      "epoch: 160 ,episode: 1219\n",
      "epoch: 170\n",
      "[INFO]  Save model to ./tmp/Amazon_Cellphones/train_agent/dqn3_model_epoch_170.ckpt\n",
      "epoch: 170 ,episode: 1219\n",
      "epoch: 180\n",
      "[INFO]  Save model to ./tmp/Amazon_Cellphones/train_agent/dqn3_model_epoch_180.ckpt\n",
      "epoch: 180 ,episode: 1219\n",
      "epoch: 190\n",
      "[INFO]  Save model to ./tmp/Amazon_Cellphones/train_agent/dqn3_model_epoch_190.ckpt\n",
      "epoch: 190 ,episode: 1219\n",
      "epoch: 200\n",
      "episode:  1300 Evaluation Average Reward: 0.026888771216181342\n",
      "episode:  1400 Evaluation Average Reward: 0.04933094367588637\n",
      "episode:  1500 Evaluation Average Reward: 0.04011438122252002\n",
      "episode:  1600 Evaluation Average Reward: 0.027146374666335755\n",
      "episode:  1700 Evaluation Average Reward: 0.027946504269493744\n",
      "episode:  1800 Evaluation Average Reward: 0.028498666612722445\n",
      "[INFO]  Save model to ./tmp/Amazon_Cellphones/train_agent/dqn3_model_epoch_200.ckpt\n",
      "epoch: 200 ,episode: 1828\n",
      "epoch: 210\n",
      "[INFO]  Save model to ./tmp/Amazon_Cellphones/train_agent/dqn3_model_epoch_210.ckpt\n",
      "epoch: 210 ,episode: 1828\n",
      "epoch: 220\n",
      "[INFO]  Save model to ./tmp/Amazon_Cellphones/train_agent/dqn3_model_epoch_220.ckpt\n",
      "epoch: 220 ,episode: 1828\n",
      "epoch: 230\n",
      "[INFO]  Save model to ./tmp/Amazon_Cellphones/train_agent/dqn3_model_epoch_230.ckpt\n",
      "epoch: 230 ,episode: 1828\n",
      "epoch: 240\n",
      "[INFO]  Save model to ./tmp/Amazon_Cellphones/train_agent/dqn3_model_epoch_240.ckpt\n",
      "epoch: 240 ,episode: 1828\n",
      "epoch: 250\n",
      "[INFO]  Save model to ./tmp/Amazon_Cellphones/train_agent/dqn3_model_epoch_250.ckpt\n",
      "epoch: 250 ,episode: 1828\n",
      "epoch: 260\n",
      "[INFO]  Save model to ./tmp/Amazon_Cellphones/train_agent/dqn3_model_epoch_260.ckpt\n",
      "epoch: 260 ,episode: 1828\n"
     ]
    }
   ],
   "source": [
    "train(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
