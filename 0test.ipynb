{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import argparse\n",
    "from math import log\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "from collections import namedtuple\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.distributions import Categorical\n",
    "import threading\n",
    "from functools import reduce\n",
    "\n",
    "from knowledge_graph import KnowledgeGraph\n",
    "from kg_env import BatchKGEnvironment\n",
    "\n",
    "from collections import deque\n",
    "from collections import namedtuple\n",
    "#from train_agent import ActorCritic\n",
    "from train_DQN import DQN\n",
    "############\n",
    "from utils import *\n",
    "#agent = DQN(env,env.state_dim,env.act_dim,gamma = args.gamma,hidden_sizes = args.hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "index = torch.tensor([20,10,25,39,5,12])  # 这些index的值，对应着point中的索引\n",
    "point = torch.ones(2,40)\n",
    "# 然后用index中的数对point进行各点取值\n",
    "result_x = point[0,index.long()]\n",
    "result_y = point[1,index] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                ve stui l           ve stui l                            love stui love s    love stui love s                     i love stui love stui love stui love stui                  i love stui love stui love stui love stui l                i love stui love stui love stui love stui lov                love stui love stui love stui love stui love               love stui love stui love stui love stui love                ove stui love stui love stui love stui love s               ve stui love stui love stui love stui love st               e stui love stui love stui love stui love stu                stui love stui love stui love stui love stu                  ui love stui love stui love stui love stu                   i love stui love stui love stui love stui                     ove stui love stui love stui love stu                        e stui love stui love stui love stu                          stui love stui love stui love stu                             i love stui love stui love st                                 ove stui love stui love s                                      stui love stui love                                           i love stui lov                                                ve stui l                                                      tui                                                          i                                                                                         "
     ]
    }
   ],
   "source": [
    "for y in range(12,-12,-1):\n",
    "    for x in range(-30,30):\n",
    "        if ((x*0.05)**2+(y*0.1)**2-1)**3-(x*0.05)**2*(y*0.1)**3<=0:\n",
    "            print('\\n'.join(''.join('i love study'[(x-y)%10])),end='')\n",
    "        else:\n",
    "            print(' ',end='')\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[''.join([(\n",
    "           if((x*0.05)**2+(y*0.1)**2-1)**3-(x*0.05)**2*(y*0.1)**3<=0:\n",
    "               'ilovestudy'[(x-y)%10]\n",
    "           else ' ')\n",
    "                          for x in range(-30,30)])\n",
    "                 for y in range(12,-12,-1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                estudyilo           estudyilo               \n",
      "            lovestudyilovestu   lovestudyilovestu           \n",
      "          ilovestudyilovestudyilovestudyilovestudyi         \n",
      "         ilovestudyilovestudyilovestudyilovestudyilo        \n",
      "        ilovestudyilovestudyilovestudyilovestudyilove       \n",
      "        lovestudyilovestudyilovestudyilovestudyiloves       \n",
      "        ovestudyilovestudyilovestudyilovestudyilovest       \n",
      "        vestudyilovestudyilovestudyilovestudyilovestu       \n",
      "        estudyilovestudyilovestudyilovestudyilovestud       \n",
      "        studyilovestudyilovestudyilovestudyilovestudy       \n",
      "         udyilovestudyilovestudyilovestudyilovestudy        \n",
      "          yilovestudyilovestudyilovestudyilovestudy         \n",
      "          ilovestudyilovestudyilovestudyilovestudyi         \n",
      "            vestudyilovestudyilovestudyilovestudy           \n",
      "             studyilovestudyilovestudyilovestudy            \n",
      "              udyilovestudyilovestudyilovestudy             \n",
      "                ilovestudyilovestudyilovestud               \n",
      "                  vestudyilovestudyilovestu                 \n",
      "                    tudyilovestudyilovest                   \n",
      "                       ilovestudyilove                      \n",
      "                          estudyilo                         \n",
      "                             dyi                            \n",
      "                              i                             \n",
      "                                                            \n"
     ]
    }
   ],
   "source": [
    "print('\\n'.join([''.join([('ilovestudy'[(x-y)%10]if((x*0.05)**2+(y*0.1)**2-1)**3-(x*0.05)**2*(y*0.1)**3<=0 else' ')\n",
    "                          for x in range(-30,30)])\n",
    "                 for y in range(12,-12,-1)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(topk_matches, test_user_products):\n",
    "    \"\"\"Compute metrics for predicted recommendations.\n",
    "    Args:\n",
    "        topk_matches: a list or dict of product ids in ascending order.\n",
    "    \"\"\"\n",
    "    invalid_users = []\n",
    "    # Compute metrics\n",
    "    precisions, recalls, ndcgs, hits = [], [], [], []\n",
    "    test_user_idxs = list(test_user_products.keys())\n",
    "    for uid in test_user_idxs:\n",
    "        if uid not in topk_matches or len(topk_matches[uid]) < 10:\n",
    "            invalid_users.append(uid)\n",
    "            continue\n",
    "        pred_list, rel_set = topk_matches[uid][::-1], test_user_products[uid]\n",
    "        if len(pred_list) == 0:\n",
    "            continue\n",
    "\n",
    "        dcg = 0.0\n",
    "        hit_num = 0.0\n",
    "        for i in range(len(pred_list)):\n",
    "            if pred_list[i] in rel_set:\n",
    "                dcg += 1. / (log(i + 2) / log(2))\n",
    "                hit_num += 1\n",
    "        # idcg\n",
    "        idcg = 0.0\n",
    "        for i in range(min(len(rel_set), len(pred_list))):\n",
    "            idcg += 1. / (log(i + 2) / log(2))\n",
    "            \n",
    "        ndcg = dcg / idcg\n",
    "        recall = hit_num / len(rel_set)\n",
    "        precision = hit_num / len(pred_list)\n",
    "        hit = 1.0 if hit_num > 0.0 else 0.0\n",
    "\n",
    "        ndcgs.append(ndcg)\n",
    "        recalls.append(recall)\n",
    "        precisions.append(precision)\n",
    "        hits.append(hit)\n",
    "\n",
    "    avg_precision = np.mean(precisions) * 100\n",
    "    avg_recall = np.mean(recalls) * 100\n",
    "    avg_ndcg = np.mean(ndcgs) * 100\n",
    "    avg_hit = np.mean(hits) * 100\n",
    "    print('NDCG={:.3f} |  Recall={:.3f} | HR={:.3f} | Precision={:.3f} | Invalid users={}'.format(\n",
    "            avg_ndcg, avg_recall, avg_hit, avg_precision, len(invalid_users)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_beam_search(env, model, uids, device, topk=[25, 5, 1]):\n",
    "    def _batch_acts_to_masks(batch_acts):\n",
    "        batch_masks = []\n",
    "        for acts in batch_acts:\n",
    "            num_acts = len(acts)\n",
    "            act_mask = np.zeros(model.action_dim, dtype=np.uint8)\n",
    "            #act_mask = np.zeros(model.act_dim, dtype=np.uint8)\n",
    "            act_mask[:num_acts] = 1\n",
    "            batch_masks.append(act_mask)\n",
    "        return np.vstack(batch_masks)\n",
    "    \n",
    "    #env KGenv\n",
    "    state_pool = env.reset(uids)  # numpy of [bs, dim]\n",
    "    path_pool = env._batch_path  # list of list, size=bs\n",
    "    probs_pool = [[] for _ in uids] #每个uid有一个列表\n",
    "    model.eval()\n",
    "    for hop in range(3):\n",
    "        state_tensor = torch.FloatTensor(state_pool).to(device)\n",
    "        acts_pool = env._batch_get_actions(path_pool, False)  # list of list, size=bs\n",
    "        actmask_pool = _batch_acts_to_masks(acts_pool)  # numpy of [bs, dim]\n",
    "        actmask_tensor = torch.ByteTensor(actmask_pool).to(device)\n",
    "        #probs, _ = \n",
    "        model((state_tensor, actmask_tensor))  # Tensor of [bs, act_dim]\n",
    "        #probs 相当于 self.Q_value\n",
    "        probs = model.Q_value + actmask_tensor.float()\n",
    "        #probs = probs + actmask_tensor.float()  # In order to differ from masked actions\n",
    "        topk_probs, topk_idxs = torch.topk(probs, topk[hop], dim=1)  # LongTensor of [bs, k]\n",
    "        topk_idxs = topk_idxs.detach().cpu().numpy()\n",
    "        topk_probs = topk_probs.detach().cpu().numpy()\n",
    "\n",
    "        new_path_pool, new_probs_pool = [], []\n",
    "        for row in range(topk_idxs.shape[0]):\n",
    "            path = path_pool[row]\n",
    "            probs = probs_pool[row]\n",
    "            for idx, p in zip(topk_idxs[row], topk_probs[row]):\n",
    "                if idx >= len(acts_pool[row]):  # act idx is invalid\n",
    "                    continue\n",
    "                relation, next_node_id = acts_pool[row][idx]  # (relation, next_node_id)\n",
    "                if relation == SELF_LOOP:\n",
    "                    next_node_type = path[-1][1]\n",
    "                else:\n",
    "                    next_node_type = KG_RELATION[path[-1][1]][relation]\n",
    "                new_path = path + [(relation, next_node_type, next_node_id)]\n",
    "                new_path_pool.append(new_path)\n",
    "                new_probs_pool.append(probs + [p])\n",
    "        path_pool = new_path_pool\n",
    "        probs_pool = new_probs_pool\n",
    "        if hop < 2:\n",
    "            state_pool = env._batch_get_state(path_pool)\n",
    "\n",
    "    return path_pool, probs_pool\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_paths(policy_file, path_file, args):\n",
    "    print('Predicting paths...')\n",
    "    env = BatchKGEnvironment(args.dataset, args.max_acts, max_path_len=args.max_path_len, state_history=args.state_history)\n",
    "    pretrain_sd = torch.load(policy_file)\n",
    "    model = DQN(env,env.state_dim, env.act_dim, gamma=args.gamma, hidden_sizes=args.hidden).to(args.device)\n",
    "    #agent = DQN(env,env.state_dim,env.act_dim,gamma = args.gamma,hidden_sizes = args.hidden)\n",
    "    '''???'''\n",
    "    model_sd = model.state_dict()\n",
    "    model_sd.update(pretrain_sd)\n",
    "    model.load_state_dict(model_sd)\n",
    "\n",
    "    test_labels = load_labels(args.dataset, 'test')\n",
    "    test_uids = list(test_labels.keys())\n",
    "\n",
    "    batch_size = 16\n",
    "    start_idx = 0\n",
    "    all_paths, all_probs = [], []\n",
    "    pbar = tqdm(total=len(test_uids)) #进度条\n",
    "    while start_idx < len(test_uids):\n",
    "        end_idx = min(start_idx + batch_size, len(test_uids))\n",
    "        batch_uids = test_uids[start_idx:end_idx]\n",
    "        paths, probs = batch_beam_search(env, model, batch_uids, args.device, topk=args.topk)\n",
    "        all_paths.extend(paths)\n",
    "        all_probs.extend(probs)\n",
    "        start_idx = end_idx\n",
    "        pbar.update(batch_size)\n",
    "    predicts = {'paths': all_paths, 'probs': all_probs}\n",
    "    pickle.dump(predicts, open(path_file, 'wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_paths(path_file, train_labels, test_labels):\n",
    "    embeds = load_embed(args.dataset)\n",
    "    user_embeds = embeds[USER]\n",
    "    purchase_embeds = embeds[PURCHASE][0]\n",
    "    product_embeds = embeds[PRODUCT]\n",
    "    scores = np.dot(user_embeds + purchase_embeds, product_embeds.T)\n",
    "\n",
    "    # 1) Get all valid paths for each user, compute path score and path probability.\n",
    "    results = pickle.load(open(path_file, 'rb'))\n",
    "    pred_paths = {uid: {} for uid in test_labels}\n",
    "    for path, probs in zip(results['paths'], results['probs']):\n",
    "        if path[-1][1] != PRODUCT:\n",
    "            continue\n",
    "        uid = path[0][2]\n",
    "        \n",
    "        if uid not in pred_paths:\n",
    "            continue\n",
    "        pid = path[-1][2]\n",
    "        if pid not in pred_paths[uid]:\n",
    "            pred_paths[uid][pid] = []\n",
    "        path_score = scores[uid][pid]\n",
    "        path_prob = reduce(lambda x, y: x * y, probs)\n",
    "        pred_paths[uid][pid].append((path_score, path_prob, path))\n",
    "\n",
    "    # 2) Pick best path for each user-product pair, also remove pid if it is in train set.\n",
    "    best_pred_paths = {}\n",
    "    for uid in pred_paths:\n",
    "        train_pids = set(train_labels[uid])\n",
    "        best_pred_paths[uid] = []\n",
    "        for pid in pred_paths[uid]:\n",
    "            if pid in train_pids:\n",
    "                continue\n",
    "            # Get the path with highest probability\n",
    "            sorted_path = sorted(pred_paths[uid][pid], key=lambda x: x[1], reverse=True)\n",
    "            best_pred_paths[uid].append(sorted_path[0])\n",
    "\n",
    "    # 3) Compute top 10 recommended products for each user.\n",
    "    sort_by = 'score'\n",
    "    pred_labels = {}\n",
    "    for uid in best_pred_paths:\n",
    "        if sort_by == 'score':\n",
    "            sorted_path = sorted(best_pred_paths[uid], key=lambda x: (x[0], x[1]), reverse=True)\n",
    "        elif sort_by == 'prob':\n",
    "            sorted_path = sorted(best_pred_paths[uid], key=lambda x: (x[1], x[0]), reverse=True)\n",
    "        top10_pids = [p[-1][2] for _, _, p in sorted_path[:10]]  # from largest to smallest\n",
    "        # add up to 10 pids if not enough\n",
    "        if args.add_products and len(top10_pids) < 10:\n",
    "            train_pids = set(train_labels[uid])\n",
    "            cand_pids = np.argsort(scores[uid])\n",
    "            for cand_pid in cand_pids[::-1]:\n",
    "                if cand_pid in train_pids or cand_pid in top10_pids:\n",
    "                    continue\n",
    "                top10_pids.append(cand_pid)\n",
    "                if len(top10_pids) >= 10:\n",
    "                    break\n",
    "        # end of add\n",
    "        pred_labels[uid] = top10_pids[::-1]  # change order to from smallest to largest!\n",
    "\n",
    "    evaluate(pred_labels, test_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(args):\n",
    "    policy_file = args.log_dir + '/dqn3_model_epoch_{}.ckpt'.format(args.epochs)\n",
    "    path_file = args.log_dir + '/dqn3_paths_epoch{}.pkl'.format(args.epochs)\n",
    "    '''\n",
    "    policy_file = '{}/policy_model_epoch_{}.ckpt'.format(args.log_dir, epoch)\n",
    "    logger.info(\"Save model to \" + policy_file)\n",
    "    torch.save(model.state_dict(), policy_file)\n",
    "    '''\n",
    "    train_labels = load_labels(args.dataset, 'train')\n",
    "    test_labels = load_labels(args.dataset, 'test')\n",
    "\n",
    "    if args.run_path:\n",
    "        predict_paths(policy_file, path_file, args)\n",
    "    if args.run_eval:\n",
    "        evaluate_paths(path_file, train_labels, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting paths...\n",
      "Load embedding: ./tmp/Amazon_Cellphones/transe_embed.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "27888it [1:23:58,  6.68it/s]                                                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load embedding: ./tmp/Amazon_Cellphones/transe_embed.pkl\n",
      "NDCG=0.886 |  Recall=1.452 | HR=2.330 | Precision=0.236 | Invalid users=4485\n"
     ]
    }
   ],
   "source": [
    "#if __name__ == '__main__':\n",
    "boolean = lambda x: (str(x).lower() == 'true')\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--dataset', type=str, default=BEAUTY, help='One of {cloth, beauty, cell, cd}')\n",
    "parser.add_argument('--name', type=str, default='train_agent', help='directory name.')\n",
    "parser.add_argument('--seed', type=int, default=123, help='random seed.')\n",
    "parser.add_argument('--gpu', type=str, default='0', help='gpu device.')\n",
    "parser.add_argument('--epochs', type=int, default=50, help='num of epochs.')\n",
    "parser.add_argument('--max_acts', type=int, default=250, help='Max number of actions.')\n",
    "parser.add_argument('--max_path_len', type=int, default=3, help='Max path length.')\n",
    "parser.add_argument('--gamma', type=float, default=0.99, help='reward discount factor.')\n",
    "parser.add_argument('--state_history', type=int, default=1, help='state history length')\n",
    "parser.add_argument('--hidden', type=int, nargs='*', default=[512, 256], help='number of samples')\n",
    "parser.add_argument('--add_products', type=boolean, default=False, help='Add predicted products up to 10')\n",
    "parser.add_argument('--topk', type=int, nargs='*', default=[25, 5, 1], help='number of samples')\n",
    "parser.add_argument('--run_path', type=boolean, default=True, help='Generate predicted path? (takes long time)')\n",
    "parser.add_argument('--run_eval', type=boolean, default=True, help='Run evaluation?')\n",
    "args = parser.parse_args(['--dataset',CELL])\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = args.gpu\n",
    "args.device = torch.device('cuda:0') if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "args.log_dir = TMP_DIR[args.dataset] + '/' + args.name\n",
    "test(args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    # DQN Agent\n",
    "    def __init__(self, env, state_dim, act_dim, gamma=0.99,hidden_sizes=[512,256]):\n",
    "        super(DQN, self).__init__()\n",
    "        self.replay_buffer = deque() #双向队列 可以从左append些什么\n",
    "        # init some parameters\n",
    "        self.time_step = 0\n",
    "        self.epsilon = INITIAL_EPSILON\n",
    "        self.state_dim = 400 #state_dim\n",
    "        self.action_dim = 251       \n",
    "        \n",
    "        self.current_net1 = nn.Linear(self.state_dim,20)\n",
    "        self.current_net2 = nn.Linear(20,self.action_dim)\n",
    "        \n",
    "        self.target_net1 = nn.Linear(self.state_dim,20)\n",
    "        self.target_net2 = nn.Linear(20,self.action_dim)\n",
    "        \n",
    "        '''state_dim 要改!!!'''\n",
    "         #act_dim\n",
    "        #env.action_space.n\n",
    "        \n",
    "        self.saved_actions = []        \n",
    "        self.rewards = []\n",
    "        self.entropy = []\n",
    "\n",
    "    \n",
    "    def forward(self,inputs):\n",
    "        state,act_mask = inputs\n",
    "        #act_mask = a_m\n",
    "        h_1 = self.current_net1(state)\n",
    "        \n",
    "        h_1 = F.dropout(F.relu(h_1))\n",
    "        self.Q_value = self.current_net2(h_1)\n",
    "        \n",
    "        h_2 = self.target_net1(state)\n",
    "        h_2 = F.dropout(F.relu(h_2))\n",
    "        self.target_Q_value = self.target_net2(h_2)\n",
    "\n",
    "        #optimizer = optim.Adam(model.parameters(), lr=args.lr)\n",
    "        \n",
    "\n",
    "    def perceive(self,state,action,reward,next_state,done,optimizer):\n",
    "        one_hot_action = np.zeros(self.action_dim)\n",
    "        #应该要 32*action_dim\n",
    "        one_hot_action[action] = 1\n",
    "        self.replay_buffer.append((state,one_hot_action,reward,next_state,done))\n",
    "        self.optimizer = optimizer\n",
    "        if len(self.replay_buffer) > REPLAY_SIZE:\n",
    "            self.replay_buffer.popleft()\n",
    "\n",
    "        if len(self.replay_buffer) > BATCH_SIZE:\n",
    "            self.train_Q_network()\n",
    "    \n",
    "    \n",
    "    def train_Q_network(self):\n",
    "        self.time_step += 1\n",
    "        # Step 1: obtain random minibatch from replay memory\n",
    "        minibatch = random.sample(self.replay_buffer,BATCH_SIZE)\n",
    "        state_batch = [data[0] for data in minibatch]\n",
    "        action_batch = [data[1] for data in minibatch]\n",
    "        reward_batch = [data[2] for data in minibatch]\n",
    "        next_state_batch = [data[3] for data in minibatch]\n",
    "        reward_batch = torch.tensor(reward_batch)\n",
    "        # Step 2: calculate y\n",
    "        y_batch = []\n",
    "        current_Q_batch = self.Q_value\n",
    "        #max_action_next = np.argmax(current_Q_batch, axis=1)\n",
    "        max_action_next=torch.argmax(current_Q_batch ,dim=1)\n",
    "        target_Q_batch = self.target_Q_value\n",
    "\n",
    "        for i in range(0,BATCH_SIZE):\n",
    "            done = minibatch[i][4]\n",
    "            if i == 0:\n",
    "                if done:\n",
    "                    y_batch = torch.tensor(reward_batch[i])\n",
    "                else:\n",
    "                    #print('2:',len(target_Q_batch),len(target_Q_batch[0]))\n",
    "                    target_Q_value = target_Q_batch[i, max_action_next[i]] #[i, i, max_action_next[i]]\n",
    "                    #print('type:',type(reward_batch),type(target_Q_value))\n",
    "                    #print(reward_batch)\n",
    "                    #print(target_Q_value)\n",
    "                    y_batch = torch.tensor(torch.tensor(reward_batch[i]) + GAMMA * target_Q_value)\n",
    "            else:\n",
    "                if done:\n",
    "                    #y_batch = np.append(y_batch,reward_batch[i])\n",
    "                    y_batch = torch.tensor(y_batch)\n",
    "                    app = torch.tensor(reward_batch[i])\n",
    "                    y_batch = torch.cat((y_batch,app),0)\n",
    "                else :\n",
    "                    #print('1:',len(target_Q_batch),len(target_Q_batch[0]))\n",
    "                    #print(len(target_Q_batch[0][0]))\n",
    "                    target_Q_value = target_Q_batch[i, max_action_next[i]] #[i, i,max_action_next[i]]\n",
    "                    #print('type:',type(reward_batch),type(target_Q_value))\n",
    "                    y_batch = torch.tensor(y_batch)\n",
    "                    app = torch.tensor(reward_batch[i]+GAMMA*target_Q_value)\n",
    "                    y_batch = torch.cat((y_batch,app),0)\n",
    "                    #y_batch = np.append(y_batch, reward_batch[i] + GAMMA * target_Q_value)\n",
    "                    \n",
    "        y_batch = y_batch.reshape(32,32)\n",
    "        #print(self.entropy)\n",
    "        self.loss = self.entropy.mean()\n",
    "        self.optimizer.zero_grad()\n",
    "        self.loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "    def egreedy_action(self,batch_state,act_mask):#batch_state = state\n",
    "        state = torch.FloatTensor(batch_state)#.to(device)  # Tensor [bs, state_dim]\n",
    "        act_mask = torch.FloatTensor(act_mask)#.to(device)  # Tensor of [bs, act_dim]\n",
    "        self((state,act_mask))\n",
    "        ''''''\n",
    "        act_mask = act_mask.type(torch.uint8)        \n",
    "        self.Q_value[1-act_mask] = -999999.0\n",
    "        self.Q_value = F.softmax(self.Q_value, dim=-1)\n",
    "        \n",
    "        m = Categorical(self.Q_value)#加起来应该不是1？说不准\n",
    "        acts = m.sample()\n",
    "        # [CAVEAT] If sampled action is out of action_space, choose the first action in action_space.\n",
    "        self.entropy = torch.cat((torch.tensor(self.entropy),m.entropy()))\n",
    "        #self.entropy.append(m.entropy())\n",
    "        \n",
    "        if random.random() <= self.epsilon:\n",
    "            acts = []\n",
    "            self.epsilon -= (INITIAL_EPSILON - FINAL_EPSILON) / 10000\n",
    "            for i in range(BATCH_SIZE):\n",
    "                acts.append(random.randint(0,self.action_dim - 1))\n",
    "            acts_log = torch.tensor(acts)\n",
    "            self.saved_actions.append(SavedAction(m.log_prob(acts_log)))#, value))\n",
    "            return acts #要return一行，不能只有一个值吧!!\n",
    "        else:\n",
    "            self.epsilon -= (INITIAL_EPSILON - FINAL_EPSILON) / 10000\n",
    "            acts_log = torch.tensor(acts)\n",
    "            self.saved_actions.append(SavedAction(m.log_prob(acts_log)))#, value))\n",
    "            return acts #np.argmax(Q_value)\n",
    "\n",
    "        \n",
    "    def action(self,batch_state,act_mask):\n",
    "        batch_state = torch.tensor(batch_state,dtype=torch.float32)\n",
    "        act_mask = torch.tensor(act_mask,dtype=torch.float32)\n",
    "        \n",
    "        self((batch_state,act_mask))\n",
    "        #self.forward(batch_state,act_mask)\n",
    "        ''''''\n",
    "        act_mask = act_mask.type(torch.uint8)\n",
    "        #self((batch_state, act_mask))        \n",
    "        self.Q_value[1-act_mask] = -999999.0\n",
    "        self.Q_value = F.softmax(self.Q_value, dim=-1)\n",
    "\n",
    "        m = Categorical(self.Q_value)#加起来应该不是1？说不准 \n",
    "        acts = m.sample()\n",
    "        # [CAVEAT] If sampled action is out of action_space, choose the first action in action_space.\n",
    "        self.entropy = torch.cat((torch.tensor(self.entropy),m.entropy()))\n",
    "        #self.entropy.append(m.entropy())\n",
    "        acts_log = torch.tensor(acts)\n",
    "        self.saved_actions.append(SavedAction(m.log_prob(acts_log)))#, value))\n",
    "        return acts"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
