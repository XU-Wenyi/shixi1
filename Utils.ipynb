{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import sys\n",
    "import random\n",
    "import pickle\n",
    "import logging\n",
    "import logging.handlers\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "import torch\n",
    "\n",
    "\n",
    "# Dataset names.\n",
    "BEAUTY = 'beauty'\n",
    "CELL = 'cell'\n",
    "CLOTH = 'cloth'\n",
    "CD = 'cd'\n",
    "\n",
    "# Dataset directories.\n",
    "DATASET_DIR = {\n",
    "    BEAUTY: './data/Amazon_Beauty',\n",
    "    CELL: './data/Amazon_Cellphones',\n",
    "    CLOTH: './data/Amazon_Clothing',\n",
    "    CD: './data/Amazon_CDs',\n",
    "}\n",
    "\n",
    "# Model result directories.\n",
    "TMP_DIR = {\n",
    "    BEAUTY: './tmp/Amazon_Beauty',\n",
    "    CELL: './tmp/Amazon_Cellphones',\n",
    "    CLOTH: './tmp/Amazon_Clothing',\n",
    "    CD: './tmp/Amazon_CDs',\n",
    "}\n",
    "\n",
    "# Label files.\n",
    "LABELS = {\n",
    "    BEAUTY: (TMP_DIR[BEAUTY] + '/train_label.pkl', TMP_DIR[BEAUTY] + '/test_label.pkl'),\n",
    "    CLOTH: (TMP_DIR[CLOTH] + '/train_label.pkl', TMP_DIR[CLOTH] + '/test_label.pkl'),\n",
    "    CELL: (TMP_DIR[CELL] + '/train_label.pkl', TMP_DIR[CELL] + '/test_label.pkl'),\n",
    "    CD: (TMP_DIR[CD] + '/train_label.pkl', TMP_DIR[CD] + '/test_label.pkl')\n",
    "}\n",
    "\n",
    "\n",
    "# Entities\n",
    "USER = 'user'\n",
    "PRODUCT = 'product'\n",
    "WORD = 'word'\n",
    "RPRODUCT = 'related_product'\n",
    "BRAND = 'brand'\n",
    "CATEGORY = 'category'\n",
    "\n",
    "\n",
    "# Relations\n",
    "PURCHASE = 'purchase'\n",
    "MENTION = 'mentions'\n",
    "DESCRIBED_AS = 'described_as'\n",
    "PRODUCED_BY = 'produced_by'\n",
    "BELONG_TO = 'belongs_to'\n",
    "ALSO_BOUGHT = 'also_bought'\n",
    "ALSO_VIEWED = 'also_viewed'\n",
    "BOUGHT_TOGETHER = 'bought_together'\n",
    "SELF_LOOP = 'self_loop'  # only for kg env\n",
    "\n",
    "KG_RELATION = {\n",
    "    USER: {\n",
    "        PURCHASE: PRODUCT,\n",
    "        MENTION: WORD,\n",
    "    },\n",
    "    WORD: {\n",
    "        MENTION: USER,\n",
    "        DESCRIBED_AS: PRODUCT,\n",
    "    },\n",
    "    PRODUCT: {\n",
    "        PURCHASE: USER,\n",
    "        DESCRIBED_AS: WORD,\n",
    "        PRODUCED_BY: BRAND,\n",
    "        BELONG_TO: CATEGORY,\n",
    "        ALSO_BOUGHT: RPRODUCT,\n",
    "        ALSO_VIEWED: RPRODUCT,\n",
    "        BOUGHT_TOGETHER: RPRODUCT,\n",
    "    },\n",
    "    BRAND: {\n",
    "        PRODUCED_BY: PRODUCT,\n",
    "    },\n",
    "    CATEGORY: {\n",
    "        BELONG_TO: PRODUCT,\n",
    "    },\n",
    "    RPRODUCT: {\n",
    "        ALSO_BOUGHT: PRODUCT,\n",
    "        ALSO_VIEWED: PRODUCT,\n",
    "        BOUGHT_TOGETHER: PRODUCT,\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "PATH_PATTERN = {\n",
    "    # length = 3\n",
    "    1: ((None, USER), (MENTION, WORD), (DESCRIBED_AS, PRODUCT)),\n",
    "    # length = 4\n",
    "    11: ((None, USER), (PURCHASE, PRODUCT), (PURCHASE, USER), (PURCHASE, PRODUCT)),\n",
    "    12: ((None, USER), (PURCHASE, PRODUCT), (DESCRIBED_AS, WORD), (DESCRIBED_AS, PRODUCT)),\n",
    "    13: ((None, USER), (PURCHASE, PRODUCT), (PRODUCED_BY, BRAND), (PRODUCED_BY, PRODUCT)),\n",
    "    14: ((None, USER), (PURCHASE, PRODUCT), (BELONG_TO, CATEGORY), (BELONG_TO, PRODUCT)),\n",
    "    15: ((None, USER), (PURCHASE, PRODUCT), (ALSO_BOUGHT, RPRODUCT), (ALSO_BOUGHT, PRODUCT)),\n",
    "    16: ((None, USER), (PURCHASE, PRODUCT), (ALSO_VIEWED, RPRODUCT), (ALSO_VIEWED, PRODUCT)),\n",
    "    17: ((None, USER), (PURCHASE, PRODUCT), (BOUGHT_TOGETHER, RPRODUCT), (BOUGHT_TOGETHER, PRODUCT)),\n",
    "    18: ((None, USER), (MENTION, WORD), (MENTION, USER), (PURCHASE, PRODUCT)),\n",
    "}\n",
    "\n",
    "\n",
    "def get_entities():\n",
    "    return list(KG_RELATION.keys())\n",
    "\n",
    "\n",
    "def get_relations(entity_head):\n",
    "    return list(KG_RELATION[entity_head].keys())\n",
    "\n",
    "\n",
    "def get_entity_tail(entity_head, relation):\n",
    "    return KG_RELATION[entity_head][relation]\n",
    "\n",
    "\n",
    "def compute_tfidf_fast(vocab, docs):\n",
    "    \"\"\"Compute TFIDF scores for all vocabs.\n",
    "\n",
    "    Args:\n",
    "        docs: list of list of integers, e.g. [[0,0,1], [1,2,0,1]]\n",
    "\n",
    "    Returns:\n",
    "        sp.csr_matrix, [num_docs, num_vocab]\n",
    "    \"\"\"\n",
    "    # (1) Compute term frequency in each doc.\n",
    "    data, indices, indptr = [], [], [0]\n",
    "    for d in docs:\n",
    "        term_count = {}\n",
    "        for term_idx in d:\n",
    "            if term_idx not in term_count:\n",
    "                term_count[term_idx] = 1\n",
    "            else:\n",
    "                term_count[term_idx] += 1\n",
    "        indices.extend(term_count.keys())\n",
    "        data.extend(term_count.values())\n",
    "        indptr.append(len(indices))\n",
    "    tf = sp.csr_matrix((data, indices, indptr), dtype=int, shape=(len(docs), len(vocab)))\n",
    "\n",
    "    # (2) Compute normalized tfidf for each term/doc.\n",
    "    transformer = TfidfTransformer(smooth_idf=True)\n",
    "    tfidf = transformer.fit_transform(tf)\n",
    "    return tfidf\n",
    "\n",
    "\n",
    "def get_logger(logname):\n",
    "    logger = logging.getLogger(logname)\n",
    "    logger.setLevel(logging.DEBUG)\n",
    "    formatter = logging.Formatter('[%(levelname)s]  %(message)s')\n",
    "    ch = logging.StreamHandler(sys.stdout)\n",
    "    ch.setFormatter(formatter)\n",
    "    logger.addHandler(ch)\n",
    "    fh = logging.handlers.RotatingFileHandler(logname, mode='w')\n",
    "    fh.setFormatter(formatter)\n",
    "    logger.addHandler(fh)\n",
    "    return logger\n",
    "\n",
    "\n",
    "def set_random_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "\n",
    "def save_dataset(dataset, dataset_obj):\n",
    "    dataset_file = TMP_DIR[dataset] + '/dataset.pkl'\n",
    "    with open(dataset_file, 'wb') as f:\n",
    "        pickle.dump(dataset_obj, f)\n",
    "\n",
    "\n",
    "def load_dataset(dataset):\n",
    "    dataset_file = TMP_DIR[dataset] + '/dataset.pkl'\n",
    "    print('dataset_file:',dataset_file)\n",
    "    dataset_obj = pickle.load(open(dataset_file, 'rb'))\n",
    "    return dataset_obj\n",
    "\n",
    "\n",
    "def save_labels(dataset, labels, mode='train'):\n",
    "    if mode == 'train':\n",
    "        label_file = LABELS[dataset][0]\n",
    "    elif mode == 'test':\n",
    "        label_file = LABELS[dataset][1]\n",
    "    else:\n",
    "        raise Exception('mode should be one of {train, test}.')\n",
    "    with open(label_file, 'wb') as f:\n",
    "        pickle.dump(labels, f)\n",
    "\n",
    "\n",
    "def load_labels(dataset, mode='train'):\n",
    "    if mode == 'train':\n",
    "        label_file = LABELS[dataset][0]\n",
    "    elif mode == 'test':\n",
    "        label_file = LABELS[dataset][1]\n",
    "    else:\n",
    "        raise Exception('mode should be one of {train, test}.')\n",
    "    user_products = pickle.load(open(label_file, 'rb'))\n",
    "    return user_products\n",
    "\n",
    "\n",
    "def save_embed(dataset, embed):\n",
    "    embed_file = '{}/transe_embed.pkl'.format(TMP_DIR[dataset])\n",
    "    pickle.dump(embed, open(embed_file, 'wb'))\n",
    "\n",
    "\n",
    "def load_embed(dataset):\n",
    "    embed_file = '{}/transe_embed.pkl'.format(TMP_DIR[dataset])\n",
    "    print('Load embedding:', embed_file)\n",
    "    embed = pickle.load(open(embed_file, 'rb'))\n",
    "    return embed\n",
    "\n",
    "\n",
    "def save_kg(dataset, kg):\n",
    "    kg_file = TMP_DIR[dataset] + '/kg.pkl'\n",
    "    pickle.dump(kg, open(kg_file, 'wb'))\n",
    "\n",
    "\n",
    "def load_kg(dataset):\n",
    "    kg_file = TMP_DIR[dataset] + '/kg.pkl'\n",
    "    kg = pickle.load(open(kg_file, 'rb'))\n",
    "    return kg\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
