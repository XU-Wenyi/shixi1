{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import argparse\n",
    "from collections import namedtuple\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "from knowledge_graph import KnowledgeGraph #用于preprocess 预处理\n",
    "from kg_env import BatchKGEnvironment\n",
    "from utils import *\n",
    "\n",
    "logger = None\n",
    "\n",
    "SavedAction = namedtuple('SavedAction', ['log_prob', 'value'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, state_dim, act_dim, gamma=0.99, hidden_sizes=[512, 256]):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        self.state_dim = state_dim\n",
    "        self.act_dim = act_dim\n",
    "        self.gamma = gamma\n",
    "\n",
    "        self.l1 = nn.Linear(state_dim, hidden_sizes[0])\n",
    "        self.l2 = nn.Linear(hidden_sizes[0], hidden_sizes[1])\n",
    "        self.actor = nn.Linear(hidden_sizes[1], act_dim)\n",
    "        self.critic = nn.Linear(hidden_sizes[1], 1)\n",
    "\n",
    "        self.saved_actions = []\n",
    "        self.rewards = []\n",
    "        self.entropy = []\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        state, act_mask = inputs  # state: [bs, state_dim], act_mask: [bs, act_dim] //batch_size\n",
    "        print('forward-state:',state.size())\n",
    "        print('act_mask:',act_mask.size())\n",
    "\n",
    "\n",
    "        '''\n",
    "        forward-state: tensor([[ 0.0045,  0.0047, -0.0045,  ...,  0.0000,  0.0000,  0.0000],\n",
    "        [-0.0020, -0.0027,  0.0026,  ...,  0.0000,  0.0000,  0.0000],\n",
    "        [ 0.0040, -0.0014, -0.0023,  ...,  0.0000,  0.0000,  0.0000],\n",
    "        ...,\n",
    "        [-0.0024, -0.0019,  0.0045,  ...,  0.0000,  0.0000,  0.0000],\n",
    "        [ 0.0018,  0.0029,  0.0010,  ...,  0.0000,  0.0000,  0.0000],\n",
    "        [ 0.0026,  0.0008,  0.0005,  ...,  0.0000,  0.0000,  0.0000]])\n",
    "        len: 32\n",
    "        size: torch.Size([32, 400])\n",
    "        '''\n",
    "        x = self.l1(state)\n",
    "        print('x = l1(state):',x.size())\n",
    "        \n",
    "        x = F.dropout(F.elu(x), p=0.5) #dropout是为了解决大网络但是小训练集的过拟合问题，但是可以一般性的避免过拟合。\n",
    "        print('x = dropout:',x.size())\n",
    "        \n",
    "\n",
    "        out = self.l2(x)\n",
    "        print('out=l2(x):',out.size())\n",
    "        \n",
    "        x = F.dropout(F.elu(out), p=0.5)\n",
    "        '''\n",
    "        print('x=drop(out):',x)\n",
    "        print('size:',x.size())\n",
    "        '''\n",
    "\n",
    "        \n",
    "        actor_logits = self.actor(x)\n",
    "        print('actor_logits:',actor_logits.size())\n",
    "        \n",
    "        actor_logits[1 - act_mask] = -999999.0\n",
    "        '''\n",
    "        import torch\n",
    "        a = torch.tensor([2,3,4,5])\n",
    "        b = torch.tensor([0,-1,-1,0])\n",
    "        a[1-b]->[3,4,4,3]\n",
    "        \n",
    "        '''\n",
    "        print('actor_logits_masked:',actor_logits.size())\n",
    "        act_probs = F.softmax(actor_logits, dim=-1)  # Tensor of [bs, act_dim]\n",
    "        \n",
    "        print('act_probs:',act_probs.size())\n",
    "        #print('...............')\n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "        state_values = self.critic(x)  # Tensor of [bs, 1]\n",
    "        return act_probs, state_values\n",
    "    \n",
    "    def select_action(self, batch_state, batch_act_mask, device):\n",
    "        state = torch.FloatTensor(batch_state).to(device)  # Tensor [bs, state_dim]\n",
    "        act_mask = torch.ByteTensor(batch_act_mask).to(device)  # Tensor of [bs, act_dim]\n",
    "        print('state:',len(state),len(state[0]))\n",
    "        print('act_mask:',len(act_mask),len(act_mask[0]))\n",
    "        probs, value = self((state, act_mask))  # act_probs: [bs, act_dim], state_value: [bs, 1] forward\n",
    "        #prob = act_probs\n",
    "        #value = state_values\n",
    "        #self(())调用forward\n",
    "        '''\n",
    "        print('probs:',probs) #act_prob\n",
    "        print('len:',len(probs))\n",
    "        print('size:',probs.size())\n",
    "        print('value:',value) #state_value\n",
    "        print('len:',len(value))\n",
    "        print('size:',value.size())\n",
    "        '''\n",
    "\n",
    "        \n",
    "        m = Categorical(probs)#32*251\n",
    "        #若probs = [0.2 0.2 0.3 0.3]\n",
    "        #那么 m.sample 取 0 1 2 3 的概率就对应probs\n",
    "        acts = m.sample()  # Tensor of [bs, ], requires_grad=False\n",
    "        '''\n",
    "        print('acts extracted from m:')\n",
    "        print(acts)\n",
    "        print('size:',acts.size())#[32]\n",
    "        '''\n",
    "        \n",
    "        \n",
    "        # [CAVEAT] If sampled action is out of action_space, choose the first action in action_space.\n",
    "        valid_idx = act_mask.gather(1, acts.view(-1, 1)).view(-1)#torch.gather(input=act_mask,dim=1,acts.view(-1,1))\n",
    "        #.view(-1行,1列)，-1 代表随便几行\n",
    "        acts[valid_idx == 0] = 0\n",
    "        print('select_act:',acts.cpu().numpy().tolist())\n",
    "        #print(len(acts.cpu().numpy().tolist()[0]))\n",
    "        \n",
    "        #print('***********************************')\n",
    "        self.saved_actions.append(SavedAction(m.log_prob(acts), value))\n",
    "        \n",
    "        self.entropy.append(m.entropy())\n",
    "        return acts.cpu().numpy().tolist()\n",
    "\n",
    "    def update(self, optimizer, device, ent_weight):\n",
    "        if len(self.rewards) <= 0:\n",
    "            del self.rewards[:]\n",
    "            del self.saved_actions[:]\n",
    "            del self.entropy[:]\n",
    "            return 0.0, 0.0, 0.0\n",
    "\n",
    "        batch_rewards = np.vstack(self.rewards).T  # numpy array of [bs, #steps]\n",
    "        batch_rewards = torch.FloatTensor(batch_rewards).to(device)\n",
    "        \n",
    "        num_steps = batch_rewards.shape[1]\n",
    "        for i in range(1, num_steps):\n",
    "            batch_rewards[:, num_steps - i - 1] += self.gamma * batch_rewards[:, num_steps - i]\n",
    "\n",
    "        actor_loss = 0\n",
    "        critic_loss = 0\n",
    "        entropy_loss = 0\n",
    "        \n",
    "        for i in range(0, num_steps):\n",
    "            log_prob, value = self.saved_actions[i]  # log_prob: Tensor of [bs, ], value: Tensor of [bs, 1]\n",
    "            advantage = batch_rewards[:, i] - value.squeeze(1)  # Tensor of [bs, ]\n",
    "            actor_loss += -log_prob * advantage.detach()  # Tensor of [bs, ]\n",
    "            critic_loss += advantage.pow(2)  # Tensor of [bs, ]\n",
    "            entropy_loss += -self.entropy[i]  # Tensor of [bs, ]\n",
    "        actor_loss = actor_loss.mean()\n",
    "        critic_loss = critic_loss.mean()\n",
    "        entropy_loss = entropy_loss.mean()\n",
    "        loss = actor_loss + critic_loss + ent_weight * entropy_loss\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        del self.rewards[:]\n",
    "        del self.saved_actions[:]\n",
    "        del self.entropy[:]\n",
    "        \n",
    "        #loss, ploss, vloss, eloss\n",
    "        return loss.item(), actor_loss.item(), critic_loss.item(), entropy_loss.item()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ACDataLoader(object):\n",
    "    def __init__(self, uids, batch_size):\n",
    "        self.uids = np.array(uids)\n",
    "        self.num_users = len(uids)\n",
    "        self.batch_size = batch_size\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self._rand_perm = np.random.permutation(self.num_users)\n",
    "        self._start_idx = 0\n",
    "        self._has_next = True\n",
    "\n",
    "    def has_next(self):\n",
    "        return self._has_next\n",
    "\n",
    "    def get_batch(self):\n",
    "        if not self._has_next:\n",
    "            return None\n",
    "        # Multiple users per batch\n",
    "        end_idx = min(self._start_idx + self.batch_size, self.num_users)\n",
    "        batch_idx = self._rand_perm[self._start_idx:end_idx]\n",
    "        batch_uids = self.uids[batch_idx]\n",
    "        self._has_next = self._has_next and end_idx < self.num_users\n",
    "        self._start_idx = end_idx\n",
    "        return batch_uids.tolist()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args):\n",
    "    env = BatchKGEnvironment(args.dataset, args.max_acts, max_path_len=args.max_path_len, state_history=args.state_history)\n",
    "    uids = list(env.kg(USER).keys())\n",
    "    dataloader = ACDataLoader(uids, args.batch_size)\n",
    "    model = ActorCritic(env.state_dim, env.act_dim, gamma=args.gamma, hidden_sizes=args.hidden).to(args.device)\n",
    "    logger.info('Parameters:' + str([i[0] for i in model.named_parameters()]))\n",
    "    optimizer = optim.Adam(model.parameters(), lr=args.lr)\n",
    "    total_losses, total_plosses, total_vlosses, total_entropy, total_rewards = [], [], [], [], []\n",
    "    step = 0\n",
    "    model.train()\n",
    "    for epoch in range(1, args.epochs + 1):\n",
    "        ### Start epoch ###\n",
    "        dataloader.reset()\n",
    "        while dataloader.has_next():\n",
    "            batch_uids = dataloader.get_batch()\n",
    "            print('batch_uids:',len(batch_uids))\n",
    "            print(batch_uids)\n",
    "            ### Start batch episodes ###\n",
    "            batch_state = env.reset(batch_uids)  # numpy array of [bs, state_dim]\n",
    "            print('batch_state:',batch_state.shape)\n",
    "            done = False\n",
    "            while not done:\n",
    "                batch_act_mask = env.batch_action_mask(dropout=args.act_dropout)  # numpy array of size [bs, act_dim]\n",
    "                '''select action'''\n",
    "                batch_act_idx = model.select_action(batch_state, batch_act_mask, args.device)  # int\n",
    "                batch_state, batch_reward, done = env.batch_step(batch_act_idx)\n",
    "                model.rewards.append(batch_reward)\n",
    "            ### End of episodes ###\n",
    "            lr = args.lr * max(1e-4, 1.0 - float(step) / (args.epochs * len(uids) / args.batch_size))\n",
    "            for pg in optimizer.param_groups:\n",
    "                pg['lr'] = lr\n",
    "            # Update policy\n",
    "            total_rewards.append(np.sum(model.rewards))\n",
    "            loss, ploss, vloss, eloss = model.update(optimizer, args.device, args.ent_weight)\n",
    "            total_losses.append(loss)\n",
    "            total_plosses.append(ploss)\n",
    "            total_vlosses.append(vloss)\n",
    "            total_entropy.append(eloss)\n",
    "            step += 1\n",
    "            # Report performance\n",
    "            if step > 0 and step % 100 == 0:\n",
    "                avg_reward = np.mean(total_rewards) / args.batch_size\n",
    "                avg_loss = np.mean(total_losses)\n",
    "                avg_ploss = np.mean(total_plosses)\n",
    "                avg_vloss = np.mean(total_vlosses)\n",
    "                avg_entropy = np.mean(total_entropy)\n",
    "                total_losses, total_plosses, total_vlosses, total_entropy, total_rewards = [], [], [], [], []\n",
    "                logger.info('epoch/step={:d}/{:d}'.format(epoch, step) + ' | loss={:.5f}'.format(avg_loss) + \n",
    "                            ' | ploss={:.5f}'.format(avg_ploss) + ' | vloss={:.5f}'.format(avg_vloss) + \n",
    "                            ' | entropy={:.5f}'.format(avg_entropy) +' | reward={:.5f}'.format(avg_reward))\n",
    "        ### END of epoch ###\n",
    "\n",
    "        policy_file = '{}/policy_model_epoch_{}.ckpt'.format(args.log_dir, epoch)\n",
    "        logger.info(\"Save model to \" + policy_file)\n",
    "        torch.save(model.state_dict(), policy_file)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--dataset', type=str, default=BEAUTY, help='One of {clothing, cell, beauty, cd}')\n",
    "parser.add_argument('--name', type=str, default='train_agent', help='directory name.')\n",
    "parser.add_argument('--seed', type=int, default=123, help='random seed.')\n",
    "parser.add_argument('--gpu', type=str, default='0', help='gpu device.')\n",
    "parser.add_argument('--epochs', type=int, default=50, help='Max number of epochs.')\n",
    "parser.add_argument('--batch_size', type=int, default=32, help='batch size.')\n",
    "parser.add_argument('--lr', type=float, default=1e-4, help='learning rate.')\n",
    "parser.add_argument('--max_acts', type=int, default=250, help='Max number of actions.')\n",
    "#\n",
    "parser.add_argument('--max_path_len', type=int, default=3, help='Max path length.')\n",
    "parser.add_argument('--gamma', type=float, default=0.99, help='reward discount factor.')\n",
    "parser.add_argument('--ent_weight', type=float, default=1e-3, help='weight factor for entropy loss')\n",
    "parser.add_argument('--act_dropout', type=float, default=0.5, help='action dropout rate.')\n",
    "parser.add_argument('--state_history', type=int, default=1, help='state history length')\n",
    "parser.add_argument('--hidden', type=int, nargs='*', default=[512, 256], help='number of samples')\n",
    "args = parser.parse_args(['--dataset',CELL])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]  Namespace(act_dropout=0.5, batch_size=32, dataset='cell', device='cpu', ent_weight=0.001, epochs=50, gamma=0.99, gpu='0', hidden=[512, 256], log_dir='./tmp/Amazon_Cellphones/train_agent', lr=0.0001, max_acts=250, max_path_len=3, name='train_agent', seed=123, state_history=1)\n"
     ]
    }
   ],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = args.gpu\n",
    "args.device = torch.device('cuda:0') if torch.cuda.is_available() else 'cpu'\n",
    "args.log_dir = '{}/{}'.format(TMP_DIR[args.dataset], args.name)\n",
    "if not os.path.isdir(args.log_dir):\n",
    "    os.makedirs(args.log_dir)\n",
    "\n",
    "global logger\n",
    "logger = get_logger(args.log_dir + '/train_log.txt')\n",
    "logger.info(args)\n",
    "\n",
    "set_random_seed(args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load embedding: ./tmp/Amazon_Cellphones/transe_embed.pkl\n",
      "[INFO]  Parameters:['l1.weight', 'l1.bias', 'l2.weight', 'l2.bias', 'actor.weight', 'actor.bias', 'critic.weight', 'critic.bias']\n",
      "batch_uids: 32\n",
      "[8639, 14910, 10146, 23298, 8040, 18852, 11923, 8586, 6391, 848, 4884, 4475, 16206, 27283, 6009, 6830, 24060, 4372, 11941, 7039, 26419, 24441, 16744, 13233, 12395, 805, 7496, 11959, 6365, 9942, 17829, 22348]\n",
      "batch_state: (32, 400)\n",
      "state: 32 400\n",
      "act_mask: 32 251\n",
      "forward-state: torch.Size([32, 400])\n",
      "act_mask: torch.Size([32, 251])\n",
      "x = l1(state): torch.Size([32, 512])\n",
      "x = dropout: torch.Size([32, 512])\n",
      "out=l2(x): torch.Size([32, 256])\n",
      "actor_logits: torch.Size([32, 251])\n",
      "actor_logits_masked: torch.Size([32, 251])\n",
      "act_probs: torch.Size([32, 251])\n",
      "select_act: [2, 3, 6, 3, 3, 9, 9, 0, 10, 4, 3, 3, 1, 16, 1, 10, 10, 7, 1, 6, 10, 9, 0, 4, 6, 5, 5, 0, 5, 1, 5, 12]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object of type 'int' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-aad596905035>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-4-29a7a32ca7d6>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(args)\u001b[0m\n\u001b[0;32m     24\u001b[0m                 \u001b[1;34m'''select action'''\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m                 \u001b[0mbatch_act_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_act_mask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# int\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'batch_act_idx'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_act_idx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_act_idx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m                 \u001b[0mbatch_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_reward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_act_idx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrewards\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_reward\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: object of type 'int' has no len()"
     ]
    }
   ],
   "source": [
    "train(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load embedding: ./tmp/Amazon_Cellphones/transe_embed.pkl\n",
      "[INFO]  Parameters:['l1.weight', 'l1.bias', 'l2.weight', 'l2.bias', 'actor.weight', 'actor.bias', 'critic.weight', 'critic.bias']\n",
      "[INFO]  epoch/step=1/100 | loss=0.03575 | ploss=-0.00155 | vloss=0.04593 | entropy=-8.63200 | reward=0.03642\n",
      "[INFO]  epoch/step=1/200 | loss=0.02726 | ploss=-0.00746 | vloss=0.04332 | entropy=-8.59969 | reward=0.03380\n",
      "[INFO]  epoch/step=1/300 | loss=0.02667 | ploss=-0.00788 | vloss=0.04316 | entropy=-8.60557 | reward=0.03493\n",
      "[INFO]  epoch/step=1/400 | loss=0.00226 | ploss=-0.03246 | vloss=0.04333 | entropy=-8.61099 | reward=0.03769\n",
      "[INFO]  epoch/step=1/500 | loss=0.01120 | ploss=-0.02822 | vloss=0.04792 | entropy=-8.50753 | reward=0.04253\n",
      "[INFO]  epoch/step=1/600 | loss=0.02836 | ploss=-0.02225 | vloss=0.05907 | entropy=-8.46007 | reward=0.04586\n",
      "[INFO]  epoch/step=1/700 | loss=0.02852 | ploss=-0.01655 | vloss=0.05347 | entropy=-8.39805 | reward=0.04727\n",
      "[INFO]  epoch/step=1/800 | loss=0.00033 | ploss=-0.05028 | vloss=0.05891 | entropy=-8.29677 | reward=0.05081\n",
      "[INFO]  Save model to ./tmp/Amazon_Cellphones/train_agent/policy_model_epoch_1.ckpt\n",
      "[INFO]  epoch/step=2/900 | loss=0.04048 | ploss=-0.00844 | vloss=0.05705 | entropy=-8.13180 | reward=0.04955\n",
      "[INFO]  epoch/step=2/1000 | loss=0.01338 | ploss=-0.04700 | vloss=0.06844 | entropy=-8.05720 | reward=0.05822\n",
      "[INFO]  epoch/step=2/1100 | loss=0.02184 | ploss=-0.03321 | vloss=0.06294 | entropy=-7.89746 | reward=0.05727\n",
      "[INFO]  epoch/step=2/1200 | loss=0.04577 | ploss=-0.01669 | vloss=0.07032 | entropy=-7.86719 | reward=0.06271\n",
      "[INFO]  epoch/step=2/1300 | loss=0.02807 | ploss=-0.04013 | vloss=0.07578 | entropy=-7.58443 | reward=0.06705\n",
      "[INFO]  epoch/step=2/1400 | loss=0.06132 | ploss=-0.00516 | vloss=0.07406 | entropy=-7.59075 | reward=0.06764\n",
      "[INFO]  epoch/step=2/1500 | loss=0.05506 | ploss=-0.01419 | vloss=0.07670 | entropy=-7.44647 | reward=0.06947\n",
      "[INFO]  epoch/step=2/1600 | loss=0.05824 | ploss=-0.01817 | vloss=0.08378 | entropy=-7.37361 | reward=0.07336\n",
      "[INFO]  epoch/step=2/1700 | loss=0.07657 | ploss=-0.00612 | vloss=0.08996 | entropy=-7.27216 | reward=0.07910\n",
      "[INFO]  Save model to ./tmp/Amazon_Cellphones/train_agent/policy_model_epoch_2.ckpt\n",
      "[INFO]  epoch/step=3/1800 | loss=0.07428 | ploss=0.00135 | vloss=0.08015 | entropy=-7.22731 | reward=0.07370\n",
      "[INFO]  epoch/step=3/1900 | loss=0.07087 | ploss=-0.01080 | vloss=0.08869 | entropy=-7.01204 | reward=0.08164\n",
      "[INFO]  epoch/step=3/2000 | loss=0.08813 | ploss=-0.00508 | vloss=0.09985 | entropy=-6.63632 | reward=0.08771\n",
      "[INFO]  epoch/step=3/2100 | loss=0.09391 | ploss=0.00480 | vloss=0.09536 | entropy=-6.25749 | reward=0.08892\n",
      "[INFO]  epoch/step=3/2200 | loss=0.12258 | ploss=0.02985 | vloss=0.09851 | entropy=-5.77438 | reward=0.09652\n",
      "[INFO]  epoch/step=3/2300 | loss=0.12721 | ploss=0.02544 | vloss=0.10723 | entropy=-5.46318 | reward=0.10271\n",
      "[INFO]  epoch/step=3/2400 | loss=0.13106 | ploss=0.03332 | vloss=0.10304 | entropy=-5.29874 | reward=0.10091\n",
      "[INFO]  epoch/step=3/2500 | loss=0.12506 | ploss=0.02687 | vloss=0.10340 | entropy=-5.21857 | reward=0.10315\n",
      "[INFO]  epoch/step=3/2600 | loss=0.14268 | ploss=0.03005 | vloss=0.11776 | entropy=-5.12492 | reward=0.11155\n",
      "[INFO]  Save model to ./tmp/Amazon_Cellphones/train_agent/policy_model_epoch_3.ckpt\n",
      "[INFO]  epoch/step=4/2700 | loss=0.14860 | ploss=0.04205 | vloss=0.11153 | entropy=-4.96901 | reward=0.11051\n",
      "[INFO]  epoch/step=4/2800 | loss=0.12697 | ploss=0.01764 | vloss=0.11406 | entropy=-4.73896 | reward=0.11432\n",
      "[INFO]  epoch/step=4/2900 | loss=0.14107 | ploss=0.03389 | vloss=0.11190 | entropy=-4.71965 | reward=0.11197\n",
      "[INFO]  epoch/step=4/3000 | loss=0.13050 | ploss=0.01771 | vloss=0.11732 | entropy=-4.53868 | reward=0.11653\n",
      "[INFO]  epoch/step=4/3100 | loss=0.16879 | ploss=0.04969 | vloss=0.12348 | entropy=-4.37758 | reward=0.12398\n",
      "[INFO]  epoch/step=4/3200 | loss=0.15608 | ploss=0.03955 | vloss=0.12080 | entropy=-4.27864 | reward=0.12125\n",
      "[INFO]  epoch/step=4/3300 | loss=0.14593 | ploss=0.03137 | vloss=0.11864 | entropy=-4.07825 | reward=0.11872\n",
      "[INFO]  epoch/step=4/3400 | loss=0.13774 | ploss=0.02807 | vloss=0.11366 | entropy=-3.99281 | reward=0.11809\n",
      "[INFO]  Save model to ./tmp/Amazon_Cellphones/train_agent/policy_model_epoch_4.ckpt\n",
      "[INFO]  epoch/step=5/3500 | loss=0.16195 | ploss=0.03554 | vloss=0.13033 | entropy=-3.92168 | reward=0.12602\n",
      "[INFO]  epoch/step=5/3600 | loss=0.14438 | ploss=0.02339 | vloss=0.12493 | entropy=-3.93631 | reward=0.12512\n",
      "[INFO]  epoch/step=5/3700 | loss=0.16814 | ploss=0.03759 | vloss=0.13431 | entropy=-3.76131 | reward=0.12868\n",
      "[INFO]  epoch/step=5/3800 | loss=0.13944 | ploss=0.01969 | vloss=0.12343 | entropy=-3.67071 | reward=0.12335\n",
      "[INFO]  epoch/step=5/3900 | loss=0.17730 | ploss=0.04556 | vloss=0.13537 | entropy=-3.62367 | reward=0.12985\n",
      "[INFO]  epoch/step=5/4000 | loss=0.15513 | ploss=0.02336 | vloss=0.13540 | entropy=-3.63588 | reward=0.13140\n",
      "[INFO]  epoch/step=5/4100 | loss=0.16774 | ploss=0.03731 | vloss=0.13396 | entropy=-3.53304 | reward=0.13063\n",
      "[INFO]  epoch/step=5/4200 | loss=0.18036 | ploss=0.04633 | vloss=0.13753 | entropy=-3.50074 | reward=0.13703\n",
      "[INFO]  epoch/step=5/4300 | loss=0.17478 | ploss=0.03872 | vloss=0.13941 | entropy=-3.34947 | reward=0.13589\n",
      "[INFO]  Save model to ./tmp/Amazon_Cellphones/train_agent/policy_model_epoch_5.ckpt\n",
      "[INFO]  epoch/step=6/4400 | loss=0.16990 | ploss=0.03888 | vloss=0.13439 | entropy=-3.37513 | reward=0.12993\n",
      "[INFO]  epoch/step=6/4500 | loss=0.17854 | ploss=0.04136 | vloss=0.14062 | entropy=-3.44441 | reward=0.13248\n",
      "[INFO]  epoch/step=6/4600 | loss=0.18241 | ploss=0.03669 | vloss=0.14914 | entropy=-3.42310 | reward=0.13453\n",
      "[INFO]  epoch/step=6/4700 | loss=0.17428 | ploss=0.02110 | vloss=0.15651 | entropy=-3.32389 | reward=0.14141\n",
      "[INFO]  epoch/step=6/4800 | loss=0.19169 | ploss=0.02972 | vloss=0.16521 | entropy=-3.23699 | reward=0.14660\n",
      "[INFO]  epoch/step=6/4900 | loss=0.19672 | ploss=0.02780 | vloss=0.17215 | entropy=-3.23325 | reward=0.15648\n",
      "[INFO]  epoch/step=6/5000 | loss=0.18733 | ploss=0.01147 | vloss=0.17898 | entropy=-3.11804 | reward=0.15731\n",
      "[INFO]  epoch/step=6/5100 | loss=0.18942 | ploss=0.01225 | vloss=0.18021 | entropy=-3.04198 | reward=0.15781\n",
      "[INFO]  epoch/step=6/5200 | loss=0.18308 | ploss=-0.00230 | vloss=0.18850 | entropy=-3.11579 | reward=0.15827\n",
      "[INFO]  Save model to ./tmp/Amazon_Cellphones/train_agent/policy_model_epoch_6.ckpt\n",
      "[INFO]  epoch/step=7/5300 | loss=0.19799 | ploss=0.00358 | vloss=0.19732 | entropy=-2.90818 | reward=0.16571\n",
      "[INFO]  epoch/step=7/5400 | loss=0.19714 | ploss=-0.00226 | vloss=0.20242 | entropy=-3.01777 | reward=0.16735\n",
      "[INFO]  epoch/step=7/5500 | loss=0.19214 | ploss=-0.00297 | vloss=0.19803 | entropy=-2.92271 | reward=0.16696\n",
      "[INFO]  epoch/step=7/5600 | loss=0.19694 | ploss=-0.00790 | vloss=0.20777 | entropy=-2.93079 | reward=0.17107\n",
      "[INFO]  epoch/step=7/5700 | loss=0.19834 | ploss=-0.00445 | vloss=0.20573 | entropy=-2.93366 | reward=0.16434\n",
      "[INFO]  epoch/step=7/5800 | loss=0.21950 | ploss=0.00016 | vloss=0.22219 | entropy=-2.84720 | reward=0.17918\n",
      "[INFO]  epoch/step=7/5900 | loss=0.19743 | ploss=-0.01810 | vloss=0.21827 | entropy=-2.75001 | reward=0.17735\n",
      "[INFO]  epoch/step=7/6000 | loss=0.20530 | ploss=-0.01758 | vloss=0.22562 | entropy=-2.74306 | reward=0.17855\n",
      "[INFO]  epoch/step=7/6100 | loss=0.21787 | ploss=-0.00743 | vloss=0.22798 | entropy=-2.67691 | reward=0.17994\n",
      "[INFO]  Save model to ./tmp/Amazon_Cellphones/train_agent/policy_model_epoch_7.ckpt\n",
      "[INFO]  epoch/step=8/6200 | loss=0.21230 | ploss=-0.01730 | vloss=0.23223 | entropy=-2.62985 | reward=0.18318\n",
      "[INFO]  epoch/step=8/6300 | loss=0.22204 | ploss=-0.00071 | vloss=0.22527 | entropy=-2.52326 | reward=0.17940\n",
      "[INFO]  epoch/step=8/6400 | loss=0.23816 | ploss=0.00540 | vloss=0.23535 | entropy=-2.59382 | reward=0.18251\n",
      "[INFO]  epoch/step=8/6500 | loss=0.22194 | ploss=-0.02300 | vloss=0.24748 | entropy=-2.54466 | reward=0.19576\n",
      "[INFO]  epoch/step=8/6600 | loss=0.23117 | ploss=-0.02367 | vloss=0.25738 | entropy=-2.54525 | reward=0.19688\n",
      "[INFO]  epoch/step=8/6700 | loss=0.22282 | ploss=-0.02175 | vloss=0.24701 | entropy=-2.43484 | reward=0.19367\n",
      "[INFO]  epoch/step=8/6800 | loss=0.20018 | ploss=-0.02829 | vloss=0.23090 | entropy=-2.43356 | reward=0.18145\n",
      "[INFO]  epoch/step=8/6900 | loss=0.23193 | ploss=-0.01401 | vloss=0.24842 | entropy=-2.47253 | reward=0.19419\n",
      "[INFO]  Save model to ./tmp/Amazon_Cellphones/train_agent/policy_model_epoch_8.ckpt\n",
      "[INFO]  epoch/step=9/7000 | loss=0.23753 | ploss=-0.01356 | vloss=0.25346 | entropy=-2.36725 | reward=0.19304\n",
      "[INFO]  epoch/step=9/7100 | loss=0.22004 | ploss=-0.02680 | vloss=0.24919 | entropy=-2.35384 | reward=0.19028\n",
      "[INFO]  epoch/step=9/7200 | loss=0.21523 | ploss=-0.03203 | vloss=0.24959 | entropy=-2.34002 | reward=0.18603\n",
      "[INFO]  epoch/step=9/7300 | loss=0.21685 | ploss=-0.02839 | vloss=0.24756 | entropy=-2.31462 | reward=0.18767\n",
      "[INFO]  epoch/step=9/7400 | loss=0.22561 | ploss=-0.03283 | vloss=0.26073 | entropy=-2.28747 | reward=0.19793\n",
      "[INFO]  epoch/step=9/7500 | loss=0.20218 | ploss=-0.04740 | vloss=0.25177 | entropy=-2.18667 | reward=0.19291\n",
      "[INFO]  epoch/step=9/7600 | loss=0.22158 | ploss=-0.03099 | vloss=0.25474 | entropy=-2.17736 | reward=0.19537\n",
      "[INFO]  epoch/step=9/7700 | loss=0.22868 | ploss=-0.03148 | vloss=0.26230 | entropy=-2.13425 | reward=0.19694\n",
      "[INFO]  epoch/step=9/7800 | loss=0.23345 | ploss=-0.04252 | vloss=0.27803 | entropy=-2.05930 | reward=0.21166\n",
      "[INFO]  Save model to ./tmp/Amazon_Cellphones/train_agent/policy_model_epoch_9.ckpt\n",
      "[INFO]  epoch/step=10/7900 | loss=0.21603 | ploss=-0.04476 | vloss=0.26287 | entropy=-2.08202 | reward=0.19611\n",
      "[INFO]  epoch/step=10/8000 | loss=0.23491 | ploss=-0.03055 | vloss=0.26752 | entropy=-2.05893 | reward=0.20220\n",
      "[INFO]  epoch/step=10/8100 | loss=0.23388 | ploss=-0.02878 | vloss=0.26473 | entropy=-2.07084 | reward=0.20105\n",
      "[INFO]  epoch/step=10/8200 | loss=0.20044 | ploss=-0.05180 | vloss=0.25428 | entropy=-2.04240 | reward=0.19277\n",
      "[INFO]  epoch/step=10/8300 | loss=0.21990 | ploss=-0.04235 | vloss=0.26430 | entropy=-2.04049 | reward=0.19721\n",
      "[INFO]  epoch/step=10/8400 | loss=0.22934 | ploss=-0.03841 | vloss=0.26973 | entropy=-1.97688 | reward=0.20412\n",
      "[INFO]  epoch/step=10/8500 | loss=0.21355 | ploss=-0.05112 | vloss=0.26668 | entropy=-2.00834 | reward=0.19779\n",
      "[INFO]  epoch/step=10/8600 | loss=0.22980 | ploss=-0.02700 | vloss=0.25883 | entropy=-2.02284 | reward=0.19305\n",
      "[INFO]  epoch/step=10/8700 | loss=0.22601 | ploss=-0.02703 | vloss=0.25504 | entropy=-1.99562 | reward=0.19071\n",
      "[INFO]  Save model to ./tmp/Amazon_Cellphones/train_agent/policy_model_epoch_10.ckpt\n",
      "[INFO]  epoch/step=11/8800 | loss=0.24480 | ploss=-0.02785 | vloss=0.27462 | entropy=-1.97180 | reward=0.20533\n",
      "[INFO]  epoch/step=11/8900 | loss=0.22597 | ploss=-0.02953 | vloss=0.25746 | entropy=-1.96084 | reward=0.19596\n",
      "[INFO]  epoch/step=11/9000 | loss=0.23097 | ploss=-0.03599 | vloss=0.26887 | entropy=-1.89800 | reward=0.19939\n",
      "[INFO]  epoch/step=11/9100 | loss=0.23479 | ploss=-0.04011 | vloss=0.27683 | entropy=-1.92493 | reward=0.20460\n",
      "[INFO]  epoch/step=11/9200 | loss=0.23858 | ploss=-0.02969 | vloss=0.27012 | entropy=-1.85475 | reward=0.19995\n",
      "[INFO]  epoch/step=11/9300 | loss=0.23362 | ploss=-0.04363 | vloss=0.27908 | entropy=-1.82827 | reward=0.20982\n",
      "[INFO]  epoch/step=11/9400 | loss=0.23376 | ploss=-0.02688 | vloss=0.26246 | entropy=-1.82504 | reward=0.20316\n",
      "[INFO]  epoch/step=11/9500 | loss=0.26346 | ploss=-0.01206 | vloss=0.27736 | entropy=-1.84071 | reward=0.21274\n",
      "[INFO]  Save model to ./tmp/Amazon_Cellphones/train_agent/policy_model_epoch_11.ckpt\n",
      "[INFO]  epoch/step=12/9600 | loss=0.23569 | ploss=-0.03626 | vloss=0.27384 | entropy=-1.88404 | reward=0.20033\n",
      "[INFO]  epoch/step=12/9700 | loss=0.22461 | ploss=-0.03981 | vloss=0.26627 | entropy=-1.85112 | reward=0.19601\n",
      "[INFO]  epoch/step=12/9800 | loss=0.23510 | ploss=-0.02704 | vloss=0.26394 | entropy=-1.79896 | reward=0.19251\n",
      "[INFO]  epoch/step=12/9900 | loss=0.24692 | ploss=-0.02776 | vloss=0.27650 | entropy=-1.82768 | reward=0.20373\n",
      "[INFO]  epoch/step=12/10000 | loss=0.24847 | ploss=-0.01739 | vloss=0.26771 | entropy=-1.85198 | reward=0.20030\n",
      "[INFO]  epoch/step=12/10100 | loss=0.25405 | ploss=-0.01886 | vloss=0.27478 | entropy=-1.86971 | reward=0.19979\n",
      "[INFO]  epoch/step=12/10200 | loss=0.22510 | ploss=-0.03497 | vloss=0.26194 | entropy=-1.87011 | reward=0.19398\n",
      "[INFO]  epoch/step=12/10300 | loss=0.27832 | ploss=-0.01136 | vloss=0.29148 | entropy=-1.80827 | reward=0.21530\n",
      "[INFO]  epoch/step=12/10400 | loss=0.25434 | ploss=-0.02492 | vloss=0.28108 | entropy=-1.81500 | reward=0.20302\n",
      "[INFO]  Save model to ./tmp/Amazon_Cellphones/train_agent/policy_model_epoch_12.ckpt\n",
      "[INFO]  epoch/step=13/10500 | loss=0.24923 | ploss=-0.03281 | vloss=0.28385 | entropy=-1.80962 | reward=0.20031\n",
      "[INFO]  epoch/step=13/10600 | loss=0.26658 | ploss=-0.01804 | vloss=0.28640 | entropy=-1.78314 | reward=0.20606\n",
      "[INFO]  epoch/step=13/10700 | loss=0.27756 | ploss=-0.00737 | vloss=0.28673 | entropy=-1.80039 | reward=0.20504\n",
      "[INFO]  epoch/step=13/10800 | loss=0.26647 | ploss=-0.00771 | vloss=0.27601 | entropy=-1.82354 | reward=0.19666\n",
      "[INFO]  epoch/step=13/10900 | loss=0.25659 | ploss=-0.03071 | vloss=0.28911 | entropy=-1.80513 | reward=0.20784\n",
      "[INFO]  epoch/step=13/11000 | loss=0.27402 | ploss=-0.02145 | vloss=0.29727 | entropy=-1.80282 | reward=0.21275\n",
      "[INFO]  epoch/step=13/11100 | loss=0.28779 | ploss=-0.00770 | vloss=0.29730 | entropy=-1.81253 | reward=0.21508\n",
      "[INFO]  epoch/step=13/11200 | loss=0.29036 | ploss=-0.00509 | vloss=0.29726 | entropy=-1.80112 | reward=0.21131\n",
      "[INFO]  epoch/step=13/11300 | loss=0.28431 | ploss=-0.01408 | vloss=0.30021 | entropy=-1.82482 | reward=0.21309\n",
      "[INFO]  Save model to ./tmp/Amazon_Cellphones/train_agent/policy_model_epoch_13.ckpt\n",
      "[INFO]  epoch/step=14/11400 | loss=0.26847 | ploss=-0.01576 | vloss=0.28605 | entropy=-1.82376 | reward=0.19671\n",
      "[INFO]  epoch/step=14/11500 | loss=0.26496 | ploss=-0.01990 | vloss=0.28664 | entropy=-1.78663 | reward=0.20184\n",
      "[INFO]  epoch/step=14/11600 | loss=0.28661 | ploss=-0.01975 | vloss=0.30814 | entropy=-1.77460 | reward=0.21337\n",
      "[INFO]  epoch/step=14/11700 | loss=0.29403 | ploss=-0.01476 | vloss=0.31055 | entropy=-1.76226 | reward=0.21773\n",
      "[INFO]  epoch/step=14/11800 | loss=0.26584 | ploss=-0.02589 | vloss=0.29350 | entropy=-1.76696 | reward=0.20497\n",
      "[INFO]  epoch/step=14/11900 | loss=0.25780 | ploss=-0.03198 | vloss=0.29157 | entropy=-1.79013 | reward=0.20034\n",
      "[INFO]  epoch/step=14/12000 | loss=0.28423 | ploss=-0.01965 | vloss=0.30567 | entropy=-1.79082 | reward=0.21406\n",
      "[INFO]  epoch/step=14/12100 | loss=0.28321 | ploss=-0.01131 | vloss=0.29630 | entropy=-1.78299 | reward=0.20723\n",
      "[INFO]  epoch/step=14/12200 | loss=0.28693 | ploss=-0.02905 | vloss=0.31775 | entropy=-1.76086 | reward=0.22107\n",
      "[INFO]  Save model to ./tmp/Amazon_Cellphones/train_agent/policy_model_epoch_14.ckpt\n",
      "[INFO]  epoch/step=15/12300 | loss=0.27099 | ploss=-0.02981 | vloss=0.30255 | entropy=-1.75383 | reward=0.20911\n",
      "[INFO]  epoch/step=15/12400 | loss=0.26994 | ploss=-0.02025 | vloss=0.29192 | entropy=-1.73094 | reward=0.19898\n",
      "[INFO]  epoch/step=15/12500 | loss=0.27080 | ploss=-0.03022 | vloss=0.30274 | entropy=-1.71860 | reward=0.21566\n",
      "[INFO]  epoch/step=15/12600 | loss=0.26816 | ploss=-0.02459 | vloss=0.29442 | entropy=-1.66764 | reward=0.20163\n",
      "[INFO]  epoch/step=15/12700 | loss=0.29802 | ploss=-0.01482 | vloss=0.31450 | entropy=-1.66245 | reward=0.22256\n",
      "[INFO]  epoch/step=15/12800 | loss=0.27643 | ploss=-0.02106 | vloss=0.29917 | entropy=-1.67688 | reward=0.20858\n",
      "[INFO]  epoch/step=15/12900 | loss=0.28348 | ploss=-0.01805 | vloss=0.30320 | entropy=-1.65941 | reward=0.20874\n",
      "[INFO]  epoch/step=15/13000 | loss=0.31107 | ploss=0.00103 | vloss=0.31170 | entropy=-1.65414 | reward=0.21902\n",
      "[INFO]  Save model to ./tmp/Amazon_Cellphones/train_agent/policy_model_epoch_15.ckpt\n",
      "[INFO]  epoch/step=16/13100 | loss=0.26271 | ploss=-0.03118 | vloss=0.29562 | entropy=-1.73250 | reward=0.20541\n",
      "[INFO]  epoch/step=16/13200 | loss=0.27567 | ploss=-0.02076 | vloss=0.29809 | entropy=-1.65845 | reward=0.21114\n",
      "[INFO]  epoch/step=16/13300 | loss=0.29494 | ploss=-0.02498 | vloss=0.32159 | entropy=-1.66841 | reward=0.21898\n",
      "[INFO]  epoch/step=16/13400 | loss=0.27290 | ploss=-0.01046 | vloss=0.28503 | entropy=-1.66744 | reward=0.20128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]  epoch/step=16/13500 | loss=0.28591 | ploss=-0.01708 | vloss=0.30466 | entropy=-1.67439 | reward=0.21281\n",
      "[INFO]  epoch/step=16/13600 | loss=0.27238 | ploss=-0.03011 | vloss=0.30418 | entropy=-1.68854 | reward=0.21205\n",
      "[INFO]  epoch/step=16/13700 | loss=0.27563 | ploss=-0.02838 | vloss=0.30568 | entropy=-1.66973 | reward=0.21384\n",
      "[INFO]  epoch/step=16/13800 | loss=0.28588 | ploss=-0.02396 | vloss=0.31146 | entropy=-1.62210 | reward=0.22485\n",
      "[INFO]  epoch/step=16/13900 | loss=0.28189 | ploss=-0.01520 | vloss=0.29874 | entropy=-1.65199 | reward=0.20663\n",
      "[INFO]  Save model to ./tmp/Amazon_Cellphones/train_agent/policy_model_epoch_16.ckpt\n",
      "[INFO]  epoch/step=17/14000 | loss=0.27647 | ploss=-0.01484 | vloss=0.29296 | entropy=-1.64474 | reward=0.20312\n",
      "[INFO]  epoch/step=17/14100 | loss=0.27154 | ploss=-0.02177 | vloss=0.29501 | entropy=-1.70294 | reward=0.20738\n",
      "[INFO]  epoch/step=17/14200 | loss=0.31381 | ploss=-0.00949 | vloss=0.32499 | entropy=-1.68808 | reward=0.22595\n",
      "[INFO]  epoch/step=17/14300 | loss=0.29287 | ploss=-0.01116 | vloss=0.30577 | entropy=-1.74757 | reward=0.21612\n",
      "[INFO]  epoch/step=17/14400 | loss=0.28136 | ploss=-0.02204 | vloss=0.30512 | entropy=-1.72857 | reward=0.21312\n",
      "[INFO]  epoch/step=17/14500 | loss=0.27105 | ploss=-0.02496 | vloss=0.29775 | entropy=-1.72984 | reward=0.21510\n",
      "[INFO]  epoch/step=17/14600 | loss=0.28401 | ploss=-0.02439 | vloss=0.31011 | entropy=-1.70389 | reward=0.21886\n",
      "[INFO]  epoch/step=17/14700 | loss=0.28373 | ploss=-0.01136 | vloss=0.29672 | entropy=-1.63782 | reward=0.20908\n",
      "[INFO]  epoch/step=17/14800 | loss=0.28200 | ploss=-0.01521 | vloss=0.29888 | entropy=-1.66535 | reward=0.20948\n",
      "[INFO]  Save model to ./tmp/Amazon_Cellphones/train_agent/policy_model_epoch_17.ckpt\n",
      "[INFO]  epoch/step=18/14900 | loss=0.28377 | ploss=-0.01854 | vloss=0.30399 | entropy=-1.68216 | reward=0.21453\n",
      "[INFO]  epoch/step=18/15000 | loss=0.28832 | ploss=-0.01203 | vloss=0.30203 | entropy=-1.67527 | reward=0.21576\n",
      "[INFO]  epoch/step=18/15100 | loss=0.27340 | ploss=-0.02452 | vloss=0.29959 | entropy=-1.67218 | reward=0.21281\n",
      "[INFO]  epoch/step=18/15200 | loss=0.28143 | ploss=-0.01509 | vloss=0.29815 | entropy=-1.63065 | reward=0.21396\n",
      "[INFO]  epoch/step=18/15300 | loss=0.28048 | ploss=-0.03348 | vloss=0.31554 | entropy=-1.57776 | reward=0.22540\n",
      "[INFO]  epoch/step=18/15400 | loss=0.27889 | ploss=-0.02245 | vloss=0.30292 | entropy=-1.57240 | reward=0.21092\n",
      "[INFO]  epoch/step=18/15500 | loss=0.28748 | ploss=-0.02463 | vloss=0.31366 | entropy=-1.55096 | reward=0.22126\n",
      "[INFO]  epoch/step=18/15600 | loss=0.29094 | ploss=-0.02410 | vloss=0.31657 | entropy=-1.52635 | reward=0.22328\n",
      "[INFO]  Save model to ./tmp/Amazon_Cellphones/train_agent/policy_model_epoch_18.ckpt\n",
      "[INFO]  epoch/step=19/15700 | loss=0.28699 | ploss=-0.02457 | vloss=0.31306 | entropy=-1.50435 | reward=0.22466\n",
      "[INFO]  epoch/step=19/15800 | loss=0.28610 | ploss=-0.02366 | vloss=0.31126 | entropy=-1.50851 | reward=0.22174\n",
      "[INFO]  epoch/step=19/15900 | loss=0.27789 | ploss=-0.02024 | vloss=0.29968 | entropy=-1.54617 | reward=0.20396\n",
      "[INFO]  epoch/step=19/16000 | loss=0.30104 | ploss=-0.01254 | vloss=0.31512 | entropy=-1.53327 | reward=0.22481\n",
      "[INFO]  epoch/step=19/16100 | loss=0.27380 | ploss=-0.02280 | vloss=0.29811 | entropy=-1.51749 | reward=0.20830\n",
      "[INFO]  epoch/step=19/16200 | loss=0.29251 | ploss=-0.00662 | vloss=0.30067 | entropy=-1.53246 | reward=0.21247\n",
      "[INFO]  epoch/step=19/16300 | loss=0.27702 | ploss=-0.02253 | vloss=0.30109 | entropy=-1.54252 | reward=0.21531\n",
      "[INFO]  epoch/step=19/16400 | loss=0.31570 | ploss=-0.00407 | vloss=0.32127 | entropy=-1.50146 | reward=0.22867\n",
      "[INFO]  epoch/step=19/16500 | loss=0.27297 | ploss=-0.02450 | vloss=0.29901 | entropy=-1.54182 | reward=0.21637\n",
      "[INFO]  Save model to ./tmp/Amazon_Cellphones/train_agent/policy_model_epoch_19.ckpt\n",
      "[INFO]  epoch/step=20/16600 | loss=0.28926 | ploss=-0.01427 | vloss=0.30506 | entropy=-1.52651 | reward=0.21454\n",
      "[INFO]  epoch/step=20/16700 | loss=0.28921 | ploss=-0.01849 | vloss=0.30917 | entropy=-1.46836 | reward=0.22145\n",
      "[INFO]  epoch/step=20/16800 | loss=0.29246 | ploss=-0.01097 | vloss=0.30493 | entropy=-1.49241 | reward=0.21706\n",
      "[INFO]  epoch/step=20/16900 | loss=0.30267 | ploss=-0.01533 | vloss=0.31944 | entropy=-1.43239 | reward=0.22721\n",
      "[INFO]  epoch/step=20/17000 | loss=0.29960 | ploss=-0.01456 | vloss=0.31563 | entropy=-1.47222 | reward=0.22870\n",
      "[INFO]  epoch/step=20/17100 | loss=0.26922 | ploss=-0.02639 | vloss=0.29707 | entropy=-1.45690 | reward=0.21427\n",
      "[INFO]  epoch/step=20/17200 | loss=0.29914 | ploss=-0.01456 | vloss=0.31515 | entropy=-1.45808 | reward=0.22625\n",
      "[INFO]  epoch/step=20/17300 | loss=0.26826 | ploss=-0.03663 | vloss=0.30636 | entropy=-1.46861 | reward=0.22405\n",
      "[INFO]  epoch/step=20/17400 | loss=0.27486 | ploss=-0.02286 | vloss=0.29916 | entropy=-1.44465 | reward=0.21817\n",
      "[INFO]  Save model to ./tmp/Amazon_Cellphones/train_agent/policy_model_epoch_20.ckpt\n",
      "[INFO]  epoch/step=21/17500 | loss=0.28360 | ploss=-0.02182 | vloss=0.30686 | entropy=-1.43861 | reward=0.21676\n",
      "[INFO]  epoch/step=21/17600 | loss=0.26436 | ploss=-0.02249 | vloss=0.28832 | entropy=-1.47243 | reward=0.20803\n",
      "[INFO]  epoch/step=21/17700 | loss=0.28134 | ploss=-0.02256 | vloss=0.30533 | entropy=-1.43140 | reward=0.21851\n",
      "[INFO]  epoch/step=21/17800 | loss=0.26974 | ploss=-0.02164 | vloss=0.29281 | entropy=-1.42700 | reward=0.20852\n",
      "[INFO]  epoch/step=21/17900 | loss=0.28606 | ploss=-0.01873 | vloss=0.30620 | entropy=-1.41029 | reward=0.22160\n",
      "[INFO]  epoch/step=21/18000 | loss=0.28339 | ploss=-0.01774 | vloss=0.30254 | entropy=-1.42043 | reward=0.21643\n",
      "[INFO]  epoch/step=21/18100 | loss=0.27980 | ploss=-0.02182 | vloss=0.30303 | entropy=-1.40876 | reward=0.22148\n",
      "[INFO]  epoch/step=21/18200 | loss=0.28327 | ploss=-0.00672 | vloss=0.29138 | entropy=-1.38811 | reward=0.21202\n",
      "[INFO]  epoch/step=21/18300 | loss=0.30736 | ploss=-0.01200 | vloss=0.32078 | entropy=-1.42066 | reward=0.23210\n",
      "[INFO]  Save model to ./tmp/Amazon_Cellphones/train_agent/policy_model_epoch_21.ckpt\n",
      "[INFO]  epoch/step=22/18400 | loss=0.28124 | ploss=-0.02146 | vloss=0.30409 | entropy=-1.38370 | reward=0.22232\n",
      "[INFO]  epoch/step=22/18500 | loss=0.28207 | ploss=-0.02324 | vloss=0.30672 | entropy=-1.40323 | reward=0.22070\n",
      "[INFO]  epoch/step=22/18600 | loss=0.28698 | ploss=-0.02371 | vloss=0.31209 | entropy=-1.39813 | reward=0.22801\n",
      "[INFO]  epoch/step=22/18700 | loss=0.29501 | ploss=-0.01098 | vloss=0.30739 | entropy=-1.39388 | reward=0.22524\n",
      "[INFO]  epoch/step=22/18800 | loss=0.28920 | ploss=-0.00859 | vloss=0.29921 | entropy=-1.42328 | reward=0.22477\n",
      "[INFO]  epoch/step=22/18900 | loss=0.28547 | ploss=-0.02345 | vloss=0.31032 | entropy=-1.39914 | reward=0.22759\n",
      "[INFO]  epoch/step=22/19000 | loss=0.27665 | ploss=-0.01944 | vloss=0.29749 | entropy=-1.39544 | reward=0.21965\n",
      "[INFO]  epoch/step=22/19100 | loss=0.28778 | ploss=-0.02463 | vloss=0.31382 | entropy=-1.41017 | reward=0.23303\n",
      "[INFO]  Save model to ./tmp/Amazon_Cellphones/train_agent/policy_model_epoch_22.ckpt\n",
      "[INFO]  epoch/step=23/19200 | loss=0.26923 | ploss=-0.02621 | vloss=0.29681 | entropy=-1.36759 | reward=0.21355\n",
      "[INFO]  epoch/step=23/19300 | loss=0.28505 | ploss=-0.02749 | vloss=0.31390 | entropy=-1.37128 | reward=0.23184\n",
      "[INFO]  epoch/step=23/19400 | loss=0.26905 | ploss=-0.02711 | vloss=0.29753 | entropy=-1.36888 | reward=0.22179\n",
      "[INFO]  epoch/step=23/19500 | loss=0.28285 | ploss=-0.02124 | vloss=0.30548 | entropy=-1.38419 | reward=0.22971\n",
      "[INFO]  epoch/step=23/19600 | loss=0.26488 | ploss=-0.02172 | vloss=0.28798 | entropy=-1.37083 | reward=0.21257\n",
      "[INFO]  epoch/step=23/19700 | loss=0.31703 | ploss=0.00859 | vloss=0.30983 | entropy=-1.39717 | reward=0.22757\n",
      "[INFO]  epoch/step=23/19800 | loss=0.25557 | ploss=-0.04138 | vloss=0.29837 | entropy=-1.42046 | reward=0.22438\n",
      "[INFO]  epoch/step=23/19900 | loss=0.27417 | ploss=-0.02991 | vloss=0.30548 | entropy=-1.39758 | reward=0.22367\n",
      "[INFO]  epoch/step=23/20000 | loss=0.28105 | ploss=-0.02098 | vloss=0.30345 | entropy=-1.42319 | reward=0.21964\n",
      "[INFO]  Save model to ./tmp/Amazon_Cellphones/train_agent/policy_model_epoch_23.ckpt\n",
      "[INFO]  epoch/step=24/20100 | loss=0.28159 | ploss=-0.02056 | vloss=0.30351 | entropy=-1.36531 | reward=0.22743\n",
      "[INFO]  epoch/step=24/20200 | loss=0.27776 | ploss=-0.02766 | vloss=0.30674 | entropy=-1.31823 | reward=0.22661\n",
      "[INFO]  epoch/step=24/20300 | loss=0.28869 | ploss=-0.02353 | vloss=0.31353 | entropy=-1.30872 | reward=0.23073\n",
      "[INFO]  epoch/step=24/20400 | loss=0.28613 | ploss=-0.01941 | vloss=0.30687 | entropy=-1.32167 | reward=0.22550\n",
      "[INFO]  epoch/step=24/20500 | loss=0.27117 | ploss=-0.02055 | vloss=0.29302 | entropy=-1.30618 | reward=0.21607\n",
      "[INFO]  epoch/step=24/20600 | loss=0.27294 | ploss=-0.03088 | vloss=0.30514 | entropy=-1.31648 | reward=0.22565\n",
      "[INFO]  epoch/step=24/20700 | loss=0.29132 | ploss=-0.01614 | vloss=0.30878 | entropy=-1.31557 | reward=0.22590\n",
      "[INFO]  epoch/step=24/20800 | loss=0.29165 | ploss=-0.02598 | vloss=0.31893 | entropy=-1.29574 | reward=0.23531\n",
      "[INFO]  epoch/step=24/20900 | loss=0.28638 | ploss=-0.01119 | vloss=0.29888 | entropy=-1.30461 | reward=0.22194\n",
      "[INFO]  Save model to ./tmp/Amazon_Cellphones/train_agent/policy_model_epoch_24.ckpt\n",
      "[INFO]  epoch/step=25/21000 | loss=0.27836 | ploss=-0.03478 | vloss=0.31439 | entropy=-1.25622 | reward=0.23077\n",
      "[INFO]  epoch/step=25/21100 | loss=0.28123 | ploss=-0.02422 | vloss=0.30672 | entropy=-1.26766 | reward=0.22525\n",
      "[INFO]  epoch/step=25/21200 | loss=0.29077 | ploss=-0.02226 | vloss=0.31429 | entropy=-1.25914 | reward=0.23378\n",
      "[INFO]  epoch/step=25/21300 | loss=0.28909 | ploss=-0.01336 | vloss=0.30370 | entropy=-1.24702 | reward=0.22131\n",
      "[INFO]  epoch/step=25/21400 | loss=0.29490 | ploss=-0.01095 | vloss=0.30708 | entropy=-1.23424 | reward=0.23275\n",
      "[INFO]  epoch/step=25/21500 | loss=0.28968 | ploss=-0.01864 | vloss=0.30957 | entropy=-1.24950 | reward=0.22711\n",
      "[INFO]  epoch/step=25/21600 | loss=0.29574 | ploss=-0.01655 | vloss=0.31357 | entropy=-1.27389 | reward=0.23149\n",
      "[INFO]  epoch/step=25/21700 | loss=0.26909 | ploss=-0.02591 | vloss=0.29628 | entropy=-1.27136 | reward=0.21869\n",
      "[INFO]  epoch/step=25/21800 | loss=0.28478 | ploss=-0.02343 | vloss=0.30947 | entropy=-1.26352 | reward=0.22343\n",
      "[INFO]  Save model to ./tmp/Amazon_Cellphones/train_agent/policy_model_epoch_25.ckpt\n",
      "[INFO]  epoch/step=26/21900 | loss=0.27267 | ploss=-0.01710 | vloss=0.29106 | entropy=-1.27897 | reward=0.21757\n",
      "[INFO]  epoch/step=26/22000 | loss=0.30073 | ploss=-0.00930 | vloss=0.31131 | entropy=-1.26883 | reward=0.23446\n",
      "[INFO]  epoch/step=26/22100 | loss=0.28836 | ploss=-0.02145 | vloss=0.31108 | entropy=-1.28214 | reward=0.22683\n",
      "[INFO]  epoch/step=26/22200 | loss=0.28293 | ploss=-0.01387 | vloss=0.29809 | entropy=-1.29139 | reward=0.21968\n",
      "[INFO]  epoch/step=26/22300 | loss=0.29141 | ploss=-0.02459 | vloss=0.31728 | entropy=-1.28124 | reward=0.23422\n",
      "[INFO]  epoch/step=26/22400 | loss=0.27355 | ploss=-0.02517 | vloss=0.30000 | entropy=-1.28311 | reward=0.22383\n",
      "[INFO]  epoch/step=26/22500 | loss=0.28794 | ploss=-0.02210 | vloss=0.31131 | entropy=-1.25848 | reward=0.22868\n",
      "[INFO]  epoch/step=26/22600 | loss=0.28679 | ploss=-0.01642 | vloss=0.30447 | entropy=-1.26063 | reward=0.22669\n",
      "[INFO]  Save model to ./tmp/Amazon_Cellphones/train_agent/policy_model_epoch_26.ckpt\n",
      "[INFO]  epoch/step=27/22700 | loss=0.28045 | ploss=-0.02529 | vloss=0.30702 | entropy=-1.27934 | reward=0.22488\n",
      "[INFO]  epoch/step=27/22800 | loss=0.27288 | ploss=-0.02253 | vloss=0.29666 | entropy=-1.24582 | reward=0.21941\n",
      "[INFO]  epoch/step=27/22900 | loss=0.29208 | ploss=-0.01169 | vloss=0.30506 | entropy=-1.29408 | reward=0.22870\n",
      "[INFO]  epoch/step=27/23000 | loss=0.29883 | ploss=-0.02136 | vloss=0.32147 | entropy=-1.27982 | reward=0.23984\n",
      "[INFO]  epoch/step=27/23100 | loss=0.28463 | ploss=-0.01515 | vloss=0.30108 | entropy=-1.30675 | reward=0.22053\n",
      "[INFO]  epoch/step=27/23200 | loss=0.27969 | ploss=-0.02086 | vloss=0.30183 | entropy=-1.28344 | reward=0.22329\n",
      "[INFO]  epoch/step=27/23300 | loss=0.27085 | ploss=-0.01795 | vloss=0.29008 | entropy=-1.28844 | reward=0.21570\n",
      "[INFO]  epoch/step=27/23400 | loss=0.27729 | ploss=-0.01614 | vloss=0.29471 | entropy=-1.27573 | reward=0.22405\n",
      "[INFO]  epoch/step=27/23500 | loss=0.27095 | ploss=-0.02290 | vloss=0.29515 | entropy=-1.29721 | reward=0.22481\n",
      "[INFO]  Save model to ./tmp/Amazon_Cellphones/train_agent/policy_model_epoch_27.ckpt\n",
      "[INFO]  epoch/step=28/23600 | loss=0.27493 | ploss=-0.01472 | vloss=0.29092 | entropy=-1.27381 | reward=0.21603\n",
      "[INFO]  epoch/step=28/23700 | loss=0.28097 | ploss=-0.01843 | vloss=0.30070 | entropy=-1.29884 | reward=0.22483\n",
      "[INFO]  epoch/step=28/23800 | loss=0.26696 | ploss=-0.03386 | vloss=0.30212 | entropy=-1.29980 | reward=0.22827\n",
      "[INFO]  epoch/step=28/23900 | loss=0.29196 | ploss=-0.00988 | vloss=0.30314 | entropy=-1.29802 | reward=0.22400\n",
      "[INFO]  epoch/step=28/24000 | loss=0.29046 | ploss=-0.00911 | vloss=0.30087 | entropy=-1.30112 | reward=0.22450\n",
      "[INFO]  epoch/step=28/24100 | loss=0.29572 | ploss=-0.02373 | vloss=0.32076 | entropy=-1.31412 | reward=0.23658\n",
      "[INFO]  epoch/step=28/24200 | loss=0.27701 | ploss=-0.01863 | vloss=0.29695 | entropy=-1.31477 | reward=0.22450\n",
      "[INFO]  epoch/step=28/24300 | loss=0.27886 | ploss=-0.02243 | vloss=0.30263 | entropy=-1.33545 | reward=0.22131\n",
      "[INFO]  epoch/step=28/24400 | loss=0.29392 | ploss=-0.01663 | vloss=0.31189 | entropy=-1.33258 | reward=0.22762\n",
      "[INFO]  Save model to ./tmp/Amazon_Cellphones/train_agent/policy_model_epoch_28.ckpt\n",
      "[INFO]  epoch/step=29/24500 | loss=0.27205 | ploss=-0.02968 | vloss=0.30306 | entropy=-1.33062 | reward=0.22293\n",
      "[INFO]  epoch/step=29/24600 | loss=0.27938 | ploss=-0.01669 | vloss=0.29737 | entropy=-1.29809 | reward=0.22169\n",
      "[INFO]  epoch/step=29/24700 | loss=0.25849 | ploss=-0.03556 | vloss=0.29538 | entropy=-1.32805 | reward=0.21646\n",
      "[INFO]  epoch/step=29/24800 | loss=0.28383 | ploss=-0.01488 | vloss=0.30002 | entropy=-1.31672 | reward=0.22742\n",
      "[INFO]  epoch/step=29/24900 | loss=0.27833 | ploss=-0.01389 | vloss=0.29353 | entropy=-1.31106 | reward=0.21670\n",
      "[INFO]  epoch/step=29/25000 | loss=0.26969 | ploss=-0.02441 | vloss=0.29544 | entropy=-1.34784 | reward=0.21591\n",
      "[INFO]  epoch/step=29/25100 | loss=0.28753 | ploss=-0.01730 | vloss=0.30615 | entropy=-1.31937 | reward=0.22405\n",
      "[INFO]  epoch/step=29/25200 | loss=0.29233 | ploss=-0.00766 | vloss=0.30133 | entropy=-1.34451 | reward=0.22070\n",
      "[INFO]  Save model to ./tmp/Amazon_Cellphones/train_agent/policy_model_epoch_29.ckpt\n",
      "[INFO]  epoch/step=30/25300 | loss=0.27366 | ploss=-0.03178 | vloss=0.30676 | entropy=-1.31796 | reward=0.22712\n",
      "[INFO]  epoch/step=30/25400 | loss=0.28997 | ploss=-0.01653 | vloss=0.30779 | entropy=-1.29043 | reward=0.22720\n",
      "[INFO]  epoch/step=30/25500 | loss=0.25868 | ploss=-0.03342 | vloss=0.29342 | entropy=-1.31889 | reward=0.22075\n",
      "[INFO]  epoch/step=30/25600 | loss=0.25748 | ploss=-0.02786 | vloss=0.28662 | entropy=-1.27731 | reward=0.21310\n",
      "[INFO]  epoch/step=30/25700 | loss=0.27401 | ploss=-0.01755 | vloss=0.29287 | entropy=-1.31425 | reward=0.21661\n",
      "[INFO]  epoch/step=30/25800 | loss=0.28058 | ploss=-0.01826 | vloss=0.30011 | entropy=-1.27777 | reward=0.22572\n",
      "[INFO]  epoch/step=30/25900 | loss=0.28350 | ploss=-0.02239 | vloss=0.30720 | entropy=-1.30570 | reward=0.22940\n",
      "[INFO]  epoch/step=30/26000 | loss=0.29044 | ploss=-0.02341 | vloss=0.31513 | entropy=-1.28832 | reward=0.24057\n",
      "[INFO]  epoch/step=30/26100 | loss=0.30659 | ploss=-0.01347 | vloss=0.32132 | entropy=-1.26489 | reward=0.23573\n",
      "[INFO]  Save model to ./tmp/Amazon_Cellphones/train_agent/policy_model_epoch_30.ckpt\n",
      "[INFO]  epoch/step=31/26200 | loss=0.28126 | ploss=-0.02846 | vloss=0.31102 | entropy=-1.30565 | reward=0.23134\n",
      "[INFO]  epoch/step=31/26300 | loss=0.27322 | ploss=-0.03351 | vloss=0.30802 | entropy=-1.28181 | reward=0.23151\n",
      "[INFO]  epoch/step=31/26400 | loss=0.27745 | ploss=-0.02096 | vloss=0.29966 | entropy=-1.24138 | reward=0.23215\n",
      "[INFO]  epoch/step=31/26500 | loss=0.27879 | ploss=-0.03160 | vloss=0.31166 | entropy=-1.26874 | reward=0.23360\n",
      "[INFO]  epoch/step=31/26600 | loss=0.28635 | ploss=-0.01957 | vloss=0.30715 | entropy=-1.23057 | reward=0.23290\n",
      "[INFO]  epoch/step=31/26700 | loss=0.28597 | ploss=-0.01432 | vloss=0.30150 | entropy=-1.21571 | reward=0.21898\n",
      "[INFO]  epoch/step=31/26800 | loss=0.28327 | ploss=-0.01787 | vloss=0.30239 | entropy=-1.24795 | reward=0.22460\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]  epoch/step=31/26900 | loss=0.27963 | ploss=-0.01720 | vloss=0.29806 | entropy=-1.22639 | reward=0.22043\n",
      "[INFO]  epoch/step=31/27000 | loss=0.29160 | ploss=-0.01824 | vloss=0.31107 | entropy=-1.23183 | reward=0.23495\n",
      "[INFO]  Save model to ./tmp/Amazon_Cellphones/train_agent/policy_model_epoch_31.ckpt\n",
      "[INFO]  epoch/step=32/27100 | loss=0.25874 | ploss=-0.02428 | vloss=0.28426 | entropy=-1.23749 | reward=0.21720\n",
      "[INFO]  epoch/step=32/27200 | loss=0.28120 | ploss=-0.02238 | vloss=0.30481 | entropy=-1.22463 | reward=0.23388\n",
      "[INFO]  epoch/step=32/27300 | loss=0.28264 | ploss=-0.02383 | vloss=0.30766 | entropy=-1.19817 | reward=0.22716\n",
      "[INFO]  epoch/step=32/27400 | loss=0.28087 | ploss=-0.01666 | vloss=0.29873 | entropy=-1.20174 | reward=0.22539\n",
      "[INFO]  epoch/step=32/27500 | loss=0.29073 | ploss=-0.01796 | vloss=0.30987 | entropy=-1.18036 | reward=0.23129\n",
      "[INFO]  epoch/step=32/27600 | loss=0.26725 | ploss=-0.01980 | vloss=0.28826 | entropy=-1.20706 | reward=0.21504\n",
      "[INFO]  epoch/step=32/27700 | loss=0.27343 | ploss=-0.01943 | vloss=0.29409 | entropy=-1.23418 | reward=0.22226\n",
      "[INFO]  epoch/step=32/27800 | loss=0.26875 | ploss=-0.03080 | vloss=0.30079 | entropy=-1.23137 | reward=0.22514\n",
      "[INFO]  epoch/step=32/27900 | loss=0.28679 | ploss=-0.02067 | vloss=0.30868 | entropy=-1.22029 | reward=0.23182\n",
      "[INFO]  Save model to ./tmp/Amazon_Cellphones/train_agent/policy_model_epoch_32.ckpt\n",
      "[INFO]  epoch/step=33/28000 | loss=0.27440 | ploss=-0.02424 | vloss=0.29985 | entropy=-1.20602 | reward=0.21892\n",
      "[INFO]  epoch/step=33/28100 | loss=0.28145 | ploss=-0.02460 | vloss=0.30727 | entropy=-1.21725 | reward=0.22829\n",
      "[INFO]  epoch/step=33/28200 | loss=0.28632 | ploss=-0.02316 | vloss=0.31068 | entropy=-1.19892 | reward=0.23247\n",
      "[INFO]  epoch/step=33/28300 | loss=0.25840 | ploss=-0.03247 | vloss=0.29208 | entropy=-1.20532 | reward=0.22222\n",
      "[INFO]  epoch/step=33/28400 | loss=0.30085 | ploss=-0.00926 | vloss=0.31131 | entropy=-1.19979 | reward=0.23240\n",
      "[INFO]  epoch/step=33/28500 | loss=0.27306 | ploss=-0.01928 | vloss=0.29356 | entropy=-1.22943 | reward=0.21877\n",
      "[INFO]  epoch/step=33/28600 | loss=0.28481 | ploss=-0.02894 | vloss=0.31496 | entropy=-1.21151 | reward=0.23144\n",
      "[INFO]  epoch/step=33/28700 | loss=0.26231 | ploss=-0.03264 | vloss=0.29617 | entropy=-1.21792 | reward=0.22424\n",
      "[INFO]  Save model to ./tmp/Amazon_Cellphones/train_agent/policy_model_epoch_33.ckpt\n",
      "[INFO]  epoch/step=34/28800 | loss=0.27981 | ploss=-0.02741 | vloss=0.30844 | entropy=-1.22294 | reward=0.22451\n",
      "[INFO]  epoch/step=34/28900 | loss=0.26662 | ploss=-0.02689 | vloss=0.29472 | entropy=-1.21085 | reward=0.21953\n",
      "[INFO]  epoch/step=34/29000 | loss=0.28123 | ploss=-0.01433 | vloss=0.29676 | entropy=-1.19846 | reward=0.22544\n",
      "[INFO]  epoch/step=34/29100 | loss=0.27153 | ploss=-0.03723 | vloss=0.30995 | entropy=-1.18057 | reward=0.22987\n",
      "[INFO]  epoch/step=34/29200 | loss=0.27882 | ploss=-0.02102 | vloss=0.30104 | entropy=-1.19871 | reward=0.22396\n",
      "[INFO]  epoch/step=34/29300 | loss=0.28068 | ploss=-0.01681 | vloss=0.29869 | entropy=-1.20018 | reward=0.22301\n",
      "[INFO]  epoch/step=34/29400 | loss=0.28479 | ploss=-0.01528 | vloss=0.30128 | entropy=-1.20213 | reward=0.22788\n",
      "[INFO]  epoch/step=34/29500 | loss=0.28005 | ploss=-0.00193 | vloss=0.28321 | entropy=-1.22544 | reward=0.21559\n",
      "[INFO]  epoch/step=34/29600 | loss=0.31164 | ploss=-0.00525 | vloss=0.31812 | entropy=-1.22786 | reward=0.23702\n",
      "[INFO]  Save model to ./tmp/Amazon_Cellphones/train_agent/policy_model_epoch_34.ckpt\n",
      "[INFO]  epoch/step=35/29700 | loss=0.28496 | ploss=-0.02503 | vloss=0.31120 | entropy=-1.21187 | reward=0.22832\n",
      "[INFO]  epoch/step=35/29800 | loss=0.28347 | ploss=-0.01569 | vloss=0.30036 | entropy=-1.20762 | reward=0.22566\n",
      "[INFO]  epoch/step=35/29900 | loss=0.27607 | ploss=-0.03129 | vloss=0.30856 | entropy=-1.19127 | reward=0.22848\n",
      "[INFO]  epoch/step=35/30000 | loss=0.29166 | ploss=-0.01860 | vloss=0.31145 | entropy=-1.19802 | reward=0.23040\n",
      "[INFO]  epoch/step=35/30100 | loss=0.30034 | ploss=-0.01181 | vloss=0.31335 | entropy=-1.20481 | reward=0.23818\n",
      "[INFO]  epoch/step=35/30200 | loss=0.26716 | ploss=-0.03422 | vloss=0.30260 | entropy=-1.21943 | reward=0.22785\n",
      "[INFO]  epoch/step=35/30300 | loss=0.29748 | ploss=-0.01922 | vloss=0.31790 | entropy=-1.19692 | reward=0.24063\n",
      "[INFO]  epoch/step=35/30400 | loss=0.29041 | ploss=-0.01180 | vloss=0.30343 | entropy=-1.22230 | reward=0.23012\n",
      "[INFO]  epoch/step=35/30500 | loss=0.27459 | ploss=-0.03172 | vloss=0.30753 | entropy=-1.22348 | reward=0.23460\n",
      "[INFO]  Save model to ./tmp/Amazon_Cellphones/train_agent/policy_model_epoch_35.ckpt\n",
      "[INFO]  epoch/step=36/30600 | loss=0.29378 | ploss=-0.00973 | vloss=0.30471 | entropy=-1.19538 | reward=0.22750\n",
      "[INFO]  epoch/step=36/30700 | loss=0.29369 | ploss=-0.01574 | vloss=0.31063 | entropy=-1.19419 | reward=0.23155\n",
      "[INFO]  epoch/step=36/30800 | loss=0.28115 | ploss=-0.03052 | vloss=0.31287 | entropy=-1.20020 | reward=0.23512\n",
      "[INFO]  epoch/step=36/30900 | loss=0.28933 | ploss=-0.01703 | vloss=0.30757 | entropy=-1.20352 | reward=0.22618\n",
      "[INFO]  epoch/step=36/31000 | loss=0.27254 | ploss=-0.02675 | vloss=0.30049 | entropy=-1.19805 | reward=0.21986\n",
      "[INFO]  epoch/step=36/31100 | loss=0.26669 | ploss=-0.02282 | vloss=0.29070 | entropy=-1.19592 | reward=0.21641\n",
      "[INFO]  epoch/step=36/31200 | loss=0.29292 | ploss=-0.01100 | vloss=0.30511 | entropy=-1.18093 | reward=0.23122\n",
      "[INFO]  epoch/step=36/31300 | loss=0.28331 | ploss=-0.01460 | vloss=0.29912 | entropy=-1.20265 | reward=0.22277\n",
      "[INFO]  Save model to ./tmp/Amazon_Cellphones/train_agent/policy_model_epoch_36.ckpt\n",
      "[INFO]  epoch/step=37/31400 | loss=0.28409 | ploss=-0.02978 | vloss=0.31508 | entropy=-1.20954 | reward=0.23479\n",
      "[INFO]  epoch/step=37/31500 | loss=0.28039 | ploss=-0.03101 | vloss=0.31259 | entropy=-1.18707 | reward=0.23645\n",
      "[INFO]  epoch/step=37/31600 | loss=0.28022 | ploss=-0.02360 | vloss=0.30500 | entropy=-1.18224 | reward=0.22712\n",
      "[INFO]  epoch/step=37/31700 | loss=0.27035 | ploss=-0.02501 | vloss=0.29659 | entropy=-1.22352 | reward=0.22645\n",
      "[INFO]  epoch/step=37/31800 | loss=0.29042 | ploss=-0.02362 | vloss=0.31520 | entropy=-1.16574 | reward=0.23839\n",
      "[INFO]  epoch/step=37/31900 | loss=0.27756 | ploss=-0.02807 | vloss=0.30684 | entropy=-1.21489 | reward=0.23060\n",
      "[INFO]  epoch/step=37/32000 | loss=0.27431 | ploss=-0.02665 | vloss=0.30214 | entropy=-1.18187 | reward=0.22501\n",
      "[INFO]  epoch/step=37/32100 | loss=0.28576 | ploss=-0.02068 | vloss=0.30762 | entropy=-1.18445 | reward=0.22447\n",
      "[INFO]  epoch/step=37/32200 | loss=0.30520 | ploss=-0.00584 | vloss=0.31221 | entropy=-1.16182 | reward=0.23226\n",
      "[INFO]  Save model to ./tmp/Amazon_Cellphones/train_agent/policy_model_epoch_37.ckpt\n",
      "[INFO]  epoch/step=38/32300 | loss=0.26675 | ploss=-0.03157 | vloss=0.29951 | entropy=-1.18539 | reward=0.22062\n",
      "[INFO]  epoch/step=38/32400 | loss=0.27918 | ploss=-0.02326 | vloss=0.30362 | entropy=-1.18800 | reward=0.22998\n",
      "[INFO]  epoch/step=38/32500 | loss=0.27633 | ploss=-0.02750 | vloss=0.30501 | entropy=-1.18025 | reward=0.22708\n",
      "[INFO]  epoch/step=38/32600 | loss=0.25993 | ploss=-0.03271 | vloss=0.29382 | entropy=-1.17381 | reward=0.21937\n",
      "[INFO]  epoch/step=38/32700 | loss=0.29155 | ploss=-0.02006 | vloss=0.31278 | entropy=-1.16594 | reward=0.23763\n",
      "[INFO]  epoch/step=38/32800 | loss=0.27663 | ploss=-0.02241 | vloss=0.30023 | entropy=-1.18791 | reward=0.22133\n",
      "[INFO]  epoch/step=38/32900 | loss=0.28081 | ploss=-0.02100 | vloss=0.30298 | entropy=-1.17582 | reward=0.22592\n",
      "[INFO]  epoch/step=38/33000 | loss=0.28575 | ploss=-0.01995 | vloss=0.30687 | entropy=-1.16720 | reward=0.22975\n",
      "[INFO]  epoch/step=38/33100 | loss=0.29016 | ploss=-0.01409 | vloss=0.30541 | entropy=-1.16406 | reward=0.23159\n",
      "[INFO]  Save model to ./tmp/Amazon_Cellphones/train_agent/policy_model_epoch_38.ckpt\n",
      "[INFO]  epoch/step=39/33200 | loss=0.28216 | ploss=-0.01805 | vloss=0.30139 | entropy=-1.17771 | reward=0.21943\n",
      "[INFO]  epoch/step=39/33300 | loss=0.26511 | ploss=-0.03307 | vloss=0.29937 | entropy=-1.18173 | reward=0.22583\n",
      "[INFO]  epoch/step=39/33400 | loss=0.26622 | ploss=-0.02152 | vloss=0.28890 | entropy=-1.16461 | reward=0.21943\n",
      "[INFO]  epoch/step=39/33500 | loss=0.29331 | ploss=-0.01781 | vloss=0.31228 | entropy=-1.16131 | reward=0.23918\n",
      "[INFO]  epoch/step=39/33600 | loss=0.28308 | ploss=-0.02137 | vloss=0.30562 | entropy=-1.16768 | reward=0.22702\n",
      "[INFO]  epoch/step=39/33700 | loss=0.27751 | ploss=-0.01554 | vloss=0.29420 | entropy=-1.15301 | reward=0.22610\n",
      "[INFO]  epoch/step=39/33800 | loss=0.27208 | ploss=-0.02399 | vloss=0.29723 | entropy=-1.16221 | reward=0.22129\n",
      "[INFO]  epoch/step=39/33900 | loss=0.29582 | ploss=-0.01528 | vloss=0.31226 | entropy=-1.16171 | reward=0.23727\n",
      "[INFO]  epoch/step=39/34000 | loss=0.27221 | ploss=-0.03230 | vloss=0.30569 | entropy=-1.18073 | reward=0.22711\n",
      "[INFO]  Save model to ./tmp/Amazon_Cellphones/train_agent/policy_model_epoch_39.ckpt\n",
      "[INFO]  epoch/step=40/34100 | loss=0.28936 | ploss=-0.01626 | vloss=0.30679 | entropy=-1.16829 | reward=0.22908\n",
      "[INFO]  epoch/step=40/34200 | loss=0.29859 | ploss=-0.01647 | vloss=0.31620 | entropy=-1.13807 | reward=0.23752\n",
      "[INFO]  epoch/step=40/34300 | loss=0.28569 | ploss=-0.02316 | vloss=0.31002 | entropy=-1.16452 | reward=0.23438\n",
      "[INFO]  epoch/step=40/34400 | loss=0.28300 | ploss=-0.02131 | vloss=0.30548 | entropy=-1.17174 | reward=0.23394\n",
      "[INFO]  epoch/step=40/34500 | loss=0.26914 | ploss=-0.03643 | vloss=0.30673 | entropy=-1.15128 | reward=0.22749\n",
      "[INFO]  epoch/step=40/34600 | loss=0.31271 | ploss=-0.01065 | vloss=0.32451 | entropy=-1.15252 | reward=0.24441\n",
      "[INFO]  epoch/step=40/34700 | loss=0.26993 | ploss=-0.02923 | vloss=0.30029 | entropy=-1.13127 | reward=0.22913\n",
      "[INFO]  epoch/step=40/34800 | loss=0.27741 | ploss=-0.01536 | vloss=0.29391 | entropy=-1.13880 | reward=0.22103\n",
      "[INFO]  Save model to ./tmp/Amazon_Cellphones/train_agent/policy_model_epoch_40.ckpt\n",
      "[INFO]  epoch/step=41/34900 | loss=0.29181 | ploss=-0.02093 | vloss=0.31391 | entropy=-1.15862 | reward=0.23055\n",
      "[INFO]  epoch/step=41/35000 | loss=0.29101 | ploss=-0.00940 | vloss=0.30158 | entropy=-1.16594 | reward=0.22609\n",
      "[INFO]  epoch/step=41/35100 | loss=0.28419 | ploss=-0.01842 | vloss=0.30378 | entropy=-1.16906 | reward=0.22951\n",
      "[INFO]  epoch/step=41/35200 | loss=0.28205 | ploss=-0.01306 | vloss=0.29629 | entropy=-1.17484 | reward=0.22142\n",
      "[INFO]  epoch/step=41/35300 | loss=0.27866 | ploss=-0.02253 | vloss=0.30232 | entropy=-1.13653 | reward=0.22725\n",
      "[INFO]  epoch/step=41/35400 | loss=0.27429 | ploss=-0.02403 | vloss=0.29949 | entropy=-1.16756 | reward=0.22727\n",
      "[INFO]  epoch/step=41/35500 | loss=0.28982 | ploss=-0.01535 | vloss=0.30634 | entropy=-1.17347 | reward=0.22992\n",
      "[INFO]  epoch/step=41/35600 | loss=0.28872 | ploss=-0.02107 | vloss=0.31093 | entropy=-1.14910 | reward=0.23170\n",
      "[INFO]  epoch/step=41/35700 | loss=0.30815 | ploss=-0.01294 | vloss=0.32222 | entropy=-1.13434 | reward=0.24062\n",
      "[INFO]  Save model to ./tmp/Amazon_Cellphones/train_agent/policy_model_epoch_41.ckpt\n",
      "[INFO]  epoch/step=42/35800 | loss=0.29546 | ploss=-0.02098 | vloss=0.31760 | entropy=-1.15112 | reward=0.23817\n",
      "[INFO]  epoch/step=42/35900 | loss=0.27873 | ploss=-0.02687 | vloss=0.30674 | entropy=-1.14375 | reward=0.23176\n",
      "[INFO]  epoch/step=42/36000 | loss=0.27094 | ploss=-0.02413 | vloss=0.29623 | entropy=-1.15198 | reward=0.21964\n",
      "[INFO]  epoch/step=42/36100 | loss=0.29883 | ploss=-0.01535 | vloss=0.31531 | entropy=-1.13534 | reward=0.23822\n",
      "[INFO]  epoch/step=42/36200 | loss=0.28807 | ploss=-0.01192 | vloss=0.30114 | entropy=-1.14688 | reward=0.22759\n",
      "[INFO]  epoch/step=42/36300 | loss=0.27366 | ploss=-0.02408 | vloss=0.29888 | entropy=-1.15001 | reward=0.22469\n",
      "[INFO]  epoch/step=42/36400 | loss=0.29578 | ploss=-0.00604 | vloss=0.30294 | entropy=-1.11234 | reward=0.22843\n",
      "[INFO]  epoch/step=42/36500 | loss=0.28078 | ploss=-0.01749 | vloss=0.29941 | entropy=-1.14284 | reward=0.22576\n",
      "[INFO]  epoch/step=42/36600 | loss=0.27593 | ploss=-0.02048 | vloss=0.29754 | entropy=-1.13210 | reward=0.22218\n",
      "[INFO]  Save model to ./tmp/Amazon_Cellphones/train_agent/policy_model_epoch_42.ckpt\n",
      "[INFO]  epoch/step=43/36700 | loss=0.27272 | ploss=-0.02767 | vloss=0.30151 | entropy=-1.12243 | reward=0.22035\n",
      "[INFO]  epoch/step=43/36800 | loss=0.28979 | ploss=-0.02386 | vloss=0.31479 | entropy=-1.14558 | reward=0.23595\n",
      "[INFO]  epoch/step=43/36900 | loss=0.27087 | ploss=-0.02643 | vloss=0.29844 | entropy=-1.13711 | reward=0.22466\n",
      "[INFO]  epoch/step=43/37000 | loss=0.27707 | ploss=-0.02814 | vloss=0.30632 | entropy=-1.10653 | reward=0.22867\n",
      "[INFO]  epoch/step=43/37100 | loss=0.27898 | ploss=-0.02331 | vloss=0.30341 | entropy=-1.12234 | reward=0.23000\n",
      "[INFO]  epoch/step=43/37200 | loss=0.26727 | ploss=-0.03480 | vloss=0.30321 | entropy=-1.14909 | reward=0.22838\n",
      "[INFO]  epoch/step=43/37300 | loss=0.27516 | ploss=-0.02252 | vloss=0.29879 | entropy=-1.10538 | reward=0.23172\n",
      "[INFO]  epoch/step=43/37400 | loss=0.27457 | ploss=-0.03826 | vloss=0.31394 | entropy=-1.11177 | reward=0.24023\n",
      "[INFO]  Save model to ./tmp/Amazon_Cellphones/train_agent/policy_model_epoch_43.ckpt\n",
      "[INFO]  epoch/step=44/37500 | loss=0.26814 | ploss=-0.02495 | vloss=0.29421 | entropy=-1.12981 | reward=0.21919\n",
      "[INFO]  epoch/step=44/37600 | loss=0.30153 | ploss=-0.00540 | vloss=0.30807 | entropy=-1.12984 | reward=0.22940\n",
      "[INFO]  epoch/step=44/37700 | loss=0.29186 | ploss=-0.02386 | vloss=0.31683 | entropy=-1.11339 | reward=0.24086\n",
      "[INFO]  epoch/step=44/37800 | loss=0.29895 | ploss=-0.01901 | vloss=0.31906 | entropy=-1.10568 | reward=0.24031\n",
      "[INFO]  epoch/step=44/37900 | loss=0.27576 | ploss=-0.02614 | vloss=0.30301 | entropy=-1.11246 | reward=0.23072\n",
      "[INFO]  epoch/step=44/38000 | loss=0.29277 | ploss=-0.01349 | vloss=0.30739 | entropy=-1.12738 | reward=0.22931\n",
      "[INFO]  epoch/step=44/38100 | loss=0.29114 | ploss=-0.02284 | vloss=0.31510 | entropy=-1.12331 | reward=0.23843\n",
      "[INFO]  epoch/step=44/38200 | loss=0.27505 | ploss=-0.02514 | vloss=0.30131 | entropy=-1.12582 | reward=0.22959\n",
      "[INFO]  epoch/step=44/38300 | loss=0.28836 | ploss=-0.01724 | vloss=0.30674 | entropy=-1.13852 | reward=0.23080\n",
      "[INFO]  Save model to ./tmp/Amazon_Cellphones/train_agent/policy_model_epoch_44.ckpt\n",
      "[INFO]  epoch/step=45/38400 | loss=0.27834 | ploss=-0.01896 | vloss=0.29841 | entropy=-1.11489 | reward=0.22059\n",
      "[INFO]  epoch/step=45/38500 | loss=0.27486 | ploss=-0.02254 | vloss=0.29852 | entropy=-1.12096 | reward=0.23116\n",
      "[INFO]  epoch/step=45/38600 | loss=0.28368 | ploss=-0.02223 | vloss=0.30704 | entropy=-1.13392 | reward=0.23266\n",
      "[INFO]  epoch/step=45/38700 | loss=0.29029 | ploss=-0.01936 | vloss=0.31076 | entropy=-1.11373 | reward=0.23617\n",
      "[INFO]  epoch/step=45/38800 | loss=0.26935 | ploss=-0.03175 | vloss=0.30223 | entropy=-1.12406 | reward=0.22982\n",
      "[INFO]  epoch/step=45/38900 | loss=0.25936 | ploss=-0.02889 | vloss=0.28938 | entropy=-1.13458 | reward=0.21520\n",
      "[INFO]  epoch/step=45/39000 | loss=0.28459 | ploss=-0.01335 | vloss=0.29906 | entropy=-1.12368 | reward=0.22271\n",
      "[INFO]  epoch/step=45/39100 | loss=0.28952 | ploss=-0.01482 | vloss=0.30546 | entropy=-1.12405 | reward=0.23075\n",
      "[INFO]  epoch/step=45/39200 | loss=0.29865 | ploss=-0.01630 | vloss=0.31606 | entropy=-1.10980 | reward=0.23665\n",
      "[INFO]  Save model to ./tmp/Amazon_Cellphones/train_agent/policy_model_epoch_45.ckpt\n",
      "[INFO]  epoch/step=46/39300 | loss=0.27436 | ploss=-0.02393 | vloss=0.29939 | entropy=-1.09546 | reward=0.22593\n",
      "[INFO]  epoch/step=46/39400 | loss=0.27899 | ploss=-0.02024 | vloss=0.30035 | entropy=-1.11917 | reward=0.22462\n",
      "[INFO]  epoch/step=46/39500 | loss=0.28899 | ploss=-0.02186 | vloss=0.31197 | entropy=-1.11979 | reward=0.23658\n",
      "[INFO]  epoch/step=46/39600 | loss=0.27560 | ploss=-0.02002 | vloss=0.29675 | entropy=-1.12805 | reward=0.22475\n",
      "[INFO]  epoch/step=46/39700 | loss=0.27656 | ploss=-0.02793 | vloss=0.30562 | entropy=-1.13435 | reward=0.23105\n",
      "[INFO]  epoch/step=46/39800 | loss=0.26943 | ploss=-0.02535 | vloss=0.29593 | entropy=-1.15362 | reward=0.22808\n",
      "[INFO]  epoch/step=46/39900 | loss=0.27543 | ploss=-0.02700 | vloss=0.30358 | entropy=-1.14722 | reward=0.22806\n",
      "[INFO]  epoch/step=46/40000 | loss=0.26369 | ploss=-0.02099 | vloss=0.28580 | entropy=-1.12587 | reward=0.21626\n",
      "[INFO]  epoch/step=46/40100 | loss=0.29233 | ploss=-0.01370 | vloss=0.30714 | entropy=-1.11027 | reward=0.23254\n",
      "[INFO]  Save model to ./tmp/Amazon_Cellphones/train_agent/policy_model_epoch_46.ckpt\n",
      "[INFO]  epoch/step=47/40200 | loss=0.29158 | ploss=-0.01350 | vloss=0.30620 | entropy=-1.11655 | reward=0.23046\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]  epoch/step=47/40300 | loss=0.26640 | ploss=-0.03844 | vloss=0.30597 | entropy=-1.11851 | reward=0.23442\n",
      "[INFO]  epoch/step=47/40400 | loss=0.29145 | ploss=-0.01039 | vloss=0.30295 | entropy=-1.10951 | reward=0.23197\n",
      "[INFO]  epoch/step=47/40500 | loss=0.28833 | ploss=-0.02095 | vloss=0.31043 | entropy=-1.14456 | reward=0.23439\n",
      "[INFO]  epoch/step=47/40600 | loss=0.28123 | ploss=-0.01990 | vloss=0.30224 | entropy=-1.11935 | reward=0.22772\n",
      "[INFO]  epoch/step=47/40700 | loss=0.27604 | ploss=-0.02554 | vloss=0.30269 | entropy=-1.11438 | reward=0.23222\n",
      "[INFO]  epoch/step=47/40800 | loss=0.27538 | ploss=-0.03276 | vloss=0.30926 | entropy=-1.12233 | reward=0.23290\n",
      "[INFO]  epoch/step=47/40900 | loss=0.29862 | ploss=-0.01471 | vloss=0.31446 | entropy=-1.12279 | reward=0.23842\n",
      "[INFO]  Save model to ./tmp/Amazon_Cellphones/train_agent/policy_model_epoch_47.ckpt\n",
      "[INFO]  epoch/step=48/41000 | loss=0.29015 | ploss=-0.01885 | vloss=0.31012 | entropy=-1.12078 | reward=0.23509\n",
      "[INFO]  epoch/step=48/41100 | loss=0.26903 | ploss=-0.02393 | vloss=0.29406 | entropy=-1.10293 | reward=0.21877\n",
      "[INFO]  epoch/step=48/41200 | loss=0.28341 | ploss=-0.01715 | vloss=0.30168 | entropy=-1.12045 | reward=0.22226\n",
      "[INFO]  epoch/step=48/41300 | loss=0.26571 | ploss=-0.02630 | vloss=0.29311 | entropy=-1.10019 | reward=0.22317\n",
      "[INFO]  epoch/step=48/41400 | loss=0.28210 | ploss=-0.01396 | vloss=0.29715 | entropy=-1.08686 | reward=0.22547\n",
      "[INFO]  epoch/step=48/41500 | loss=0.28849 | ploss=-0.01597 | vloss=0.30559 | entropy=-1.12888 | reward=0.23198\n",
      "[INFO]  epoch/step=48/41600 | loss=0.29918 | ploss=-0.01539 | vloss=0.31569 | entropy=-1.11049 | reward=0.24316\n",
      "[INFO]  epoch/step=48/41700 | loss=0.27698 | ploss=-0.02659 | vloss=0.30468 | entropy=-1.11528 | reward=0.22537\n",
      "[INFO]  epoch/step=48/41800 | loss=0.29695 | ploss=-0.01339 | vloss=0.31146 | entropy=-1.12125 | reward=0.23749\n",
      "[INFO]  Save model to ./tmp/Amazon_Cellphones/train_agent/policy_model_epoch_48.ckpt\n",
      "[INFO]  epoch/step=49/41900 | loss=0.27460 | ploss=-0.02687 | vloss=0.30258 | entropy=-1.11257 | reward=0.23277\n",
      "[INFO]  epoch/step=49/42000 | loss=0.27300 | ploss=-0.02995 | vloss=0.30407 | entropy=-1.12030 | reward=0.22845\n",
      "[INFO]  epoch/step=49/42100 | loss=0.24964 | ploss=-0.03075 | vloss=0.28153 | entropy=-1.14030 | reward=0.21141\n",
      "[INFO]  epoch/step=49/42200 | loss=0.28780 | ploss=-0.01563 | vloss=0.30455 | entropy=-1.12381 | reward=0.23137\n",
      "[INFO]  epoch/step=49/42300 | loss=0.27633 | ploss=-0.02613 | vloss=0.30357 | entropy=-1.10300 | reward=0.22750\n",
      "[INFO]  epoch/step=49/42400 | loss=0.29031 | ploss=-0.01805 | vloss=0.30948 | entropy=-1.12027 | reward=0.23362\n",
      "[INFO]  epoch/step=49/42500 | loss=0.29598 | ploss=-0.01357 | vloss=0.31069 | entropy=-1.13935 | reward=0.23525\n",
      "[INFO]  epoch/step=49/42600 | loss=0.27996 | ploss=-0.02666 | vloss=0.30773 | entropy=-1.11588 | reward=0.23422\n",
      "[INFO]  epoch/step=49/42700 | loss=0.27993 | ploss=-0.02532 | vloss=0.30638 | entropy=-1.13680 | reward=0.22830\n",
      "[INFO]  Save model to ./tmp/Amazon_Cellphones/train_agent/policy_model_epoch_49.ckpt\n",
      "[INFO]  epoch/step=50/42800 | loss=0.28818 | ploss=-0.01799 | vloss=0.30730 | entropy=-1.11921 | reward=0.23046\n",
      "[INFO]  epoch/step=50/42900 | loss=0.27044 | ploss=-0.02913 | vloss=0.30069 | entropy=-1.12224 | reward=0.22392\n",
      "[INFO]  epoch/step=50/43000 | loss=0.29164 | ploss=-0.00873 | vloss=0.30147 | entropy=-1.10185 | reward=0.22771\n",
      "[INFO]  epoch/step=50/43100 | loss=0.28205 | ploss=-0.02301 | vloss=0.30618 | entropy=-1.11717 | reward=0.23548\n",
      "[INFO]  epoch/step=50/43200 | loss=0.28662 | ploss=-0.01893 | vloss=0.30665 | entropy=-1.09930 | reward=0.23115\n",
      "[INFO]  epoch/step=50/43300 | loss=0.26602 | ploss=-0.02952 | vloss=0.29666 | entropy=-1.11485 | reward=0.22223\n",
      "[INFO]  epoch/step=50/43400 | loss=0.25515 | ploss=-0.03746 | vloss=0.29371 | entropy=-1.10514 | reward=0.22325\n",
      "[INFO]  epoch/step=50/43500 | loss=0.30353 | ploss=-0.00554 | vloss=0.31019 | entropy=-1.12253 | reward=0.23133\n",
      "[INFO]  epoch/step=50/43600 | loss=0.28423 | ploss=-0.01516 | vloss=0.30051 | entropy=-1.12251 | reward=0.21950\n",
      "[INFO]  Save model to ./tmp/Amazon_Cellphones/train_agent/policy_model_epoch_50.ckpt\n"
     ]
    }
   ],
   "source": [
    "train(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "<class 'list'>\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'reshape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-6b979ceab583>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0ma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'reshape'"
     ]
    }
   ],
   "source": [
    "a = [1,2,3]\n",
    "k = [b for b in a]\n",
    "print(type(a))\n",
    "print(type(k))\n",
    "a.reshape(-1,1)\n",
    "\n",
    "next_state_batch = next_state_batch.reshape(-1,400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--dataset', type=str, default=BEAUTY, help='One of {clothing, cell, beauty, cd}')\n",
    "    parser.add_argument('--name', type=str, default='train_agent', help='directory name.')\n",
    "    parser.add_argument('--seed', type=int, default=123, help='random seed.')\n",
    "    parser.add_argument('--gpu', type=str, default='0', help='gpu device.')\n",
    "    parser.add_argument('--epochs', type=int, default=50, help='Max number of epochs.')\n",
    "    parser.add_argument('--batch_size', type=int, default=32, help='batch size.')\n",
    "    parser.add_argument('--lr', type=float, default=1e-4, help='learning rate.')\n",
    "    parser.add_argument('--max_acts', type=int, default=250, help='Max number of actions.')\n",
    "    parser.add_argument('--max_path_len', type=int, default=3, help='Max path length.')\n",
    "    parser.add_argument('--gamma', type=float, default=0.99, help='reward discount factor.')\n",
    "    parser.add_argument('--ent_weight', type=float, default=1e-3, help='weight factor for entropy loss')\n",
    "    parser.add_argument('--act_dropout', type=float, default=0.5, help='action dropout rate.')\n",
    "    parser.add_argument('--state_history', type=int, default=1, help='state history length')\n",
    "    parser.add_argument('--hidden', type=int, nargs='*', default=[512, 256], help='number of samples')\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = args.gpu\n",
    "    args.device = torch.device('cuda:0') if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "    args.log_dir = '{}/{}'.format(TMP_DIR[args.dataset], args.name)\n",
    "    if not os.path.isdir(args.log_dir):\n",
    "        os.makedirs(args.log_dir)\n",
    "\n",
    "    global logger\n",
    "    logger = get_logger(args.log_dir + '/train_log.txt')\n",
    "    logger.info(args)\n",
    "\n",
    "    set_random_seed(args.seed)\n",
    "    train(args)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0],\n",
      "        [1],\n",
      "        [1],\n",
      "        [0]])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected tensor [2, 2], src [3, 3] and index [2, 2] to have the same size apart from dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-09c8b474a186>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[0mc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected tensor [2, 2], src [3, 3] and index [2, 2] to have the same size apart from dimension 1"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "a = torch.tensor([[1,2,3],[4,5,6],[7,8,9]])\n",
    "b = torch.tensor([[0,1],[1,0]])\n",
    "\n",
    "\n",
    "\n",
    "print(b.view(-1,1))\n",
    "print(a.gather(1, b))\n",
    "c = a.gather(1, b.view(-1, 1)).view(-1)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1.],\n",
       "        [4., 3.]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.Tensor([[1,2],[3,4]])\n",
    "torch.gather(t, 1, torch.LongTensor([[0,0],[1,0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "t = np.array(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, state_dim, act_dim, gamma=0.99, hidden_sizes=[512, 256]):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        self.state_dim = state_dim\n",
    "        self.act_dim = act_dim\n",
    "        self.gamma = gamma\n",
    "\n",
    "        self.l1 = nn.Linear(state_dim, hidden_sizes[0])\n",
    "        self.l2 = nn.Linear(hidden_sizes[0], hidden_sizes[1])\n",
    "        self.actor = nn.Linear(hidden_sizes[1], act_dim)\n",
    "        self.critic = nn.Linear(hidden_sizes[1], 1)\n",
    "\n",
    "        self.saved_actions = []\n",
    "        self.rewards = []\n",
    "        self.entropy = []\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        state, act_mask = inputs  # state: [bs, state_dim], act_mask: [bs, act_dim] //batch_size\n",
    "\n",
    "        x = self.l1(state)\n",
    "\n",
    "        x = F.dropout(F.elu(x), p=0.5) #dropout是为了解决大网络但是小训练集的过拟合问题，但是可以一般性的避免过拟合。\n",
    "        out = self.l2(x)\n",
    "\n",
    "        x = F.dropout(F.elu(out), p=0.5)\n",
    "        actor_logits = self.actor(x)\n",
    "        actor_logits[1 - act_mask] = -999999.0\n",
    "        act_probs = F.softmax(actor_logits, dim=-1)  # Tensor of [bs, act_dim]\n",
    "        \n",
    "        state_values = self.critic(x)  # Tensor of [bs, 1]\n",
    "        return act_probs, state_values\n",
    "    \n",
    "    def select_action(self, batch_state, batch_act_mask, device):\n",
    "        state = torch.FloatTensor(batch_state).to(device)  # Tensor [bs, state_dim]\n",
    "        act_mask = torch.ByteTensor(batch_act_mask).to(device)  # Tensor of [bs, act_dim]\n",
    "        probs, value = self((state, act_mask))  # act_probs: [bs, act_dim], state_value: [bs, 1] forward\n",
    "\n",
    "        m = Categorical(probs)#32*251\n",
    "        #若probs = [0.2 0.2 0.3 0.3]\n",
    "        #那么 m.sample 取 0 1 2 3 的概率就对应probs\n",
    "        acts = m.sample()  # Tensor of [bs, ], requires_grad=False\n",
    "        \n",
    "        # [CAVEAT] If sampled action is out of action_space, choose the first action in action_space.\n",
    "        valid_idx = act_mask.gather(1, acts.view(-1, 1)).view(-1)#torch.gather(input=act_mask,dim=1,acts.view(-1,1))\n",
    "        #.view(-1行,1列)，-1 代表随便几行\n",
    "        acts[valid_idx == 0] = 0\n",
    "        #print('***********************************')\n",
    "        self.saved_actions.append(SavedAction(m.log_prob(acts), value))\n",
    "        \n",
    "        self.entropy.append(m.entropy())\n",
    "        \n",
    "        return acts.cpu().numpy().tolist()\n",
    "\n",
    "    def update(self, optimizer, device, ent_weight):\n",
    "        if len(self.rewards) <= 0:\n",
    "            del self.rewards[:]\n",
    "            del self.saved_actions[:]\n",
    "            del self.entropy[:]\n",
    "            return 0.0, 0.0, 0.0\n",
    "\n",
    "        batch_rewards = np.vstack(self.rewards).T  # numpy array of [bs, #steps]\n",
    "        batch_rewards = torch.FloatTensor(batch_rewards).to(device)\n",
    "        \n",
    "        num_steps = batch_rewards.shape[1]\n",
    "        for i in range(1, num_steps):\n",
    "            batch_rewards[:, num_steps - i - 1] += self.gamma * batch_rewards[:, num_steps - i]\n",
    "\n",
    "        actor_loss = 0\n",
    "        critic_loss = 0\n",
    "        entropy_loss = 0\n",
    "        \n",
    "        for i in range(0, num_steps):\n",
    "            log_prob, value = self.saved_actions[i]  # log_prob: Tensor of [bs, ], value: Tensor of [bs, 1]\n",
    "            advantage = batch_rewards[:, i] - value.squeeze(1)  # Tensor of [bs, ]\n",
    "            actor_loss += -log_prob * advantage.detach()  # Tensor of [bs, ]\n",
    "            critic_loss += advantage.pow(2)  # Tensor of [bs, ]\n",
    "            entropy_loss += -self.entropy[i]  # Tensor of [bs, ]\n",
    "        actor_loss = actor_loss.mean()\n",
    "        critic_loss = critic_loss.mean()\n",
    "        entropy_loss = entropy_loss.mean()\n",
    "        loss = actor_loss + critic_loss + ent_weight * entropy_loss\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        del self.rewards[:]\n",
    "        del self.saved_actions[:]\n",
    "        del self.entropy[:]\n",
    "        \n",
    "        #loss, ploss, vloss, eloss\n",
    "        return loss.item(), actor_loss.item(), critic_loss.item(), entropy_loss.item()\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
