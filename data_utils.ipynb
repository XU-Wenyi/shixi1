{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import gzip\n",
    "import pickle\n",
    "from easydict import EasyDict as edict\n",
    "import random\n",
    "\n",
    "\n",
    "class AmazonDataset(object):\n",
    "    \"\"\"This class is used to load data files and save in the instance.\"\"\"\n",
    "\n",
    "    def __init__(self, data_dir, set_name='train', word_sampling_rate=1e-4):\n",
    "        self.data_dir = data_dir\n",
    "        if not self.data_dir.endswith('/'):\n",
    "            self.data_dir += '/'\n",
    "        self.review_file = set_name + '.txt.gz'\n",
    "        self.load_entities()\n",
    "        self.load_product_relations()\n",
    "        self.load_reviews()\n",
    "        self.create_word_sampling_rate(word_sampling_rate)\n",
    "\n",
    "    def _load_file(self, filename):\n",
    "        with gzip.open(self.data_dir + filename, 'r') as f:\n",
    "            # In Python 3, must use decode() to convert bytes to string!\n",
    "            return [line.decode('utf-8').strip() for line in f]\n",
    "\n",
    "    def load_entities(self):\n",
    "        \"\"\"Load 6 global entities from data files:\n",
    "        `user`, `product`, `word`, `related_product`, `brand`, `category`.\n",
    "        Create a member variable for each entity associated with attributes:\n",
    "        - `vocab`: a list of string indicating entity values.\n",
    "        - `vocab_size`: vocabulary size.\n",
    "        \"\"\"\n",
    "        entity_files = edict(\n",
    "                user='users.txt.gz',\n",
    "                product='product.txt.gz',\n",
    "                word='vocab.txt.gz',\n",
    "                related_product='related_product.txt.gz',\n",
    "                brand='brand.txt.gz',\n",
    "                category='category.txt.gz',\n",
    "        )\n",
    "        for name in entity_files:\n",
    "            vocab = self._load_file(entity_files[name])\n",
    "            setattr(self, name, edict(vocab=vocab, vocab_size=len(vocab)))\n",
    "            print('Load', name, 'of size', len(vocab))\n",
    "            print('vocab: ',vocab[0],';',vocab[1],';',vocab[2])\n",
    "\n",
    "    def load_reviews(self):\n",
    "        \"\"\"Load user-product reviews from train/test data files.\n",
    "        Create member variable `review` associated with following attributes:\n",
    "        - `data`: list of tuples (user_idx, product_idx, [word_idx...]).\n",
    "        - `size`: number of reviews.\n",
    "        - `product_distrib`: product vocab frequency among all reviews.\n",
    "        - `product_uniform_distrib`: product vocab frequency (all 1's)\n",
    "        - `word_distrib`: word vocab frequency among all reviews.\n",
    "        - `word_count`: number of words (including duplicates).\n",
    "        - `review_distrib`: always 1.\n",
    "        \"\"\"\n",
    "        review_data = []  # (user_idx, product_idx, [word1_idx,...,wordn_idx])\n",
    "        product_distrib = np.zeros(self.product.vocab_size)#product的vocab-产品序列号\n",
    "        #word_distrib = np.zeros(self.word.vocab_size)\n",
    "        #word_count = 0\n",
    "        for line in self._load_file(self.review_file):\n",
    "            arr = line.split('\\t')\n",
    "            user_idx = int(arr[0])\n",
    "            product_idx = int(arr[1])\n",
    "            #word_indices = [int(i) for i in arr[2].split(' ')]  # list of word idx\n",
    "            review_data.append((user_idx, product_idx))\n",
    "            '''\n",
    "            user-product 对应，感觉product_distrib也不应该有\n",
    "            review_distrib --- ?\n",
    "            '''\n",
    "            #review_data.append((user_idx, product_idx, word_indices))\n",
    "            product_distrib[product_idx] += 1\n",
    "            #for wi in word_indices:\n",
    "            #    word_distrib[wi] += 1\n",
    "            #word_count += len(word_indices)\n",
    "        self.review = edict(\n",
    "                data=review_data,#(user_idx,product_idx)\n",
    "                size=len(review_data),\n",
    "                product_distrib=product_distrib,\n",
    "                product_uniform_distrib=np.ones(self.product.vocab_size),\n",
    "                #word_distrib=word_distrib,\n",
    "                #word_count=word_count,\n",
    "                #review_distrib=np.ones(len(review_data)) #set to 1 now\n",
    "        )\n",
    "        #print('Load review of size', self.review.size, 'word count=', word_count)\n",
    "\n",
    "    def load_product_relations(self):\n",
    "        \"\"\"Load 5 product -> ? relations:\n",
    "        - `produced_by`: product -> brand,\n",
    "        - `belongs_to`: product -> category,\n",
    "        - `also_bought`: product -> related_product,\n",
    "        - `also_viewed`: product -> related_product,\n",
    "        - `bought_together`: product -> related_product,\n",
    "        Create member variable for each relation associated with following attributes:\n",
    "        - `data`: list of list of entity_tail indices (can be empty).\n",
    "        - `et_vocab`: vocabulary of entity_tail (copy from entity vocab).\n",
    "        - `et_distrib`: frequency of entity_tail vocab.\n",
    "        \"\"\"\n",
    "        product_relations = edict(\n",
    "                produced_by=('brand_p_b.txt.gz', self.brand),  # (filename, entity_tail)\n",
    "                belongs_to=('category_p_c.txt.gz', self.category),\n",
    "                also_bought=('also_bought_p_p.txt.gz', self.related_product),'''also_viewed和also_bought都是related product？'''\n",
    "                also_viewed=('also_viewed_p_p.txt.gz', self.related_product),\n",
    "                bought_together=('bought_together_p_p.txt.gz', self.related_product),\n",
    "        )\n",
    "        for name in product_relations:\n",
    "            print('relation-name-',name)\n",
    "            # We save information of entity_tail (et) in each relation.\n",
    "            # Note that `data` variable saves list of entity_tail indices.\n",
    "            # The i-th record of `data` variable is the entity_tail idx (i.e. product_idx=i).\n",
    "            # So for each product-relation, there are always |products| records.\n",
    "            relation = edict(\n",
    "                    data=[],\n",
    "                    et_vocab=product_relations[name][1].vocab, #copy of brand, catgory ... 's vocab \n",
    "                    et_distrib=np.zeros(product_relations[name][1].vocab_size) #[1] means self.brand ..\n",
    "            )\n",
    "            for line in self._load_file(product_relations[name][0]): #[0] means brand_p_b.txt.gz ..\n",
    "                knowledge = []\n",
    "                for x in line.split(' '):  # some lines may be empty\n",
    "                    if len(x) > 0:\n",
    "                        x = int(x)\n",
    "                        knowledge.append(x)\n",
    "                        relation.et_distrib[x] += 1\n",
    "                relation.data.append(knowledge)\n",
    "            setattr(self, name, relation)\n",
    "            print('Load', name, 'of size', len(relation.data))\n",
    "\n",
    "    def create_word_sampling_rate(self, sampling_threshold):\n",
    "        print('Create word sampling rate')\n",
    "        self.word_sampling_rate = np.ones(self.word.vocab_size)\n",
    "        if sampling_threshold <= 0:\n",
    "            return\n",
    "        threshold = sum(self.review.word_distrib) * sampling_threshold\n",
    "        for i in range(self.word.vocab_size):\n",
    "            if self.review.word_distrib[i] == 0:\n",
    "                continue\n",
    "            self.word_sampling_rate[i] = min((np.sqrt(float(self.review.word_distrib[i]) / threshold) + 1) * threshold / float(self.review.word_distrib[i]), 1.0)\n",
    "\n",
    "\n",
    "class AmazonDataLoader(object):\n",
    "    \"\"\"This class acts as the dataloader for training knowledge graph embeddings.\"\"\"\n",
    "\n",
    "    def __init__(self, dataset, batch_size):\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        #self.review_size = self.dataset.review.size\n",
    "        self.product_relations = ['produced_by', 'belongs_to', 'also_bought', 'also_viewed', 'bought_together']\n",
    "        self.finished_word_num = 0\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        # Shuffle reviews order\n",
    "        #self.review_seq = np.random.permutation(self.review_size)\n",
    "        self.cur_review_i = 0\n",
    "        self.cur_word_i = 0\n",
    "        self._has_next = True\n",
    "\n",
    "    def get_batch(self):\n",
    "        \"\"\"Return a matrix of [batch_size x 8], where each row contains\n",
    "        (u_id, p_id, w_id, b_id, c_id, rp_id, rp_id, rp_id).\n",
    "        \"\"\"\n",
    "        batch = []\n",
    "        review_idx = self.review_seq[self.cur_review_i]\n",
    "        \n",
    "        user_idx, product_idx, text_list = self.dataset.review.data[review_idx]\n",
    "        '''\n",
    "        self.review = edict(\n",
    "                data=review_data,\n",
    "                #size=len(review_data),\n",
    "                #product_distrib=product_distrib,\n",
    "                product_uniform_distrib=np.ones(self.product.vocab_size),\n",
    "                #word_distrib=word_distrib,\n",
    "                #word_count=word_count,\n",
    "                #review_distrib=np.ones(len(review_data)) #set to 1 now\n",
    "        )\n",
    "        '''\n",
    "        product_knowledge = {pr: getattr(self.dataset, pr).data[product_idx] for pr in self.product_relations}\n",
    "        print('user:',user_idx,' product:',product_idx,'text_list:',text_list[0])\n",
    "        print('product_knowledge:',product_knowledge)\n",
    "        while len(batch) < self.batch_size:\n",
    "            # 1) Sample the word\n",
    "            word_idx = text_list[self.cur_word_i]# text_list does not exist\n",
    "            print('cur_word_i:',self.cur_word_i)\n",
    "            print('word_idx:',word_idx)\n",
    "            '''这个地方不知道怎么改！'''\n",
    "            if random.random() < self.dataset.word_sampling_rate[word_idx]:\n",
    "                data = [user_idx, product_idx, word_idx] #没有word index, 因为没有text_list\n",
    "                #data = [user_idx, product_idx] ''''''\n",
    "                for pr in self.product_relations:\n",
    "                    if len(product_knowledge[pr]) <= 0:\n",
    "                        data.append(-1)\n",
    "                    else:\n",
    "                        data.append(random.choice(product_knowledge[pr]))\n",
    "                batch.append(data)\n",
    "            '''\n",
    "            # 2) Move to next word/review\n",
    "            self.cur_word_i += 1\n",
    "            self.finished_word_num += 1\n",
    "            if self.cur_word_i >= len(text_list): #meiyou!\n",
    "                self.cur_review_i += 1\n",
    "                if self.cur_review_i >= self.review_size:\n",
    "                    self._has_next = False\n",
    "                    break\n",
    "                self.cur_word_i = 0\n",
    "                review_idx = self.review_seq[self.cur_review_i]\n",
    "                user_idx, product_idx, text_list = self.dataset.review.data[review_idx]\n",
    "            '''\n",
    "            product_knowledge = {pr: getattr(self.dataset, pr).data[product_idx] for pr in self.product_relations}\n",
    "            \n",
    "        print('batch:',np.array(batch))\n",
    "        return np.array(batch)\n",
    "\n",
    "    def has_next(self):\n",
    "        \"\"\"Has next batch.\"\"\"\n",
    "        return self._has_next\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
