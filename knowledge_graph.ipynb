{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "from math import log\n",
    "from tqdm import tqdm\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "import gzip\n",
    "import pickle\n",
    "import random\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "from utils import *\n",
    "from data_utils import AmazonDataset\n",
    "\n",
    "\n",
    "class KnowledgeGraph(object):\n",
    "\n",
    "    def __init__(self, dataset):\n",
    "        self.G = dict()\n",
    "        self._load_entities(dataset)\n",
    "        #self._load_reviews(dataset)\n",
    "        self._load_knowledge(dataset)\n",
    "        self._clean()\n",
    "        self.top_matches = None\n",
    "\n",
    "    #1.\n",
    "    '''\n",
    "    def get_entities():\n",
    "        return list(KG_RELATION.keys())\n",
    "    '''\n",
    "    def _load_entities(self, dataset):\n",
    "        print('Load entities...')\n",
    "        num_nodes = 0\n",
    "        for entity in get_entities():# from utils import *\n",
    "            self.G[entity] = {}\n",
    "            vocab_size = getattr(dataset, entity).vocab_size #在dataset里找到entity属性的东西\n",
    "            '''\n",
    "            >>>class A(object):\n",
    "            ...     bar = 1\n",
    "            ... \n",
    "            >>> a = A()\n",
    "            >>> getattr(a, 'bar')        # 获取属性 bar 值\n",
    "            1\n",
    "            '''\n",
    "            for eid in range(vocab_size):\n",
    "                self.G[entity][eid] = {r: [] for r in get_relations(entity)}\n",
    "            num_nodes += vocab_size\n",
    "        print('Total {:d} nodes.'.format(num_nodes))\n",
    "\n",
    "    #2.\n",
    "    def _load_reviews(self, dataset, word_tfidf_threshold=0.1, word_freq_threshold=5000):\n",
    "        print('Load reviews...')\n",
    "        # (1) Filter words by both tfidf and frequency.\n",
    "        vocab = dataset.word.vocab\n",
    "        reviews = [d[2] for d in dataset.review.data]\n",
    "        review_tfidf = compute_tfidf_fast(vocab, reviews)\n",
    "        distrib = dataset.review.word_distrib\n",
    "\n",
    "        num_edges = 0\n",
    "        all_removed_words = []\n",
    "        for rid, data in enumerate(dataset.review.data):\n",
    "            #review id,user ,product id \n",
    "            uid, pid, review = data\n",
    "            doc_tfidf = review_tfidf[rid].toarray()[0]\n",
    "            \n",
    "            remained_words = [wid for wid in set(review)\n",
    "                              if doc_tfidf[wid] >= word_tfidf_threshold\n",
    "                              and distrib[wid] <= word_freq_threshold]\n",
    "            \n",
    "            removed_words = set(review).difference(remained_words)  # only for visualize\n",
    "            removed_words = [vocab[wid] for wid in removed_words]\n",
    "            all_removed_words.append(removed_words)\n",
    "            if len(remained_words) <= 0:\n",
    "                continue\n",
    "\n",
    "            # (2) Add edges.\n",
    "            self._add_edge(USER, uid, PURCHASE, PRODUCT, pid)\n",
    "            num_edges += 2\n",
    "            for wid in remained_words:\n",
    "                self._add_edge(USER, uid, MENTION, WORD, wid)\n",
    "                self._add_edge(PRODUCT, pid, DESCRIBED_AS, WORD, wid)\n",
    "                num_edges += 4\n",
    "        print('Total {:d} review edges.'.format(num_edges))\n",
    "\n",
    "        with open('./tmp/review_removed_words.txt', 'w') as f:\n",
    "            f.writelines([' '.join(words) + '\\n' for words in all_removed_words])\n",
    "            \n",
    "    #3.\n",
    "    def _load_knowledge(self, dataset):\n",
    "        for relation in [PRODUCED_BY, BELONG_TO, ALSO_BOUGHT, ALSO_VIEWED, BOUGHT_TOGETHER]:\n",
    "            print('Load knowledge {}...'.format(relation))\n",
    "            data = getattr(dataset, relation).data\n",
    "            num_edges = 0\n",
    "            for pid, eids in enumerate(data):\n",
    "                if len(eids) <= 0:\n",
    "                    continue\n",
    "                for eid in set(eids):\n",
    "                    et_type = get_entity_tail(PRODUCT, relation)\n",
    "                    self._add_edge(PRODUCT, pid, relation, et_type, eid)\n",
    "                    num_edges += 2\n",
    "            print('Total {:d} {:s} edges.'.format(num_edges, relation))\n",
    "\n",
    "    def _add_edge(self, etype1, eid1, relation, etype2, eid2):\n",
    "        self.G[etype1][eid1][relation].append(eid2)\n",
    "        self.G[etype2][eid2][relation].append(eid1)\n",
    "        \n",
    "    #4.\n",
    "    def _clean(self):\n",
    "        print('Remove duplicates...')\n",
    "        for etype in self.G:\n",
    "            for eid in self.G[etype]:\n",
    "                for r in self.G[etype][eid]:\n",
    "                    data = self.G[etype][eid][r]\n",
    "                    data = tuple(sorted(set(data)))\n",
    "                    self.G[etype][eid][r] = data\n",
    "\n",
    "    def compute_degrees(self):\n",
    "        print('Compute node degrees...')\n",
    "        self.degrees = {}\n",
    "        self.max_degree = {}\n",
    "        for etype in self.G:\n",
    "            self.degrees[etype] = {}\n",
    "            for eid in self.G[etype]:\n",
    "                count = 0\n",
    "                for r in self.G[etype][eid]:\n",
    "                    count += len(self.G[etype][eid][r])\n",
    "                self.degrees[etype][eid] = count\n",
    "\n",
    "    def get(self, eh_type, eh_id=None, relation=None):\n",
    "        data = self.G\n",
    "        if eh_type is not None:\n",
    "            data = data[eh_type]\n",
    "        if eh_id is not None:\n",
    "            data = data[eh_id]\n",
    "        if relation is not None:\n",
    "            data = data[relation]\n",
    "        return data\n",
    "\n",
    "    def __call__(self, eh_type, eh_id=None, relation=None):\n",
    "        return self.get(eh_type, eh_id, relation)\n",
    "\n",
    "    def get_tails(self, entity_type, entity_id, relation):\n",
    "        return self.G[entity_type][entity_id][relation]\n",
    "\n",
    "    def get_tails_given_user(self, entity_type, entity_id, relation, user_id):\n",
    "        \"\"\" Very important!\n",
    "        :param entity_type:\n",
    "        :param entity_id:\n",
    "        :param relation:\n",
    "        :param user_id:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        tail_type = KG_RELATION[entity_type][relation]\n",
    "        tail_ids = self.G[entity_type][entity_id][relation]\n",
    "        if tail_type not in self.top_matches:\n",
    "            return tail_ids\n",
    "        top_match_set = set(self.top_matches[tail_type][user_id])\n",
    "        top_k = len(top_match_set)\n",
    "        if len(tail_ids) > top_k:\n",
    "            tail_ids = top_match_set.intersection(tail_ids)\n",
    "        return list(tail_ids)\n",
    "\n",
    "    def trim_edges(self):\n",
    "        degrees = {}\n",
    "        for entity in self.G:\n",
    "            degrees[entity] = {}\n",
    "            for eid in self.G[entity]:\n",
    "                for r in self.G[entity][eid]:\n",
    "                    if r not in degrees[entity]:\n",
    "                        degrees[entity][r] = []\n",
    "                    degrees[entity][r].append(len(self.G[entity][eid][r]))\n",
    "\n",
    "        for entity in degrees:\n",
    "            for r in degrees[entity]:\n",
    "                tmp = sorted(degrees[entity][r], reverse=True)\n",
    "                print(entity, r, tmp[:10])\n",
    "\n",
    "    def set_top_matches(self, u_u_match, u_p_match, u_w_match):\n",
    "        self.top_matches = {\n",
    "            USER: u_u_match,\n",
    "            PRODUCT: u_p_match,\n",
    "            WORD: u_w_match,\n",
    "        }\n",
    "\n",
    "    def heuristic_search(self, uid, pid, pattern_id, trim_edges=False):\n",
    "        if trim_edges and self.top_matches is None:\n",
    "            raise Exception('To enable edge-trimming, must set top_matches of users first!')\n",
    "        if trim_edges:\n",
    "            _get = lambda e, i, r: self.get_tails_given_user(e, i, r, uid)\n",
    "        else:\n",
    "            _get = lambda e, i, r: self.get_tails(e, i, r)\n",
    "\n",
    "        pattern = PATH_PATTERN[pattern_id]\n",
    "        paths = []\n",
    "        if pattern_id == 1:  # OK\n",
    "            wids_u = set(_get(USER, uid, MENTION))  # USER->MENTION->WORD\n",
    "            wids_p = set(_get(PRODUCT, pid, DESCRIBED_AS))  # PRODUCT->DESCRIBE->WORD\n",
    "            intersect_nodes = wids_u.intersection(wids_p)\n",
    "            paths = [(uid, x, pid) for x in intersect_nodes]\n",
    "        elif pattern_id in [11, 12, 13, 14, 15, 16, 17]:\n",
    "            pids_u = set(_get(USER, uid, PURCHASE))  # USER->PURCHASE->PRODUCT\n",
    "            pids_u = pids_u.difference([pid])  # exclude target product\n",
    "            nodes_p = set(_get(PRODUCT, pid, pattern[3][0]))  # PRODUCT->relation->node2\n",
    "            if pattern[2][1] == USER:\n",
    "                nodes_p.difference([uid])\n",
    "            for pid_u in pids_u:\n",
    "                relation, entity_tail = pattern[2][0], pattern[2][1]\n",
    "                et_ids = set(_get(PRODUCT, pid_u, relation))  # USER->PURCHASE->PRODUCT->relation->node2\n",
    "                intersect_nodes = et_ids.intersection(nodes_p)\n",
    "                tmp_paths = [(uid, pid_u, x, pid) for x in intersect_nodes]\n",
    "                paths.extend(tmp_paths)\n",
    "        elif pattern_id == 18:\n",
    "            wids_u = set(_get(USER, uid, MENTION))  # USER->MENTION->WORD\n",
    "            uids_p = set(_get(PRODUCT, pid, PURCHASE))  # PRODUCT->PURCHASE->USER\n",
    "            uids_p = uids_p.difference([uid])  # exclude source user\n",
    "            for uid_p in uids_p:\n",
    "                wids_u_p = set(_get(USER, uid_p, MENTION))  # PRODUCT->PURCHASE->USER->MENTION->WORD\n",
    "                intersect_nodes = wids_u.intersection(wids_u_p)\n",
    "                tmp_paths = [(uid, x, uid_p, pid) for x in intersect_nodes]\n",
    "                paths.extend(tmp_paths)\n",
    "\n",
    "        return paths\n",
    "\n",
    "\n",
    "def check_test_path(dataset_str, kg):\n",
    "    # Check if there exists at least one path for any user-product in test set.\n",
    "    test_user_products = load_labels(dataset_str, 'test')\n",
    "    for uid in test_user_products:\n",
    "        for pid in test_user_products[uid]:\n",
    "            count = 0\n",
    "            for pattern_id in [1, 11, 12, 13, 14, 15, 16, 17, 18]:\n",
    "                tmp_path = kg.heuristic_search(uid, pid, pattern_id)\n",
    "                count += len(tmp_path)\n",
    "            if count == 0:\n",
    "                print(uid, pid)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
