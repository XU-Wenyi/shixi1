{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "from easydict import EasyDict as edict\n",
    "import numpy as np\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from utils import *\n",
    "from data_utils import AmazonDataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KnowledgeEmbedding(nn.Module):\n",
    "    def __init__(self, dataset, args):\n",
    "        super(KnowledgeEmbedding, self).__init__()\n",
    "        self.embed_size = args.embed_size\n",
    "        self.num_neg_samples = args.num_neg_samples\n",
    "        self.device = args.device\n",
    "        self.l2_lambda = args.l2_lambda\n",
    "\n",
    "        # Initialize entity embeddings.\n",
    "        self.entities = edict(\n",
    "            user=edict(vocab_size=dataset.user.vocab_size),\n",
    "            product=edict(vocab_size=dataset.product.vocab_size),\n",
    "            word=edict(vocab_size=dataset.word.vocab_size),\n",
    "            related_product=edict(vocab_size=dataset.related_product.vocab_size),\n",
    "            brand=edict(vocab_size=dataset.brand.vocab_size),\n",
    "            category=edict(vocab_size=dataset.category.vocab_size),\n",
    "        )\n",
    "        for e in self.entities:\n",
    "            embed = self._entity_embedding(self.entities[e].vocab_size)\n",
    "            setattr(self, e, embed)\n",
    "        \n",
    "        # Initialize relation embeddings and relation biases.\n",
    "        self.relations = edict(\n",
    "            purchase=edict(\n",
    "                et='product',\n",
    "                et_distrib=self._make_distrib(dataset.review.product_uniform_distrib)),\n",
    "            mentions=edict(\n",
    "                et='word',\n",
    "                et_distrib=self._make_distrib(dataset.review.word_distrib)),\n",
    "            describe_as=edict(\n",
    "                et='word',\n",
    "                et_distrib=self._make_distrib(dataset.review.word_distrib)),\n",
    "            produced_by=edict(\n",
    "                et='brand',\n",
    "                et_distrib=self._make_distrib(dataset.produced_by.et_distrib)),\n",
    "            belongs_to=edict(\n",
    "                et='category',\n",
    "                et_distrib=self._make_distrib(dataset.belongs_to.et_distrib)),\n",
    "            also_bought=edict(\n",
    "                et='related_product',\n",
    "                et_distrib=self._make_distrib(dataset.also_bought.et_distrib)),\n",
    "            also_viewed=edict(\n",
    "                et='related_product',\n",
    "                et_distrib=self._make_distrib(dataset.also_viewed.et_distrib)),\n",
    "            bought_together=edict(\n",
    "                et='related_product',\n",
    "                et_distrib=self._make_distrib(dataset.bought_together.et_distrib)),\n",
    "        )\n",
    "        for r in self.relations:\n",
    "            embed = self._relation_embedding()\n",
    "            setattr(self, r, embed)\n",
    "            bias = self._relation_bias(len(self.relations[r].et_distrib))\n",
    "            setattr(self, r + '_bias', bias)\n",
    "\n",
    "    def _entity_embedding(self, vocab_size):\n",
    "        \"\"\"Create entity embedding of size [vocab_size+1, embed_size].\n",
    "            Note that last dimension is always 0's.\n",
    "            初始化，uniform分布\n",
    "        \"\"\"\n",
    "        embed = nn.Embedding(vocab_size + 1, self.embed_size, padding_idx=-1, sparse=False)\n",
    "        initrange = 0.5 / self.embed_size\n",
    "        weight = torch.FloatTensor(vocab_size + 1, self.embed_size).uniform_(-initrange, initrange)\n",
    "        embed.weight = nn.Parameter(weight)\n",
    "        return embed\n",
    "\n",
    "    def _relation_embedding(self):\n",
    "        \"\"\"Create relation vector of size [1, embed_size].\"\"\"\n",
    "        initrange = 0.5 / self.embed_size\n",
    "        weight = torch.FloatTensor(1, self.embed_size).uniform_(-initrange, initrange)\n",
    "        embed = nn.Parameter(weight)\n",
    "        return embed\n",
    "\n",
    "    def _relation_bias(self, vocab_size):\n",
    "        \"\"\"Create relation bias of size [vocab_size+1].\"\"\"\n",
    "        bias = nn.Embedding(vocab_size + 1, 1, padding_idx=-1, sparse=False)\n",
    "        bias.weight = nn.Parameter(torch.zeros(vocab_size + 1, 1))\n",
    "        return bias\n",
    "\n",
    "    def _make_distrib(self, distrib):\n",
    "        \"\"\"Normalize input numpy vector to distribution.\"\"\"\n",
    "        distrib = np.power(np.array(distrib, dtype=np.float), 0.75)\n",
    "        distrib = distrib / distrib.sum()\n",
    "        distrib = torch.FloatTensor(distrib).to(self.device)\n",
    "        return distrib\n",
    "        #np.array 的0.75次方\n",
    "\n",
    "    def forward(self, batch_idxs):\n",
    "        loss = self.compute_loss(batch_idxs)\n",
    "        return loss\n",
    "\n",
    "    def compute_loss(self, batch_idxs):\n",
    "        \"\"\"Compute knowledge graph negative sampling loss.\n",
    "        batch_idxs: batch_size * 8 array, where each row is\n",
    "                (u_id, p_id, w_id, b_id, c_id, rp_id, rp_id, rp_id).\n",
    "        \"\"\"\n",
    "        user_idxs = batch_idxs[:, 0]\n",
    "        product_idxs = batch_idxs[:, 1]\n",
    "        word_idxs = batch_idxs[:, 2]\n",
    "        brand_idxs = batch_idxs[:, 3]\n",
    "        category_idxs = batch_idxs[:, 4]\n",
    "        rproduct1_idxs = batch_idxs[:, 5]\n",
    "        rproduct2_idxs = batch_idxs[:, 6]\n",
    "        rproduct3_idxs = batch_idxs[:, 7]\n",
    "\n",
    "        regularizations = []\n",
    "\n",
    "        # user + purchase -> product\n",
    "        up_loss, up_embeds = self.neg_loss('user', 'purchase', 'product', user_idxs, product_idxs)\n",
    "        regularizations.extend(up_embeds)\n",
    "        #extend 类似于 append \n",
    "        loss = up_loss\n",
    "\n",
    "        # user + mentions -> word\n",
    "        uw_loss, uw_embeds = self.neg_loss('user', 'mentions', 'word', user_idxs, word_idxs)\n",
    "        regularizations.extend(uw_embeds)\n",
    "        loss += uw_loss\n",
    "\n",
    "        # product + describe_as -> word\n",
    "        pw_loss, pw_embeds = self.neg_loss('product', 'describe_as', 'word', product_idxs, word_idxs)\n",
    "        regularizations.extend(pw_embeds)\n",
    "        loss += pw_loss\n",
    "\n",
    "        # product + produced_by -> brand\n",
    "        pb_loss, pb_embeds = self.neg_loss('product', 'produced_by', 'brand', product_idxs, brand_idxs)\n",
    "        if pb_loss is not None:\n",
    "            regularizations.extend(pb_embeds)\n",
    "            loss += pb_loss\n",
    "\n",
    "        # product + belongs_to -> category\n",
    "        pc_loss, pc_embeds = self.neg_loss('product', 'belongs_to', 'category', product_idxs, category_idxs)\n",
    "        if pc_loss is not None:\n",
    "            regularizations.extend(pc_embeds)\n",
    "            loss += pc_loss\n",
    "\n",
    "        # product + also_bought -> related_product1\n",
    "        pr1_loss, pr1_embeds = self.neg_loss('product', 'also_bought', 'related_product', product_idxs, rproduct1_idxs)\n",
    "        if pr1_loss is not None:\n",
    "            regularizations.extend(pr1_embeds)\n",
    "            loss += pr1_loss\n",
    "\n",
    "        # product + also_viewed -> related_product2\n",
    "        pr2_loss, pr2_embeds = self.neg_loss('product', 'also_viewed', 'related_product', product_idxs, rproduct2_idxs)\n",
    "        if pr2_loss is not None:\n",
    "            regularizations.extend(pr2_embeds)\n",
    "            loss += pr2_loss\n",
    "\n",
    "        # product + bought_together -> related_product3\n",
    "        pr3_loss, pr3_embeds = self.neg_loss('product', 'bought_together', 'related_product', product_idxs, rproduct3_idxs)\n",
    "        if pr3_loss is not None:\n",
    "            regularizations.extend(pr3_embeds)\n",
    "            loss += pr3_loss\n",
    "\n",
    "        # l2 regularization\n",
    "        if self.l2_lambda > 0:\n",
    "            l2_loss = 0.0\n",
    "            for term in regularizations:\n",
    "                l2_loss += torch.norm(term)\n",
    "            loss += self.l2_lambda * l2_loss\n",
    "        \n",
    "        print('up uw pw brand cat rp1 rp2 rp2')\n",
    "        print(regularizations)\n",
    "        #print('reg')\n",
    "        #print()\n",
    "        return loss\n",
    "\n",
    "    def neg_loss(self, entity_head, relation, entity_tail, entity_head_idxs, entity_tail_idxs):\n",
    "        #par example:\n",
    "        #entity_head = 'product',relation = 'bought_together', entity_tail = 'related_product', \n",
    "        #entity_head_idxs = prod_id, tail_idx=rp_id\n",
    "        \n",
    "        # Entity tail indices can be -1. Remove these indices. Batch size may be changed!\n",
    "        mask = entity_tail_idxs >= 0 #mask = True(1) or false(0) \n",
    "        fixed_entity_head_idxs = entity_head_idxs[mask]\n",
    "        fixed_entity_tail_idxs = entity_tail_idxs[mask]\n",
    "        if fixed_entity_head_idxs.size(0) <= 0:\n",
    "            return None, []\n",
    "\n",
    "        entity_head_embedding = getattr(self, entity_head)  # nn.Embedding\n",
    "        entity_tail_embedding = getattr(self, entity_tail)  # nn.Embedding\n",
    "        relation_vec = getattr(self, relation)  # [1, embed_size]\n",
    "        relation_bias_embedding = getattr(self, relation + '_bias')  # nn.Embedding\n",
    "        entity_tail_distrib = self.relations[relation].et_distrib  # [vocab_size]\n",
    "\n",
    "        return kg_neg_loss(entity_head_embedding, entity_tail_embedding,\n",
    "                           fixed_entity_head_idxs, fixed_entity_tail_idxs,\n",
    "                           relation_vec, relation_bias_embedding, self.num_neg_samples, entity_tail_distrib)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#entity_head_embedding, entity_tail_embedding,\n",
    "#fixed_entity_head_idxs, fixed_entity_tail_idxs,\n",
    "#relation_vec, relation_bias_embedding, self.num_neg_samples, entity_tail_distrib\n",
    "\n",
    "#entity_head_embed+relation->entity_tail_embed\n",
    "\n",
    "#entity_head_embed, entity_tail_embed, \n",
    "#entity_head_idxs, entity_tail_idxs,\n",
    "#relation_vec, relation_bias_embed, num_samples, distrib\n",
    "\n",
    "def kg_neg_loss(entity_head_embed, entity_tail_embed, \n",
    "                entity_head_idxs, entity_tail_idxs,\n",
    "                relation_vec, relation_bias_embed, num_samples, distrib):\n",
    "    \"\"\"Compute negative sampling loss for triple (entity_head, relation, entity_tail).\n",
    "\n",
    "    Args:\n",
    "        entity_head_embed: Tensor of size [batch_size, embed_size].\n",
    "        entity_tail_embed: Tensor of size [batch_size, embed_size].\n",
    "        entity_head_idxs:\n",
    "        entity_tail_idxs:\n",
    "        relation_vec: Parameter of size [1, embed_size].\n",
    "        relation_bias: Tensor of size [batch_size]\n",
    "        num_samples: An integer.\n",
    "        distrib: Tensor of size [vocab_size].\n",
    "\n",
    "    Returns:\n",
    "        A tensor of [1].\n",
    "    \"\"\"\n",
    "    batch_size = entity_head_idxs.size(0)\n",
    "    entity_head_vec = entity_head_embed(entity_head_idxs)  # [batch_size, embed_size]\n",
    "    example_vec = entity_head_vec + relation_vec  # [batch_size, embed_size]\n",
    "    example_vec = example_vec.unsqueeze(2)  # [batch_size, embed_size, 1]\n",
    "\n",
    "    \n",
    "    entity_tail_vec = entity_tail_embed(entity_tail_idxs)  # [batch_size, embed_size]\n",
    "    pos_vec = entity_tail_vec.unsqueeze(1)  # [batch_size, 1, embed_size]\n",
    "    relation_bias = relation_bias_embed(entity_tail_idxs).squeeze(1)  # [batch_size]\n",
    "    pos_logits = torch.bmm(pos_vec, example_vec).squeeze() + relation_bias  # [batch_size]\n",
    "    pos_loss = -pos_logits.sigmoid().log()  # [batch_size]\n",
    "\n",
    "    \n",
    "    neg_sample_idx = torch.multinomial(distrib, num_samples, replacement=True).view(-1)\n",
    "    neg_vec = entity_tail_embed(neg_sample_idx)  # [num_samples, embed_size]\n",
    "    neg_logits = torch.mm(example_vec.squeeze(2), neg_vec.transpose(1, 0).contiguous())\n",
    "    neg_logits += relation_bias.unsqueeze(1)  # [batch_size, num_samples]\n",
    "    neg_loss = -neg_logits.neg().sigmoid().log().sum(1)  # [batch_size] #.neg()取负值 1->-1\n",
    "\n",
    "    loss = (pos_loss + neg_loss).mean()\n",
    "    print('kg_neg_loss:')\n",
    "    print(loss)\n",
    "    #print([entity_head_vec, entity_tail_vec, neg_vec])\n",
    "    return loss, [entity_head_vec, entity_tail_vec, neg_vec]\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
