{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "from collections import namedtuple\n",
    "#import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import argparse\n",
    "\n",
    "from knowledge_graph import KnowledgeGraph\n",
    "from kg_env import BatchKGEnvironment\n",
    "from utils import *\n",
    "\n",
    "logger = None\n",
    "\n",
    "SavedAction = namedtuple('SavedAction', ['log_prob'])#, 'value'])\n",
    "\n",
    "# Hyper Parameters for DQN\n",
    "GAMMA = 0.9 # discount factor for target Q\n",
    "INITIAL_EPSILON = 0.5 # starting value of epsilon\n",
    "FINAL_EPSILON = 0.01 # final value of epsilon\n",
    "REPLAY_SIZE = 10000 # experience replay buffer size\n",
    "BATCH_SIZE = 32 # size of minibatch\n",
    "REPLACE_TARGET_FREQ = 10 # frequency to update target Q network\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    # DQN Agent\n",
    "    def __init__(self, env, state_dim, act_dim, gamma=0.99,hidden_sizes=[512,256]):\n",
    "        super(DQN, self).__init__()\n",
    "        '''\n",
    "        class Net(nn.Module):\n",
    "        def __init__(self):\n",
    "        super().__init__()\n",
    "        '''\n",
    "        self.replay_buffer = deque() #双向队列 可以从左append些什么\n",
    "        # init some parameters\n",
    "        self.time_step = 0\n",
    "        self.epsilon = INITIAL_EPSILON\n",
    "        self.state_dim = 400 #state_dim\n",
    "        self.action_dim = 251       \n",
    "        \n",
    "        self.current_net1 = nn.Linear(self.state_dim,20)\n",
    "        self.current_net2 = nn.Linear(20,self.action_dim)\n",
    "        \n",
    "        self.target_net1 = nn.Linear(self.state_dim,20)\n",
    "        self.target_net2 = nn.Linear(20,self.action_dim)\n",
    "        \n",
    "        '''state_dim 要改!!!'''\n",
    "         #act_dim\n",
    "        #env.action_space.n\n",
    "        \n",
    "        self.saved_actions = []        \n",
    "        self.rewards = []\n",
    "        self.entropy = []\n",
    "        \n",
    "        # Init session\n",
    "        #self.session = tf.InteractiveSession()\n",
    "        #self.session.run(tf.global_variables_initializer())\n",
    "    \n",
    "    def forward(self,inputs):\n",
    "        state,act_mask = inputs\n",
    "        #act_mask = a_m\n",
    "        h_1 = self.current_net1(state)\n",
    "        \n",
    "        h_1 = F.dropout(F.relu(h_1))\n",
    "        self.Q_value = self.current_net2(h_1)\n",
    "        \n",
    "        h_2 = self.target_net1(state)\n",
    "        h_2 = F.dropout(F.relu(h_2))\n",
    "        self.target_Q_value = self.target_net2(h_2)\n",
    "\n",
    "        #optimizer = optim.Adam(model.parameters(), lr=args.lr)\n",
    "        \n",
    "\n",
    "    def perceive(self,state,action,reward,next_state,done,optimizer):\n",
    "        one_hot_action = np.zeros(self.action_dim)\n",
    "        #应该要 32*action_dim\n",
    "        one_hot_action[action] = 1\n",
    "        self.replay_buffer.append((state,one_hot_action,reward,next_state,done))\n",
    "        self.optimizer = optimizer\n",
    "        if len(self.replay_buffer) > REPLAY_SIZE:\n",
    "            self.replay_buffer.popleft()\n",
    "\n",
    "        if len(self.replay_buffer) > BATCH_SIZE:\n",
    "            self.train_Q_network()\n",
    "    \n",
    "    \n",
    "    def train_Q_network(self):\n",
    "        self.time_step += 1\n",
    "        # Step 1: obtain random minibatch from replay memory\n",
    "        minibatch = random.sample(self.replay_buffer,BATCH_SIZE)\n",
    "        state_batch = [data[0] for data in minibatch]\n",
    "        action_batch = [data[1] for data in minibatch]\n",
    "        reward_batch = [data[2] for data in minibatch]\n",
    "        next_state_batch = [data[3] for data in minibatch]\n",
    "        reward_batch = torch.tensor(reward_batch)\n",
    "        # Step 2: calculate y\n",
    "        y_batch = []\n",
    "        current_Q_batch = self.Q_value\n",
    "        #max_action_next = np.argmax(current_Q_batch, axis=1)\n",
    "        max_action_next=torch.argmax(current_Q_batch ,dim=1)\n",
    "        target_Q_batch = self.target_Q_value\n",
    "\n",
    "        for i in range(0,BATCH_SIZE):\n",
    "            done = minibatch[i][4]\n",
    "            if i == 0:\n",
    "                if done:\n",
    "                    y_batch = torch.tensor(reward_batch[i])\n",
    "                else:\n",
    "                    #print('2:',len(target_Q_batch),len(target_Q_batch[0]))\n",
    "                    target_Q_value = target_Q_batch[i, max_action_next[i]] #[i, i, max_action_next[i]]\n",
    "                    #print('type:',type(reward_batch),type(target_Q_value))\n",
    "                    #print(reward_batch)\n",
    "                    #print(target_Q_value)\n",
    "                    y_batch = torch.tensor(torch.tensor(reward_batch[i]) + GAMMA * target_Q_value)\n",
    "            else:\n",
    "                if done:\n",
    "                    #y_batch = np.append(y_batch,reward_batch[i])\n",
    "                    y_batch = torch.tensor(y_batch)\n",
    "                    app = torch.tensor(reward_batch[i])\n",
    "                    y_batch = torch.cat((y_batch,app),0)\n",
    "                else :\n",
    "                    #print('1:',len(target_Q_batch),len(target_Q_batch[0]))\n",
    "                    #print(len(target_Q_batch[0][0]))\n",
    "                    target_Q_value = target_Q_batch[i, max_action_next[i]] #[i, i,max_action_next[i]]\n",
    "                    #print('type:',type(reward_batch),type(target_Q_value))\n",
    "                    y_batch = torch.tensor(y_batch)\n",
    "                    app = torch.tensor(reward_batch[i]+GAMMA*target_Q_value)\n",
    "                    y_batch = torch.cat((y_batch,app),0)\n",
    "                    #y_batch = np.append(y_batch, reward_batch[i] + GAMMA * target_Q_value)\n",
    "                    \n",
    "        y_batch = y_batch.reshape(32,32)\n",
    "        #print(self.entropy)\n",
    "        self.loss = self.entropy.mean()\n",
    "        self.optimizer.zero_grad()\n",
    "        self.loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "    def egreedy_action(self,batch_state,act_mask):#batch_state = state\n",
    "        state = torch.FloatTensor(batch_state)#.to(device)  # Tensor [bs, state_dim]\n",
    "        act_mask = torch.FloatTensor(act_mask)#.to(device)  # Tensor of [bs, act_dim]\n",
    "        self((state,act_mask))\n",
    "        ''''''\n",
    "        act_mask = act_mask.type(torch.uint8)        \n",
    "        self.Q_value[1-act_mask] = -999999.0\n",
    "        self.Q_value = F.softmax(self.Q_value, dim=-1)\n",
    "        \n",
    "        m = Categorical(self.Q_value)#加起来应该不是1？说不准\n",
    "        acts = m.sample()\n",
    "        # [CAVEAT] If sampled action is out of action_space, choose the first action in action_space.\n",
    "        self.entropy = torch.cat((torch.tensor(self.entropy),m.entropy()))\n",
    "        #self.entropy.append(m.entropy())\n",
    "        \n",
    "        if random.random() <= self.epsilon:\n",
    "            acts = []\n",
    "            self.epsilon -= (INITIAL_EPSILON - FINAL_EPSILON) / 10000\n",
    "            for i in range(BATCH_SIZE):\n",
    "                acts.append(random.randint(0,self.action_dim - 1))\n",
    "            acts_log = torch.tensor(acts)\n",
    "            self.saved_actions.append(SavedAction(m.log_prob(acts_log)))#, value))\n",
    "            return acts #要return一行，不能只有一个值吧!!\n",
    "        else:\n",
    "            self.epsilon -= (INITIAL_EPSILON - FINAL_EPSILON) / 10000\n",
    "            acts_log = torch.tensor(acts)\n",
    "            self.saved_actions.append(SavedAction(m.log_prob(acts_log)))#, value))\n",
    "            return acts #np.argmax(Q_value)\n",
    "\n",
    "        \n",
    "    def action(self,batch_state,act_mask):\n",
    "        batch_state = torch.tensor(batch_state,dtype=torch.float32)\n",
    "        act_mask = torch.tensor(act_mask,dtype=torch.float32)\n",
    "        \n",
    "        self((batch_state,act_mask))\n",
    "        #self.forward(batch_state,act_mask)\n",
    "        ''''''\n",
    "        act_mask = act_mask.type(torch.uint8)\n",
    "        #self((batch_state, act_mask))        \n",
    "        self.Q_value[1-act_mask] = -999999.0\n",
    "        self.Q_value = F.softmax(self.Q_value, dim=-1)\n",
    "\n",
    "        m = Categorical(self.Q_value)#加起来应该不是1？说不准 \n",
    "        acts = m.sample()\n",
    "        # [CAVEAT] If sampled action is out of action_space, choose the first action in action_space.\n",
    "        self.entropy = torch.cat((torch.tensor(self.entropy),m.entropy()))\n",
    "        #self.entropy.append(m.entropy())\n",
    "        acts_log = torch.tensor(acts)\n",
    "        self.saved_actions.append(SavedAction(m.log_prob(acts_log)))#, value))\n",
    "        #!!!!\n",
    "        return acts #np.argmax(Q_value)\n",
    "\n",
    "    def update_target_q_network(self, episode):\n",
    "        # update target Q netowrk\n",
    "        if episode % REPLACE_TARGET_FREQ == 0:\n",
    "            self.session.run(self.target_replace_op)\n",
    "            self.saver.save(self.session, 'ckp')  # 2. 保存模型和变量\n",
    "            #print('episode '+str(episode) +', target Q network params replaced!')\n",
    "    \n",
    "    '''\n",
    "    def update(self,optimizer):\n",
    "        batch_rewards = np.vstack(self.rewards).T  # numpy array of [bs, #steps]\n",
    "        #batch_rewards = torch.FloatTensor(batch_rewards).to(device)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    '''   \n",
    "    '''\n",
    "    def update(self, optimizer, device, ent_weight):\n",
    "        if len(self.rewards) <= 0:\n",
    "            del self.rewards[:]\n",
    "            del self.saved_actions[:]\n",
    "            del self.entropy[:]\n",
    "            return 0.0, 0.0, 0.0\n",
    "\n",
    "        batch_rewards = np.vstack(self.rewards).T  # numpy array of [bs, #steps]\n",
    "        batch_rewards = torch.FloatTensor(batch_rewards).to(device)\n",
    "        \n",
    "        num_steps = batch_rewards.shape[1]\n",
    "        for i in range(1, num_steps):\n",
    "            batch_rewards[:, num_steps - i - 1] += self.gamma * batch_rewards[:, num_steps - i]\n",
    "\n",
    "        actor_loss = 0\n",
    "        critic_loss = 0\n",
    "        entropy_loss = 0\n",
    "        \n",
    "        for i in range(0, num_steps):\n",
    "            log_prob, value = self.saved_actions[i]  # log_prob: Tensor of [bs, ], value: Tensor of [bs, 1]\n",
    "            advantage = batch_rewards[:, i] - value.squeeze(1)  # Tensor of [bs, ]\n",
    "            actor_loss += -log_prob * advantage.detach()  # Tensor of [bs, ]\n",
    "            critic_loss += advantage.pow(2)  # Tensor of [bs, ]\n",
    "            entropy_loss += -self.entropy[i]  # Tensor of [bs, ]\n",
    "        actor_loss = actor_loss.mean()\n",
    "        critic_loss = critic_loss.mean()\n",
    "        entropy_loss = entropy_loss.mean()\n",
    "        loss = actor_loss + critic_loss + ent_weight * entropy_loss\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        del self.rewards[:]\n",
    "        del self.saved_actions[:]\n",
    "        del self.entropy[:]\n",
    "        \n",
    "        #loss, ploss, vloss, eloss\n",
    "        return loss.item(), actor_loss.item(), critic_loss.item(), entropy_loss.item()\n",
    "    \n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ACDataLoader(object):\n",
    "    def __init__(self, uids, batch_size):\n",
    "        self.uids = np.array(uids)\n",
    "        self.num_users = len(uids)\n",
    "        self.batch_size = batch_size\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self._rand_perm = np.random.permutation(self.num_users)\n",
    "        self._start_idx = 0\n",
    "        self._has_next = True\n",
    "\n",
    "    def has_next(self):\n",
    "        return self._has_next\n",
    "\n",
    "\n",
    "    #直接把训练集和测试机给改了就行了 序号都是 32的倍数就可\n",
    "    def get_batch(self):\n",
    "        if not self._has_next:\n",
    "            return None\n",
    "        # Multiple users per batch\n",
    "        end_idx = min(self._start_idx + self.batch_size, self.num_users)\n",
    "        #print('get_batch,end_idx:',end_idx,';',self.num_users)\n",
    "        batch_idx = self._rand_perm[self._start_idx:end_idx]\n",
    "        batch_uids = self.uids[batch_idx]\n",
    "        self._has_next = self._has_next and end_idx < self.num_users\n",
    "        self._start_idx = end_idx\n",
    "        \n",
    "        return batch_uids.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPISODE = 3000 # Episode limitation\n",
    "STEP = 300 # Step limitation in an episode\n",
    "TEST = 5 # The number of experiment test every 100 episode\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--dataset', type=str, default=BEAUTY, help='One of {clothing, cell, beauty, cd}')\n",
    "parser.add_argument('--name', type=str, default='train_agent', help='directory name.')\n",
    "parser.add_argument('--seed', type=int, default=123, help='random seed.')\n",
    "parser.add_argument('--gpu', type=str, default='0', help='gpu device.')\n",
    "parser.add_argument('--epochs', type=int, default=100, help='Max number of epochs.')\n",
    "parser.add_argument('--batch_size', type=int, default=32, help='batch size.')\n",
    "parser.add_argument('--lr', type=float, default=1e-4, help='learning rate.')\n",
    "parser.add_argument('--max_acts', type=int, default=250, help='Max number of actions.')\n",
    "#parser.add_argument('--state_dim', type=int, default=250, help='Max number of actions.')\n",
    "#\n",
    "parser.add_argument('--max_path_len', type=int, default=3, help='Max path length.')\n",
    "parser.add_argument('--gamma', type=float, default=0.99, help='reward discount factor.')\n",
    "parser.add_argument('--ent_weight', type=float, default=1e-2, help='weight factor for entropy loss')\n",
    "parser.add_argument('--act_dropout', type=float, default=0.5, help='action dropout rate.')\n",
    "parser.add_argument('--state_history', type=int, default=1, help='state history length')\n",
    "parser.add_argument('--hidden', type=int, nargs='*', default=[512, 256], help='number of samples')\n",
    "args = parser.parse_args(['--dataset',CELL])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]  Namespace(act_dropout=0.5, batch_size=32, dataset='cell', device='cpu', ent_weight=0.01, epochs=100, gamma=0.99, gpu='0', hidden=[512, 256], log_dir='./tmp/Amazon_Cellphones/train_agent', lr=0.0001, max_acts=250, max_path_len=3, name='train_agent', seed=123, state_history=1)\n"
     ]
    }
   ],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = args.gpu\n",
    "args.device = torch.device('cuda:0') if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "args.log_dir = '{}/{}'.format(TMP_DIR[args.dataset], args.name)\n",
    "if not os.path.isdir(args.log_dir):\n",
    "    os.makedirs(args.log_dir)\n",
    "\n",
    "global logger\n",
    "logger = get_logger(args.log_dir + '/train_log_DQN.txt')\n",
    "logger.info(args)\n",
    "\n",
    "set_random_seed(args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args):\n",
    "    \n",
    "    # initialize OpenAI Gym env and dqn agent\n",
    "    #env = gym.make(ENV_NAME)\n",
    "    env = BatchKGEnvironment(args.dataset, args.max_acts, max_path_len=args.max_path_len, state_history=args.state_history)\n",
    "    uids = list(env.kg(USER).keys())\n",
    "    print('uids:',len(uids))\n",
    "    uids = np.arange(19488).tolist()\n",
    "    agent = DQN(env,env.state_dim,env.act_dim,gamma = args.gamma,hidden_sizes = args.hidden)\n",
    "    dataloader = ACDataLoader(uids, args.batch_size)\n",
    "    logger.info('Parameters:' + str([i[0] for i in agent.named_parameters()]))\n",
    "    optimizer = optim.Adam(agent.parameters(), lr=args.lr)\n",
    "    #model = ActorCritic(env.state_dim, env.act_dim, gamma=args.gamma, hidden_sizes=args.hidden).to(args.device)\n",
    "    #logger.info('Parameters:' + str([i[0] for i in model.named_parameters()]))\n",
    "\n",
    "    '''    \n",
    "    uids = list(env.kg(USER).keys())\n",
    "    dataloader = ACDataLoader(uids, args.batch_size)\n",
    "    model = ActorCritic(env.state_dim, env.act_dim, gamma=args.gamma, hidden_sizes=args.hidden).to(args.device)\n",
    "    logger.info('Parameters:' + str([i[0] for i in model.named_parameters()]))\n",
    "    '''\n",
    "    episode = 1\n",
    "    for epoch in range(0, args.epochs):\n",
    "        ### Start epoch ###\n",
    "        if epoch % 10 == 0:\n",
    "            print('epoch:',epoch)\n",
    "        dataloader.reset()\n",
    "        while dataloader.has_next():\n",
    "            batch_uids = dataloader.get_batch()\n",
    "            ### Start batch episodes ###\n",
    "            #print('batch_uids:',batch_uids,';',len(batch_uids))\n",
    "            batch_state1 = env.reset(batch_uids)  # numpy array of [bs, state_dim]\n",
    "            #print('egreedy_action, batch_state1:',batch_state1,';',len(batch_state1),';',len(batch_state1[0]))\n",
    "            for step in range(STEP):#while not done:\n",
    "\n",
    "                batch_act_mask = env.batch_action_mask(dropout=args.act_dropout)  # numpy array of size [bs, act_dim]\n",
    "                '''select action'''\n",
    "                #print('shape of batch state1:',batch_state1.shape)\n",
    "                batch_act_idx = agent.egreedy_action(batch_state1,batch_act_mask)#batch_act_mask\n",
    "                batch_state2, batch_reward, done = env.batch_step(batch_act_idx)\n",
    "                batch_state2 = batch_state2.reshape((-1,400))\n",
    "                agent.perceive(batch_state1,batch_act_idx,batch_reward,batch_state2,done,optimizer)\n",
    "                agent.rewards.append(batch_reward)\n",
    "                #print('batch_state2:',batch_state2,';',len(batch_state2),';',len(batch_state2[0]))\n",
    "                batch_state1 = batch_state2\n",
    "                #train Q network\n",
    "                if done:\n",
    "                    break\n",
    "                # Test every 100 episodes\n",
    "            if epoch % 100 == 0:\n",
    "                total_reward = 0\n",
    "                for i in range(TEST):\n",
    "                    #batch_uids = dataloader.get_batch()\n",
    "                    #print('action batch_uids:',batch_uids)\n",
    "                    ### Start batch episodes ###\n",
    "                    batch_state1 = env.reset(batch_uids)  # numpy array of [bs, state_dim]\n",
    "                    #state = env.reset()\n",
    "                    for j in range(STEP):\n",
    "                        #env.render()#在屏幕上显示画面，不需要\n",
    "                        batch_act_mask = env.batch_action_mask(dropout=args.act_dropout)\n",
    "                        #print('before action, batch_state1:',batch_state1,';',len(batch_state1),';',len(batch_state1[0]))\n",
    "                        action = agent.action(batch_state1,batch_act_mask) # direct action for test\n",
    "                        batch_state2, batch_reward, done = env.batch_step(action)\n",
    "                        batch_state1 = batch_state2\n",
    "                        total_reward += batch_reward\n",
    "                        if done:\n",
    "                            break\n",
    "                ave_reward = total_reward/TEST\n",
    "                if episode % 100 == 0:\n",
    "                    print ('episode: ',episode,'Evaluation Average Reward:',sum(ave_reward)/len(ave_reward))\n",
    "                episode = episode + 1 \n",
    "        #agent.update_target_q_network(epoch)\n",
    "        \n",
    "        #########\n",
    "        if epoch % 100 == 0:\n",
    "            policy_file = '{}/dqn-2_model_epoch_{}.ckpt'.format(args.log_dir, epoch)\n",
    "            logger.info(\"Save model to \" + policy_file)\n",
    "            print('epoch:',epoch,',episode:',episode)\n",
    "            torch.save(agent.state_dict(), policy_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load embedding: ./tmp/Amazon_Cellphones/transe_embed.pkl\n",
      "uids: 27879\n",
      "[INFO]  Parameters:['current_net1.weight', 'current_net1.bias', 'current_net2.weight', 'current_net2.bias', 'target_net1.weight', 'target_net1.bias', 'target_net2.weight', 'target_net2.bias']\n",
      "epoch: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:126: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:139: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:159: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:161: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:90: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:94: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:95: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:102: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:83: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode:  100 Evaluation Average Reward: 0.042938327498268335\n",
      "episode:  200 Evaluation Average Reward: 0.023402003130468075\n",
      "episode:  300 Evaluation Average Reward: 0.04071598120863201\n",
      "episode:  400 Evaluation Average Reward: 0.01928118197247386\n",
      "episode:  500 Evaluation Average Reward: 0.018164239246834774\n",
      "episode:  600 Evaluation Average Reward: 0.02113535286625847\n",
      "[INFO]  Save model to ./tmp/Amazon_Cellphones/train_agent/dqn-2_model_epoch_0.ckpt\n",
      "epoch: 0 ,episode: 610\n"
     ]
    }
   ],
   "source": [
    "train(args)#-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load embedding: ./tmp/Amazon_Cellphones/transe_embed.pkl\n",
      "uids: 27879\n",
      "[INFO]  Parameters:['current_net1.weight', 'current_net1.bias', 'current_net2.weight', 'current_net2.bias', 'target_net1.weight', 'target_net1.bias', 'target_net2.weight', 'target_net2.bias']\n",
      "epoch: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:126: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:139: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:159: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:161: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:90: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:94: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:95: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:102: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:83: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode:  100 Evaluation Average Reward: 0.042938327498268335\n",
      "episode:  200 Evaluation Average Reward: 0.023402003130468075\n",
      "episode:  300 Evaluation Average Reward: 0.04071598120863201\n",
      "episode:  400 Evaluation Average Reward: 0.01928118197247386\n",
      "episode:  500 Evaluation Average Reward: 0.018164239246834774\n",
      "episode:  600 Evaluation Average Reward: 0.02113535286625847\n",
      "[INFO]  Save model to ./tmp/Amazon_Cellphones/train_agent/dqn_model_epoch_0.ckpt\n",
      "epoch: 0 ,episode: 610\n",
      "[INFO]  Save model to ./tmp/Amazon_Cellphones/train_agent/dqn_model_epoch_1.ckpt\n",
      "epoch: 1 ,episode: 610\n",
      "[INFO]  Save model to ./tmp/Amazon_Cellphones/train_agent/dqn_model_epoch_2.ckpt\n",
      "epoch: 2 ,episode: 610\n",
      "[INFO]  Save model to ./tmp/Amazon_Cellphones/train_agent/dqn_model_epoch_3.ckpt\n",
      "epoch: 3 ,episode: 610\n",
      "[INFO]  Save model to ./tmp/Amazon_Cellphones/train_agent/dqn_model_epoch_4.ckpt\n",
      "epoch: 4 ,episode: 610\n",
      "[INFO]  Save model to ./tmp/Amazon_Cellphones/train_agent/dqn_model_epoch_5.ckpt\n",
      "epoch: 5 ,episode: 610\n",
      "[INFO]  Save model to ./tmp/Amazon_Cellphones/train_agent/dqn_model_epoch_6.ckpt\n",
      "epoch: 6 ,episode: 610\n",
      "[INFO]  Save model to ./tmp/Amazon_Cellphones/train_agent/dqn_model_epoch_7.ckpt\n",
      "epoch: 7 ,episode: 610\n",
      "[INFO]  Save model to ./tmp/Amazon_Cellphones/train_agent/dqn_model_epoch_8.ckpt\n",
      "epoch: 8 ,episode: 610\n",
      "[INFO]  Save model to ./tmp/Amazon_Cellphones/train_agent/dqn_model_epoch_9.ckpt\n",
      "epoch: 9 ,episode: 610\n",
      "epoch: 10\n",
      "[INFO]  Save model to ./tmp/Amazon_Cellphones/train_agent/dqn_model_epoch_10.ckpt\n",
      "epoch: 10 ,episode: 610\n",
      "[INFO]  Save model to ./tmp/Amazon_Cellphones/train_agent/dqn_model_epoch_11.ckpt\n",
      "epoch: 11 ,episode: 610\n",
      "[INFO]  Save model to ./tmp/Amazon_Cellphones/train_agent/dqn_model_epoch_12.ckpt\n",
      "epoch: 12 ,episode: 610\n",
      "[INFO]  Save model to ./tmp/Amazon_Cellphones/train_agent/dqn_model_epoch_13.ckpt\n",
      "epoch: 13 ,episode: 610\n",
      "[INFO]  Save model to ./tmp/Amazon_Cellphones/train_agent/dqn_model_epoch_14.ckpt\n",
      "epoch: 14 ,episode: 610\n",
      "[INFO]  Save model to ./tmp/Amazon_Cellphones/train_agent/dqn_model_epoch_15.ckpt\n",
      "epoch: 15 ,episode: 610\n",
      "[INFO]  Save model to ./tmp/Amazon_Cellphones/train_agent/dqn_model_epoch_16.ckpt\n",
      "epoch: 16 ,episode: 610\n",
      "[INFO]  Save model to ./tmp/Amazon_Cellphones/train_agent/dqn_model_epoch_17.ckpt\n",
      "epoch: 17 ,episode: 610\n",
      "[INFO]  Save model to ./tmp/Amazon_Cellphones/train_agent/dqn_model_epoch_18.ckpt\n",
      "epoch: 18 ,episode: 610\n",
      "[INFO]  Save model to ./tmp/Amazon_Cellphones/train_agent/dqn_model_epoch_19.ckpt\n",
      "epoch: 19 ,episode: 610\n",
      "epoch: 20\n",
      "[INFO]  Save model to ./tmp/Amazon_Cellphones/train_agent/dqn_model_epoch_20.ckpt\n",
      "epoch: 20 ,episode: 610\n",
      "[INFO]  Save model to ./tmp/Amazon_Cellphones/train_agent/dqn_model_epoch_21.ckpt\n",
      "epoch: 21 ,episode: 610\n",
      "[INFO]  Save model to ./tmp/Amazon_Cellphones/train_agent/dqn_model_epoch_22.ckpt\n",
      "epoch: 22 ,episode: 610\n",
      "[INFO]  Save model to ./tmp/Amazon_Cellphones/train_agent/dqn_model_epoch_23.ckpt\n",
      "epoch: 23 ,episode: 610\n",
      "[INFO]  Save model to ./tmp/Amazon_Cellphones/train_agent/dqn_model_epoch_24.ckpt\n",
      "epoch: 24 ,episode: 610\n",
      "[INFO]  Save model to ./tmp/Amazon_Cellphones/train_agent/dqn_model_epoch_25.ckpt\n",
      "epoch: 25 ,episode: 610\n",
      "[INFO]  Save model to ./tmp/Amazon_Cellphones/train_agent/dqn_model_epoch_26.ckpt\n",
      "epoch: 26 ,episode: 610\n",
      "[INFO]  Save model to ./tmp/Amazon_Cellphones/train_agent/dqn_model_epoch_27.ckpt\n",
      "epoch: 27 ,episode: 610\n",
      "[INFO]  Save model to ./tmp/Amazon_Cellphones/train_agent/dqn_model_epoch_28.ckpt\n",
      "epoch: 28 ,episode: 610\n",
      "[INFO]  Save model to ./tmp/Amazon_Cellphones/train_agent/dqn_model_epoch_29.ckpt\n",
      "epoch: 29 ,episode: 610\n",
      "epoch: 30\n",
      "[INFO]  Save model to ./tmp/Amazon_Cellphones/train_agent/dqn_model_epoch_30.ckpt\n",
      "epoch: 30 ,episode: 610\n",
      "[INFO]  Save model to ./tmp/Amazon_Cellphones/train_agent/dqn_model_epoch_31.ckpt\n",
      "epoch: 31 ,episode: 610\n",
      "[INFO]  Save model to ./tmp/Amazon_Cellphones/train_agent/dqn_model_epoch_32.ckpt\n",
      "epoch: 32 ,episode: 610\n",
      "[INFO]  Save model to ./tmp/Amazon_Cellphones/train_agent/dqn_model_epoch_33.ckpt\n",
      "epoch: 33 ,episode: 610\n",
      "[INFO]  Save model to ./tmp/Amazon_Cellphones/train_agent/dqn_model_epoch_34.ckpt\n",
      "epoch: 34 ,episode: 610\n",
      "[INFO]  Save model to ./tmp/Amazon_Cellphones/train_agent/dqn_model_epoch_35.ckpt\n",
      "epoch: 35 ,episode: 610\n",
      "[INFO]  Save model to ./tmp/Amazon_Cellphones/train_agent/dqn_model_epoch_36.ckpt\n",
      "epoch: 36 ,episode: 610\n",
      "[INFO]  Save model to ./tmp/Amazon_Cellphones/train_agent/dqn_model_epoch_37.ckpt\n",
      "epoch: 37 ,episode: 610\n",
      "[INFO]  Save model to ./tmp/Amazon_Cellphones/train_agent/dqn_model_epoch_38.ckpt\n",
      "epoch: 38 ,episode: 610\n",
      "[INFO]  Save model to ./tmp/Amazon_Cellphones/train_agent/dqn_model_epoch_39.ckpt\n",
      "epoch: 39 ,episode: 610\n",
      "epoch: 40\n",
      "[INFO]  Save model to ./tmp/Amazon_Cellphones/train_agent/dqn_model_epoch_40.ckpt\n",
      "epoch: 40 ,episode: 610\n",
      "[INFO]  Save model to ./tmp/Amazon_Cellphones/train_agent/dqn_model_epoch_41.ckpt\n",
      "epoch: 41 ,episode: 610\n",
      "[INFO]  Save model to ./tmp/Amazon_Cellphones/train_agent/dqn_model_epoch_42.ckpt\n",
      "epoch: 42 ,episode: 610\n",
      "[INFO]  Save model to ./tmp/Amazon_Cellphones/train_agent/dqn_model_epoch_43.ckpt\n",
      "epoch: 43 ,episode: 610\n",
      "[INFO]  Save model to ./tmp/Amazon_Cellphones/train_agent/dqn_model_epoch_44.ckpt\n",
      "epoch: 44 ,episode: 610\n",
      "[INFO]  Save model to ./tmp/Amazon_Cellphones/train_agent/dqn_model_epoch_45.ckpt\n",
      "epoch: 45 ,episode: 610\n",
      "[INFO]  Save model to ./tmp/Amazon_Cellphones/train_agent/dqn_model_epoch_46.ckpt\n",
      "epoch: 46 ,episode: 610\n",
      "[INFO]  Save model to ./tmp/Amazon_Cellphones/train_agent/dqn_model_epoch_47.ckpt\n",
      "epoch: 47 ,episode: 610\n",
      "[INFO]  Save model to ./tmp/Amazon_Cellphones/train_agent/dqn_model_epoch_48.ckpt\n",
      "epoch: 48 ,episode: 610\n",
      "[INFO]  Save model to ./tmp/Amazon_Cellphones/train_agent/dqn_model_epoch_49.ckpt\n",
      "epoch: 49 ,episode: 610\n",
      "epoch: 50\n",
      "[INFO]  Save model to ./tmp/Amazon_Cellphones/train_agent/dqn_model_epoch_50.ckpt\n",
      "epoch: 50 ,episode: 610\n",
      "[INFO]  Save model to ./tmp/Amazon_Cellphones/train_agent/dqn_model_epoch_51.ckpt\n",
      "epoch: 51 ,episode: 610\n",
      "[INFO]  Save model to ./tmp/Amazon_Cellphones/train_agent/dqn_model_epoch_52.ckpt\n",
      "epoch: 52 ,episode: 610\n",
      "[INFO]  Save model to ./tmp/Amazon_Cellphones/train_agent/dqn_model_epoch_53.ckpt\n",
      "epoch: 53 ,episode: 610\n",
      "[INFO]  Save model to ./tmp/Amazon_Cellphones/train_agent/dqn_model_epoch_54.ckpt\n",
      "epoch: 54 ,episode: 610\n",
      "[INFO]  Save model to ./tmp/Amazon_Cellphones/train_agent/dqn_model_epoch_55.ckpt\n",
      "epoch: 55 ,episode: 610\n",
      "[INFO]  Save model to ./tmp/Amazon_Cellphones/train_agent/dqn_model_epoch_56.ckpt\n",
      "epoch: 56 ,episode: 610\n",
      "[INFO]  Save model to ./tmp/Amazon_Cellphones/train_agent/dqn_model_epoch_57.ckpt\n",
      "epoch: 57 ,episode: 610\n",
      "[INFO]  Save model to ./tmp/Amazon_Cellphones/train_agent/dqn_model_epoch_58.ckpt\n",
      "epoch: 58 ,episode: 610\n",
      "[INFO]  Save model to ./tmp/Amazon_Cellphones/train_agent/dqn_model_epoch_59.ckpt\n",
      "epoch: 59 ,episode: 610\n",
      "epoch: 60\n",
      "[INFO]  Save model to ./tmp/Amazon_Cellphones/train_agent/dqn_model_epoch_60.ckpt\n",
      "epoch: 60 ,episode: 610\n",
      "[INFO]  Save model to ./tmp/Amazon_Cellphones/train_agent/dqn_model_epoch_61.ckpt\n",
      "epoch: 61 ,episode: 610\n",
      "[INFO]  Save model to ./tmp/Amazon_Cellphones/train_agent/dqn_model_epoch_62.ckpt\n",
      "epoch: 62 ,episode: 610\n",
      "[INFO]  Save model to ./tmp/Amazon_Cellphones/train_agent/dqn_model_epoch_63.ckpt\n",
      "epoch: 63 ,episode: 610\n",
      "[INFO]  Save model to ./tmp/Amazon_Cellphones/train_agent/dqn_model_epoch_64.ckpt\n",
      "epoch: 64 ,episode: 610\n",
      "[INFO]  Save model to ./tmp/Amazon_Cellphones/train_agent/dqn_model_epoch_65.ckpt\n",
      "epoch: 65 ,episode: 610\n",
      "[INFO]  Save model to ./tmp/Amazon_Cellphones/train_agent/dqn_model_epoch_66.ckpt\n",
      "epoch: 66 ,episode: 610\n",
      "[INFO]  Save model to ./tmp/Amazon_Cellphones/train_agent/dqn_model_epoch_67.ckpt\n",
      "epoch: 67 ,episode: 610\n",
      "[INFO]  Save model to ./tmp/Amazon_Cellphones/train_agent/dqn_model_epoch_68.ckpt\n",
      "epoch: 68 ,episode: 610\n",
      "[INFO]  Save model to ./tmp/Amazon_Cellphones/train_agent/dqn_model_epoch_69.ckpt\n",
      "epoch: 69 ,episode: 610\n",
      "epoch: 70\n",
      "[INFO]  Save model to ./tmp/Amazon_Cellphones/train_agent/dqn_model_epoch_70.ckpt\n",
      "epoch: 70 ,episode: 610\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-aad596905035>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-6-f1b2dc791bda>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(args)\u001b[0m\n\u001b[0;32m     38\u001b[0m                 \u001b[1;31m#print('shape of batch state1:',batch_state1.shape)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m                 \u001b[0mbatch_act_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0megreedy_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_state1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_act_mask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;31m#batch_act_mask\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m                 \u001b[0mbatch_state2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_reward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_act_idx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m                 \u001b[0mbatch_state2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch_state2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m400\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m                 \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mperceive\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_state1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_act_idx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_reward\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_state2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\pgpr\\kg_env.py\u001b[0m in \u001b[0;36mbatch_step\u001b[1;34m(self, batch_act_idx)\u001b[0m\n\u001b[0;32m    247\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_done\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_done\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# must run before get actions, etc.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    248\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_batch_curr_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_batch_get_state\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_batch_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 249\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_batch_curr_actions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_batch_get_actions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_batch_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_done\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    250\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_batch_curr_reward\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_batch_get_reward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_batch_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    251\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\pgpr\\kg_env.py\u001b[0m in \u001b[0;36m_batch_get_actions\u001b[1;34m(self, batch_path, done)\u001b[0m\n\u001b[0;32m    136\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    137\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_batch_get_actions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 138\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_actions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mpath\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mbatch_path\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    139\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_get_state\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\pgpr\\kg_env.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    136\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    137\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_batch_get_actions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 138\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_actions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mpath\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mbatch_path\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    139\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_get_state\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\pgpr\\kg_env.py\u001b[0m in \u001b[0;36m_get_actions\u001b[1;34m(self, path, done)\u001b[0m\n\u001b[0;32m    122\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# BRAND, CATEGORY, RELATED_PRODUCT\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    123\u001b[0m                 \u001b[0msrc_embed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0muser_embed\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membeds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mPURCHASE\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membeds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 124\u001b[1;33m             \u001b[0mscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc_embed\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membeds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnext_node_type\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnext_node_id\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    125\u001b[0m             \u001b[1;31m# This trimming may filter out target products!\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    126\u001b[0m             \u001b[1;31m# Manually set the score of target products a very large number.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3],\n",
       "        [4, 5, 6]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([[1,2,3]])\n",
    "a = torch.cat((a,torch.tensor([[4,5,6]])),0)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected torch.FloatTensor (got torch.LongTensor)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-934b9bb960b7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: expected torch.FloatTensor (got torch.LongTensor)"
     ]
    }
   ],
   "source": [
    "a = [torch.tensor([4,6]),torch.tensor([5,2])]\n",
    "a = torch.cat(a)\n",
    "a = torch.FloatTensor(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4, 6, 5, 2])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([4, 6, 5, 2], dtype=int64)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = []\n",
    "b.append(a)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "b.append(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([4, 6, 5, 2], dtype=int64), array([4, 6, 5, 2], dtype=int64)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'mean'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-781b9fa94bde>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'mean'"
     ]
    }
   ],
   "source": [
    "torch.()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
