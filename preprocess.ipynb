{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import gzip\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "from data_utils import AmazonDataset\n",
    "from knowledge_graph import KnowledgeGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_labels(dataset, mode='train'):\n",
    "    review_file = '{}/{}.txt.gz'.format(DATASET_DIR[dataset], mode)\n",
    "    print('review_file:',review_file)\n",
    "    user_products = {}  # {uid: [pid,...], ...}\n",
    "    with gzip.open(review_file, 'r') as f:\n",
    "        for line in f:\n",
    "            line = line.decode('utf-8').strip()\n",
    "            arr = line.split('\\t')\n",
    "            user_idx = int(arr[0])\n",
    "            product_idx = int(arr[1])\n",
    "            if user_idx not in user_products:\n",
    "                user_products[user_idx] = []\n",
    "            user_products[user_idx].append(product_idx)\n",
    "    print('user_products:',user_products[0])\n",
    "    save_labels(dataset, user_products, mode=mode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLOTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load user of size 39387\n",
      "Load product of size 23033\n",
      "Load word of size 21366\n",
      "Load related_product of size 339367\n",
      "Load brand of size 1182\n",
      "Load category of size 1193\n",
      "Load produced_by of size 23033\n",
      "Load belongs_to of size 23033\n",
      "Load also_bought of size 23033\n",
      "Load also_viewed of size 23033\n",
      "Load bought_together of size 23033\n",
      "word_indices: [5]\n",
      "Load review of size 194439 word count= 194439\n",
      "Create word sampling rate\n",
      "--------\n",
      "cloth\n",
      "--------\n",
      "<data_utils.AmazonDataset object at 0x000001A7702090B8>\n"
     ]
    }
   ],
   "source": [
    "if not os.path.isdir(TMP_DIR[CLOTH]):\n",
    "    os.makedirs(TMP_DIR[CLOTH])\n",
    "dataset = AmazonDataset(DATASET_DIR[CLOTH])\n",
    "save_dataset(CLOTH, dataset)\n",
    "print('--------')\n",
    "print(CLOTH)\n",
    "print('--------')\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create cloth knowledge graph from dataset...\n",
      "dataset_file: ./tmp/Amazon_Clothing/dataset.pkl\n",
      "Load entities...\n",
      "Total 425528 nodes.\n",
      "Load reviews...\n",
      "Total 0 review edges.\n",
      "uid pid reamained_words. 19768 ; 6191 ; []\n",
      "type of remained words: <class 'list'>\n",
      "Load knowledge produced_by...\n",
      "Total 7928 produced_by edges.\n",
      "Load knowledge belongs_to...\n",
      "Total 309666 belongs_to edges.\n",
      "Load knowledge also_bought...\n",
      "Total 2826284 also_bought edges.\n",
      "Load knowledge also_viewed...\n",
      "Total 289588 also_viewed edges.\n",
      "Load knowledge bought_together...\n",
      "Total 31674 bought_together edges.\n",
      "Remove duplicates...\n",
      "Compute node degrees...\n"
     ]
    }
   ],
   "source": [
    "# Generate knowledge graph instance.\n",
    "# ========== BEGIN ========== #\n",
    "print('Create', CLOTH, 'knowledge graph from dataset...')\n",
    "dataset_cloth = load_dataset(CLOTH)\n",
    "#print(dataset)\n",
    "kg_cloth = KnowledgeGraph(dataset_cloth)\n",
    "#print(kg)\n",
    "kg_cloth.compute_degrees()\n",
    "save_kg(CLOTH, kg_cloth)\n",
    "# =========== END =========== #\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate cloth train/test labels.\n",
      "review_file: ./data/Amazon_Clothing/train.txt.gz\n",
      "user_products: [22034, 17059, 8668, 469, 21869]\n",
      "review_file: ./data/Amazon_Clothing/test.txt.gz\n",
      "user_products: [3375]\n"
     ]
    }
   ],
   "source": [
    "# Genereate train/test labels.\n",
    "# ========== BEGIN ========== #\n",
    "print('Generate', CLOTH, 'train/test labels.')\n",
    "generate_labels(CLOTH, 'train')#表示第一个用户买的商品的编号\n",
    "generate_labels(CLOTH, 'test')\n",
    "# =========== END =========== #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load user of size 75258\n",
      "Load product of size 64443\n",
      "Load word of size 202959\n",
      "Load related_product of size 236255\n",
      "Load brand of size 1414\n",
      "Load category of size 770\n",
      "Load produced_by of size 64443\n",
      "Load belongs_to of size 64443\n",
      "Load also_bought of size 64443\n",
      "Load also_viewed of size 64443\n",
      "Load bought_together of size 64443\n",
      "word_indices: [5]\n",
      "Load review of size 194439 word count= 194439\n",
      "Create word sampling rate\n"
     ]
    }
   ],
   "source": [
    "if not os.path.isdir(TMP_DIR[CD]):\n",
    "    os.makedirs(TMP_DIR[CD])\n",
    "dataset = AmazonDataset(DATASET_DIR[CD])\n",
    "save_dataset(CD, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create cd knowledge graph from dataset...\n",
      "dataset_file: ./tmp/Amazon_CDs/dataset.pkl\n",
      "--\n",
      "Load entities...\n",
      "Total 581099 nodes.\n",
      "Load reviews...\n",
      "Total 0 review edges.\n",
      "uid pid reamained_words. 19768 ; 6191 ; []\n",
      "type of remained words: <class 'list'>\n",
      "Load knowledge produced_by...\n",
      "Total 26762 produced_by edges.\n",
      "Load knowledge belongs_to...\n",
      "Total 933902 belongs_to edges.\n",
      "Load knowledge also_bought...\n",
      "Total 7383192 also_bought edges.\n",
      "Load knowledge also_viewed...\n",
      "Total 34848 also_viewed edges.\n",
      "Load knowledge bought_together...\n",
      "Total 88152 bought_together edges.\n",
      "Remove duplicates...\n",
      "--\n",
      "Compute node degrees...\n",
      "--\n"
     ]
    }
   ],
   "source": [
    "# Generate knowledge graph instance.\n",
    "# ========== BEGIN ========== #\n",
    "print('Create', CD, 'knowledge graph from dataset...')\n",
    "dataset_cd = load_dataset(CD)\n",
    "print('--')\n",
    "kg_cd = KnowledgeGraph(dataset_cd)\n",
    "print('--')\n",
    "kg_cd.compute_degrees()\n",
    "print('--')\n",
    "save_kg(CD, kg_cd)\n",
    "# =========== END =========== #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate cd train/test labels.\n",
      "review_file: ./data/Amazon_CDs/train.txt.gz\n",
      "user_products: [26837, 40754, 32754, 7929, 1296]\n",
      "review_file: ./data/Amazon_CDs/test.txt.gz\n",
      "user_products: [13551]\n"
     ]
    }
   ],
   "source": [
    "# Genereate train/test labels.\n",
    "# ========== BEGIN ========== #\n",
    "print('Generate', CD, 'train/test labels.')\n",
    "generate_labels(CD, 'train')#表示第一个用户买的商品的编号\n",
    "generate_labels(CD, 'test')\n",
    "# =========== END =========== #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BEAUTY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load user of size 22363\n",
      "Load product of size 12101\n",
      "Load word of size 22564\n",
      "Load related_product of size 164721\n",
      "Load brand of size 2077\n",
      "Load category of size 248\n",
      "Load produced_by of size 12101\n",
      "Load belongs_to of size 12101\n",
      "Load also_bought of size 12101\n",
      "Load also_viewed of size 12101\n",
      "Load bought_together of size 12101\n",
      "word_indices: [5]\n",
      "Load review of size 194439 word count= 194439\n",
      "Create word sampling rate\n"
     ]
    }
   ],
   "source": [
    "if not os.path.isdir(TMP_DIR[BEAUTY]):\n",
    "    os.makedirs(TMP_DIR[BEAUTY])\n",
    "dataset = AmazonDataset(DATASET_DIR[BEAUTY])\n",
    "save_dataset(BEAUTY, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create beauty knowledge graph from dataset...\n",
      "dataset_file: ./tmp/Amazon_Beauty/dataset.pkl\n",
      "--\n",
      "Load entities...\n",
      "Total 224074 nodes.\n",
      "Load reviews...\n",
      "Total 0 review edges.\n",
      "uid pid reamained_words. 19768 ; 6191 ; []\n",
      "type of remained words: <class 'list'>\n",
      "Load knowledge produced_by...\n",
      "Total 20042 produced_by edges.\n",
      "Load knowledge belongs_to...\n",
      "Total 99512 belongs_to edges.\n",
      "Load knowledge also_bought...\n",
      "Total 1782364 also_bought edges.\n",
      "Load knowledge also_viewed...\n",
      "Total 310700 also_viewed edges.\n",
      "Load knowledge bought_together...\n",
      "Total 18042 bought_together edges.\n",
      "Remove duplicates...\n",
      "--\n",
      "Compute node degrees...\n",
      "--\n"
     ]
    }
   ],
   "source": [
    "# Generate knowledge graph instance.\n",
    "# ========== BEGIN ========== #\n",
    "print('Create', BEAUTY, 'knowledge graph from dataset...')\n",
    "dataset_beauty = load_dataset(BEAUTY)\n",
    "print('--')\n",
    "kg_beauty = KnowledgeGraph(dataset_beauty)\n",
    "print('--')\n",
    "kg_beauty.compute_degrees()\n",
    "print('--')\n",
    "save_kg(BEAUTY, kg_beauty)\n",
    "# =========== END =========== #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate beauty train/test labels.\n",
      "review_file: ./data/Amazon_Beauty/train.txt.gz\n",
      "user_products: [1086, 2725, 2136, 535, 8179]\n",
      "review_file: ./data/Amazon_Beauty/test.txt.gz\n",
      "user_products: [7745]\n"
     ]
    }
   ],
   "source": [
    "# Genereate train/test labels.\n",
    "# ========== BEGIN ========== #\n",
    "print('Generate', BEAUTY, 'train/test labels.')\n",
    "generate_labels(BEAUTY, 'train')#表示第一个用户买的商品的编号\n",
    "generate_labels(BEAUTY, 'test')\n",
    "# =========== END =========== #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CELL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load user of size 27879\n",
      "Load product of size 10429\n",
      "Load word of size 22493\n",
      "Load related_product of size 101287\n",
      "Load brand of size 955\n",
      "Load category of size 206\n",
      "Load produced_by of size 10429\n",
      "Load belongs_to of size 10429\n",
      "Load also_bought of size 10429\n",
      "Load also_viewed of size 10429\n",
      "Load bought_together of size 10429\n",
      "word_indices: [5]\n",
      "Load review of size 194439 word count= 194439\n",
      "Create word sampling rate\n"
     ]
    }
   ],
   "source": [
    "#parser = argparse.ArgumentParser()\n",
    "#parser.add_argument('--dataset', type=str, default=BEAUTY, help='One of {BEAUTY, CELL, CD, CLOTH}.')\n",
    "#args = parser.parse_args()\n",
    "\n",
    "# Create AmazonDataset instance for dataset.\n",
    "# ========== BEGIN ========== #\n",
    "#print('Load', args.dataset, 'dataset from file...')\n",
    "if not os.path.isdir(TMP_DIR[CELL]):\n",
    "    os.makedirs(TMP_DIR[CELL])\n",
    "dataset = AmazonDataset(DATASET_DIR[CELL])\n",
    "save_dataset(CELL, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create cell knowledge graph from dataset...\n",
      "dataset_file: ./tmp/Amazon_Cellphones/dataset.pkl\n",
      "Load entities...\n",
      "Total 163249 nodes.\n",
      "Load reviews...\n",
      "Total 1166634 review edges.\n",
      "uid pid reamained_words. 19768 ; 6191 ; [5]\n",
      "type of remained words: <class 'list'>\n",
      "Load knowledge produced_by...\n",
      "Total 10836 produced_by edges.\n",
      "Load knowledge belongs_to...\n",
      "Total 72786 belongs_to edges.\n",
      "Load knowledge also_bought...\n",
      "Total 1179008 also_bought edges.\n",
      "Load knowledge also_viewed...\n",
      "Total 25860 also_viewed edges.\n",
      "Load knowledge bought_together...\n",
      "Total 16888 bought_together edges.\n",
      "Remove duplicates...\n",
      "Compute node degrees...\n"
     ]
    }
   ],
   "source": [
    "# Generate knowledge graph instance.\n",
    "# ========== BEGIN ========== #\n",
    "print('Create', CELL, 'knowledge graph from dataset...')\n",
    "dataset = load_dataset(CELL)\n",
    "#print(dataset)\n",
    "kg = KnowledgeGraph(dataset)\n",
    "#print(kg)\n",
    "kg.compute_degrees()\n",
    "save_kg(CELL, kg)\n",
    "# =========== END =========== #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate cell train/test labels.\n",
      "review_file: ./data/Amazon_Cellphones/train.txt.gz\n",
      "user_products: [6155, 6837, 4833, 9995, 10030, 4170, 3955, 7043, 3819, 9863, 6520, 10206, 6532, 8731, 9325]\n",
      "review_file: ./data/Amazon_Cellphones/test.txt.gz\n",
      "user_products: [8014, 2989, 1816, 2429, 7437, 1813]\n"
     ]
    }
   ],
   "source": [
    "# Genereate train/test labels.\n",
    "# ========== BEGIN ========== #\n",
    "print('Generate', CELL, 'train/test labels.')\n",
    "generate_labels(CELL, 'train')\n",
    "generate_labels(CELL, 'test')\n",
    "# =========== END =========== #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#main\n",
    "'''\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--dataset', type=str, default=BEAUTY, help='One of {BEAUTY, CELL, CD, CLOTH}.')\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # Create AmazonDataset instance for dataset.\n",
    "    # ========== BEGIN ========== #\n",
    "    print('Load', args.dataset, 'dataset from file...')\n",
    "    \n",
    "    if not os.path.isdir(TMP_DIR[args.dataset]):\n",
    "        os.makedirs(TMP_DIR[args.dataset])\n",
    "    dataset = AmazonDataset(DATASET_DIR[args.dataset])\n",
    "    save_dataset(args.dataset, dataset)\n",
    "\n",
    "    # Generate knowledge graph instance.\n",
    "    # ========== BEGIN ========== #\n",
    "    print('Create', args.dataset, 'knowledge graph from dataset...')\n",
    "    dataset = load_dataset(args.dataset)\n",
    "    kg = KnowledgeGraph(dataset)\n",
    "    kg.compute_degrees()\n",
    "    save_kg(args.dataset, kg)\n",
    "    # =========== END =========== #\n",
    "\n",
    "    # Genereate train/test labels.\n",
    "    # ========== BEGIN ========== #\n",
    "    print('Generate', args.dataset, 'train/test labels.')\n",
    "    generate_labels(args.dataset, 'train')\n",
    "    generate_labels(args.dataset, 'test')\n",
    "    # =========== END =========== #\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "node() takes no arguments",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-62100ab3eeca>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mval\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mval\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mroot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[0mroot\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: node() takes no arguments"
     ]
    }
   ],
   "source": [
    "class node():\n",
    "    def _init_(self,val):\n",
    "        self.left = None\n",
    "        self.right = None\n",
    "        self.val = val\n",
    "\n",
    "root = node(1)\n",
    "root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
